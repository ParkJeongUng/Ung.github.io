{
  
    
        "post0": {
            "title": "확률적 경사 하강법 SGDClassifier",
            "content": ". &#51216;&#51652;&#51201;&#51064; &#54617;&#49845; . 훈련 데이터가 한번에 준비되는 것이 아니라 조금씩 전달됨 | 기존의 훈련 데이터에 새로 추가되는 데이터를 쌓아 모델을 매일매일 다시 훈련한다. --&gt; 너무 오랜기간 데이터 쌓이면 용량이 너무 커져버림 | 새로운 데이터를 추가할 때 이전 데이터를 버림으로써 훈련 데이터 크기를 일정하게 유지 --&gt; 버린 데이터에 중요한 데이터가 섞여있으면 문제가 생김 | 앞서 훈련한 모델을 버리지 않고 새로운 데이터에 대해서만 조금씩 더 훈련한다. --&gt; 점진적 학습 | . &#49552;&#49892;&#54632;&#49688; // &#51060;&#51652; &#48516;&#47448; --&gt; &#47196;&#51648;&#49828;&#54001; &#49552;&#49892;&#54632;&#49688; // &#45796;&#51473; &#48516;&#47448; --&gt; &#53356;&#47196;&#49828;&#50644;&#53944;&#47196;&#54588; &#49552;&#49892;&#54632;&#49688; . 어떤 문제에서 알고리즘이 얼마나 엉터리인지를 측정하는 기준 | 값이 작을수록 좋지만 어떤 값이 최솟값인지 알지 못함 | 분류문제에서 손실은 정답을 맞추지 못하는 것 | . import pandas as pd fish = pd.read_csv(&#39;https://bit.ly/fish_csv_data&#39;) # Species 는 타겟 나머지는 입력 데이터 fish_input = fish[[&#39;Weight&#39;, &#39;Length&#39;, &#39;Diagonal&#39;, &#39;Height&#39;, &#39;Width&#39;]].to_numpy() fish_target = fish[&#39;Species&#39;].to_numpy() # train test split from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(fish_input, fish_target, random_state = 42) # standardscaler from sklearn.preprocessing import StandardScaler ss = StandardScaler() ss.fit(train_input) train_scaled = ss.transform(train_input) test_scaled = ss.transform(test_input) fish.head() . Species Weight Length Diagonal Height Width . 0 Bream | 242.0 | 25.4 | 30.0 | 11.5200 | 4.0200 | . 1 Bream | 290.0 | 26.3 | 31.2 | 12.4800 | 4.3056 | . 2 Bream | 340.0 | 26.5 | 31.1 | 12.3778 | 4.6961 | . 3 Bream | 363.0 | 29.0 | 33.5 | 12.7300 | 4.4555 | . 4 Bream | 430.0 | 29.0 | 34.0 | 12.4440 | 5.1340 | . from sklearn.linear_model import SGDClassifier sc = SGDClassifier(loss = &#39;log&#39;, max_iter = 10, random_state = 42) # &quot;loss = &#39;log&#39;&quot;는 로지스틱 손실 함수를 지정, max_iter는 에포크 횟수를 지정 sc.fit(train_scaled, train_target) print(sc.score(train_scaled, train_target)) print(sc.score(test_scaled, test_target)) print(pd.unique(fish_target)) # 다중분류에서 로지스틱 손실함수를 지정할 경우 클래스 마다 이진분류 모델을 만든다 --&gt; ex) 도미를 양성클래스로 놓고 나머지를 모두 음성클래스로 만들어서 모델을 만드는 방식 이런방식을 OvR (one versus Rest)라고 함. . 0.773109243697479 0.775 [&#39;Bream&#39; &#39;Roach&#39; &#39;Whitefish&#39; &#39;Parkki&#39; &#39;Perch&#39; &#39;Pike&#39; &#39;Smelt&#39;] . /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit. ConvergenceWarning) . # partial_fit() 을 호출할 때마다 1에포크씩 이어서 훈련함 sc.partial_fit(train_scaled, train_target) print(sc.score(train_scaled, train_target)) print(sc.score(test_scaled, test_target)) # 에포크가 늘어나니 점수가 높아짐 --&gt; 에포크를 더 늘려볼 필요가 있겠음 . 0.8151260504201681 0.825 . import numpy as np sc = SGDClassifier(loss = &#39;log&#39;, random_state = 42) train_score = [] test_score = [] # partial_fit()을 300번 반복 --&gt; partial_fit() 만 사용하려면 전체 클래스의 레이블을 partial_fit() 에 전달해주어야 함 classes = np.unique(train_target) for _ in range(0, 300): # 파이썬의 &quot; _ &quot; 는 나중에 사용하지 않고 그냥 버리는 값을 넣어두는 용도 --&gt; 여기서는 반복횟수를 임시저장하는 용도 sc.partial_fit(train_scaled, train_target, classes = classes) train_score.append(sc.score(train_scaled, train_target)) test_score.append(sc.score(test_scaled, test_target)) . import matplotlib.pyplot as plt plt.plot(train_score) plt.plot(test_score) plt.xlabel(&#39;epoch&#39;) plt.ylabel(&#39;accuracy&#39;) plt.show() # 에포크가 100이상부터는 과대적합으로 보임 . sc = SGDClassifier(loss = &#39;log&#39;, max_iter = 100, tol = None, random_state=42) # tol 매개변수는 일정 에포크동안 성능이 향상되지 않으면 더 훈련하지 않고 자동으로 멈춘다. None 으로 지정해 무조건 100 에포크까지 실행하도록 함 sc.fit(train_scaled, train_target) print(sc.score(train_scaled, train_target)) print(sc.score(test_scaled, test_target)) . 0.957983193277311 0.925 . &#50672;&#49845; . import pandas as pd fish = pd.read_csv(&#39;https://bit.ly/fish_csv_data&#39;) fish_input = fish[[&#39;Weight&#39;, &#39;Length&#39;, &#39;Diagonal&#39;, &#39;Height&#39;, &#39;Width&#39;]].to_numpy() fish_target = fish[&#39;Species&#39;].to_numpy() from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(fish_input, fish_target, random_state = 42) from sklearn.preprocessing import StandardScaler ss = StandardScaler() ss.fit(train_input) train_scaled = ss.transform(train_input) test_scaled = ss.transform(test_input) from sklearn.linear_model import SGDClassifier sc = SGDClassifier(loss = &#39;log&#39;, random_state = 42, max_iter = 10) sc.fit(train_scaled, train_target) print(sc.score(train_scaled, train_target)) print(sc.score(test_scaled, test_target)) sc.partial_fit(train_scaled, train_target) print(sc.score(train_scaled, train_target)) print(sc.score(test_scaled, test_target)) # 에포크 지정 import numpy as np import matplotlib.pyplot as plt train_score = [] test_score = [] classes = np.unique(train_target) sc = SGDClassifier(loss = &#39;log&#39;, random_state = 42) for _ in range(0, 300): sc.partial_fit(train_scaled, train_target, classes = classes) train_score.append(sc.score(train_scaled, train_target)) test_score.append(sc.score(test_scaled, test_target)) plt.plot(train_score) plt.plot(test_score) plt.show() sc = SGDClassifier(loss = &#39;log&#39;, random_state = 42, tol = None, max_iter = 100) sc.fit(train_scaled, train_target) print(sc.score(train_scaled, train_target)) print(sc.score(test_scaled, test_target)) . /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit. ConvergenceWarning) . 0.773109243697479 0.775 0.8151260504201681 0.825 . 0.957983193277311 0.925 .",
            "url": "https://parkjeongung.github.io/Ung.github.io/2021/12/26/%ED%99%95%EB%A5%A0%EC%A0%81_%EA%B2%BD%EC%82%AC_%ED%95%98%EA%B0%95%EB%B2%95_SGDClassifier.html",
            "relUrl": "/2021/12/26/%ED%99%95%EB%A5%A0%EC%A0%81_%EA%B2%BD%EC%82%AC_%ED%95%98%EA%B0%95%EB%B2%95_SGDClassifier.html",
            "date": " • Dec 26, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "지도학습 비지도학습 // 훈련세트 테스트세트",
            "content": ". &#54984;&#47144;&#49464;&#53944;&#50752; &#53580;&#49828;&#53944;&#49464;&#53944; . 샘플링편향 발생 . fish_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0, 9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0] fish_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0, 6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9] . fish_data = [[l, w] for l, w in zip(fish_length, fish_weight)] fish_target = [1] * 35 + [0] * 14 . from sklearn.neighbors import KNeighborsClassifier kn = KNeighborsClassifier() . train_input = fish_data[: 35] train_target = fish_target[: 35] test_input = fish_data[35: ] test_target = fish_target[35: ] . # 테스트세트 --&gt; score()함수로 모델 평가 kn = kn.fit(train_input, train_target) kn.score(test_input, test_target) #샘플링 편향 발생 --&gt; 마지막 14개(빙어 특성 14개)를 test_input 으로 넣어놔서 훈련에 사용된 train_input 에는 빙어가 하나도 없음 --&gt; 데이터를 섞든지 골고루 샘플을 뽑아야 함 --&gt; numpy 사용 . 0.0 . NUMPY&#47484; &#51060;&#50857;&#54620; &#49368;&#54540;&#47553; &#54200;&#54693; &#54644;&#44208; . 파이썬의 대표적인 배열 라이브러리 | 고차원의 배열을 쉽게 만들고 조작가능 | . import numpy as np . input_arr = np.array(fish_data) target_arr = np.array(fish_target) . print(input_arr.shape) . (49, 2) . # input 과 target 에서 같은 인덱스는 함께 선택되어야 한다. input의 2번은 train으로 target의 2번은 test로 가면 안된다. # 넘파이의 random 함수들은 실행할 때마다 다른 결과를 만든다. --&gt; random.seed()를 지정하면 항상 일정한 결과를 얻을 수 있다. np.random.seed(42) index = np.arange(49) np.random.shuffle(index) index . array([13, 45, 47, 44, 17, 27, 26, 25, 31, 19, 12, 4, 34, 8, 3, 6, 40, 41, 46, 15, 9, 16, 24, 33, 30, 0, 43, 32, 5, 29, 11, 36, 1, 21, 2, 37, 35, 23, 39, 10, 22, 18, 48, 20, 7, 42, 14, 28, 38]) . print(input_arr[[1,3]]) # input_arr의 2번째와 4번째 원소 출력 # numpy 배열을 인덱스로 전하기 --&gt; 훈련세트 생성 train_input = input_arr[index [ : 35]] train_target = input_arr[index [ : 35]] #랜덤으로 만들어진 index의 첫번째 원소는 13 --&gt; input_arr의 14번째 원소(index = 13)가 train_input의 1번째 원소(index = 0)에 들어감 print(input_arr[13], train_input[0]) # numpy 배열을 인덱스로 전하기 --&gt; 테스트세트 생성 test_input = input_arr[index [35 : ]] test_target = input_arr[index [35 : ]] #랜덤으로 만들어진 index의 35번째 원소는 37 --&gt; input_arr의 38번째 원소(indexe = 37)가 test_input의 1번째 원소(index = 0)에 들어감 print(input_arr[37], test_input[0]) . [[ 26.3 290. ] [ 29. 363. ]] [ 32. 340.] [ 32. 340.] [10.6 7. ] [10.6 7. ] . import matplotlib.pyplot as plt plt.scatter(train_input[ :, 0], train_input[ :, 1]) plt.scatter(test_input[ :, 0], test_input[ :, 1]) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() . &#50672;&#49845; . fish_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0, 9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0] fish_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0, 6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9] fish_data = [[l, w] for l, w in zip(fish_length, fish_weight)] fish_target = [1] * 35 + [0] * 14 import numpy as np input_arr = np.array(fish_data) target_arr = np.array(fish_target) np.random.seed(42) index = np.arange(49) np.random.shuffle(index) train_input = input_arr[index[ : 35]] train_target = target_arr[index [ : 35]] test_input = input_arr[index[35 :]] test_target = target_arr[index[35 :]] from sklearn.neighbors import KNeighborsClassifier kn = KNeighborsClassifier() kn.fit(train_input, train_target) kn.score(test_input, test_target) . 1.0 .",
            "url": "https://parkjeongung.github.io/Ung.github.io/2021/12/26/%EC%A7%80%EB%8F%84%ED%95%99%EC%8A%B5_%EB%B9%84%EC%A7%80%EB%8F%84%ED%95%99%EC%8A%B5_%ED%9B%88%EB%A0%A8%EC%84%B8%ED%8A%B8_%ED%85%8C%EC%8A%A4%ED%8A%B8%EC%84%B8%ED%8A%B8.html",
            "relUrl": "/2021/12/26/%EC%A7%80%EB%8F%84%ED%95%99%EC%8A%B5_%EB%B9%84%EC%A7%80%EB%8F%84%ED%95%99%EC%8A%B5_%ED%9B%88%EB%A0%A8%EC%84%B8%ED%8A%B8_%ED%85%8C%EC%8A%A4%ED%8A%B8%EC%84%B8%ED%8A%B8.html",
            "date": " • Dec 26, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "선형 회귀 // LinearRegression",
            "content": ". import numpy as np perch_length = np.array([8.4, 13.7, 15.0, 16.2, 17.4, 18.0, 18.7, 19.0, 19.6, 20.0, 21.0, 21.0, 21.0, 21.3, 22.0, 22.0, 22.0, 22.0, 22.0, 22.5, 22.5, 22.7, 23.0, 23.5, 24.0, 24.0, 24.6, 25.0, 25.6, 26.5, 27.3, 27.5, 27.5, 27.5, 28.0, 28.7, 30.0, 32.8, 34.5, 35.0, 36.5, 36.0, 37.0, 37.0, 39.0, 39.0, 39.0, 40.0, 40.0, 40.0, 40.0, 42.0, 43.0, 43.0, 43.5, 44.0]) # 농어의 타겟 = 무게 perch_weight = np.array([5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0, 115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0, 150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0, 218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0, 556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0, 850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0, 1000.0]) # train test split from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(perch_length, perch_weight, random_state = 42) # 2차원 변형 train_input = train_input.reshape(-1, 1) test_input = test_input.reshape(-1, 1) # 모델링 --&gt; 이웃개수 3개인 최근접회귀 from sklearn.neighbors import KNeighborsRegressor knr = KNeighborsRegressor(n_neighbors = 3) knr.fit(train_input, train_target) . KNeighborsRegressor(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;, metric_params=None, n_jobs=None, n_neighbors=3, p=2, weights=&#39;uniform&#39;) . 50cm &#51064; &#45453;&#50612;&#51032; &#47924;&#44172; &#50696;&#52769; . print(knr.predict([[50]])) # 그러나 실제 50cm인 농어의 무게와 오차가 심하다는 예시 . [1033.33333333] . distancecs, indexes = knr.kneighbors([[50]]) predict = knr.predict([[50]]) import matplotlib.pyplot as plt plt.scatter(train_input, train_target) plt.scatter(50, predict, marker = &#39;^&#39;) plt.scatter(train_input[indexes], train_target[indexes], marker = &#39;D&#39;) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() # 훈련세트의 범위를 벗어났기 때문에 엉뚱한 값을 출력함 --&gt; 100cm 농어의 경우도 똑같은 무게를 출력 print(knr.predict([[100]])) . [1033.33333333] . &#49440;&#54805; &#54924;&#44480; &#47784;&#45944;&#47553; . KNeighborsRegressor로 위 문제를 해결하려면 train 세트를 길이가 긴 농어가 포함되도록 다시 만들어야 한다. | 선형 회귀를 이용하면 train 세트의 범위를 벗어나도 길이가 긴 농어의 무게를 예측할 수 있다. | 선형 회귀에서 알고리즘이 찾아낸 직선의 기울기나 y절편 값을 &quot;모델 파라미터&quot; 라고 한다. | . from sklearn.linear_model import LinearRegression lr = LinearRegression() lr.fit(train_input, train_target) # 50cm 농어의 무게 예측값과 100cm 농어의 무게 예측값 print(lr.predict([[50], [100]])) . [1241.83860323 3192.69585141] . # 이러한 파라미터들은 coef_(기울기) 와 intercept(y절편) 에 저장되어 있다. # 머신러닝에서 &quot;기울기&quot;는 &quot;계수&quot; 또는 &quot;가중치&quot; 라고도 한다. print(lr.coef_, lr.intercept_) # 훈련세트와 농어의 길이 15 ~ 50를 기준으로 알고리즘이 찾아낸 직선 그리기 import matplotlib.pyplot as plt plt.scatter(train_input, train_target) # 15 ~ 50까지 직선 그리기 plt.plot([15, 50], [15 * lr.coef_ + lr.intercept_, 50 * lr.coef_ + lr.intercept_]) # 50cm 농어 데이터 predict = lr.predict([[50]]) plt.scatter(50, predict, marker = &#39;^&#39;) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() . [39.01714496] -709.0186449535477 . print(lr.score(train_input, train_target)) print(lr.score(test_input, test_target)) # 테스트세트의 점수가 현저히 낮은 과대적합 # 훈련세트의 점수 또한 높은 편이 아니기에 전체적으로 과소적합 되었다. # 직선을 확인했을 때 X = 15 인 구간에서 Y 값이 0 이하가 나온다. --&gt; 현실에서는 불가능 . 0.9398463339976039 0.8247503123313558 . &#45796;&#54637; &#54924;&#44480; . import matplotlib.pyplot as plt plt.scatter(train_input, train_target) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() # train세트의 산점도는 일직선 이라기 보다는 곡선의 형태에 가깝다 --&gt; 2차방정식의 그래프를 그려보자 --&gt; y = ax^2 + bx + c . # np.column_stack() 을 이용해서 train세트의 제곱과 train세트 2배열을 나란히 붙인다. --&gt; test세트도 마찬가지 train_poly = np.column_stack((train_input ** 2, train_input)) test_poly = np.column_stack((test_input ** 2, test_input)) print(train_poly.shape, test_poly.shape) # 모든 원소 제곱이 하나의 열을 이루었기 때문에 열이 2개로 늘어났다. . (42, 2) (14, 2) . from sklearn.linear_model import LinearRegression lr = LinearRegression() lr.fit(train_poly, train_target) print(lr.predict([[50 ** 2, 50]])) . [1573.98423528] . print(lr.coef_, lr.intercept_) # trian 세트 산점도를 그리고 그 위에 계수와 절편을 이용해 모델링에 사용된 곡선 그리기 x = np.arange(15, 51) # 15cm ~ 50cm길이의 농어 배열 생성 import matplotlib.pyplot as plt plt.scatter(train_input, train_target) plt.plot(x, lr.coef_[0] * x ** 2 + lr.coef_[1] * x + lr.intercept_) plt.scatter(50, lr.predict([[50 ** 2, 50]]), marker = &#39;^&#39;) plt.show() . [ 1.01433211 -21.55792498] 116.05021078278276 . print(lr.score(train_poly, train_target)) print(lr.score(test_poly, test_target)) # 과소적합이 아직 조금은 남아있음 --&gt; 조금 더 복잡한 모델 생성 . 0.9706807451768623 0.9775935108325122 . &#50672;&#49845; . import numpy as np perch_length = np.array([8.4, 13.7, 15.0, 16.2, 17.4, 18.0, 18.7, 19.0, 19.6, 20.0, 21.0, 21.0, 21.0, 21.3, 22.0, 22.0, 22.0, 22.0, 22.0, 22.5, 22.5, 22.7, 23.0, 23.5, 24.0, 24.0, 24.6, 25.0, 25.6, 26.5, 27.3, 27.5, 27.5, 27.5, 28.0, 28.7, 30.0, 32.8, 34.5, 35.0, 36.5, 36.0, 37.0, 37.0, 39.0, 39.0, 39.0, 40.0, 40.0, 40.0, 40.0, 42.0, 43.0, 43.0, 43.5, 44.0]) # 농어의 타겟 = 무게 perch_weight = np.array([5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0, 115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0, 150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0, 218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0, 556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0, 850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0, 1000.0]) # train test split from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(perch_length, perch_weight, random_state = 42) # 2차원 변형 train_input = train_input.reshape(-1,1) test_input = test_input.reshape(-1,1) # 선형회귀 모델링 from sklearn.linear_model import LinearRegression lr = LinearRegression() lr.fit(train_input, train_target) # 점수 확인 --&gt; 과대적합 이면서 전체적인 과소적합 print(lr.score(train_input, train_target)) print(lr.score(test_input, test_target)) # 산점도와 직선 확인 import matplotlib.pyplot as plt plt.scatter(train_input, train_target) plt.plot([15, 50], [15 * lr.coef_ + lr.intercept_, 50 * lr.coef_ + lr.intercept_]) plt.scatter(50, lr.predict([[50]]), marker = &#39;^&#39;) print(plt.show()) # 2차방정식 선형회귀 모델링 # 제곱형태의 train, test세트 준비 train_poly = np.column_stack((train_input ** 2, train_input)) test_poly = np.column_stack((test_input ** 2, test_input)) # 모델링 lr.fit(train_poly, train_target) # 점수확인 --&gt; 점수 많이 좋아짐 print(lr.score(train_poly, train_target)) print(lr.score(test_poly, test_target)) # 산점도와 곡선확인 plt.scatter(train_input, train_target) x = np.arange(15, 51) plt.plot(x, x ** 2 * lr.coef_[0] + x * lr.coef_[1] + lr.intercept_) plt.scatter(50, lr.predict([[50 ** 2, 50]]), marker = &#39;^&#39;) plt.show() . 0.9398463339976039 0.8247503123313558 . None 0.9706807451768623 0.9775935108325122 .",
            "url": "https://parkjeongung.github.io/Ung.github.io/2021/12/26/%EC%84%A0%ED%98%95_%ED%9A%8C%EA%B7%80_LinearRegression.html",
            "relUrl": "/2021/12/26/%EC%84%A0%ED%98%95_%ED%9A%8C%EA%B7%80_LinearRegression.html",
            "date": " • Dec 26, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "로지스틱회귀 LogisticeRegression // 시그모이드 // 소프트맥스",
            "content": ". import pandas as pd fish = pd.read_csv(&#39;https://bit.ly/fish_csv_data&#39;) fish.head() . Species Weight Length Diagonal Height Width . 0 Bream | 242.0 | 25.4 | 30.0 | 11.5200 | 4.0200 | . 1 Bream | 290.0 | 26.3 | 31.2 | 12.4800 | 4.3056 | . 2 Bream | 340.0 | 26.5 | 31.1 | 12.3778 | 4.6961 | . 3 Bream | 363.0 | 29.0 | 33.5 | 12.7300 | 4.4555 | . 4 Bream | 430.0 | 29.0 | 34.0 | 12.4440 | 5.1340 | . print(pd.unique(fish[&#39;Species&#39;])) . [&#39;Bream&#39; &#39;Roach&#39; &#39;Whitefish&#39; &#39;Parkki&#39; &#39;Perch&#39; &#39;Pike&#39; &#39;Smelt&#39;] . # Species 를 제외한 나머지 열이 특성이 된다. fish_input = fish[[&#39;Weight&#39;, &#39;Length&#39;, &#39;Diagonal&#39;, &#39;Height&#39;, &#39;Width&#39;]].to_numpy() fish_target = fish[&#39;Species&#39;].to_numpy() # 넘파이 배열로 잘 바뀌었는지 확인 print(fish_input[: 5]) . [[242. 25.4 30. 11.52 4.02 ] [290. 26.3 31.2 12.48 4.3056] [340. 26.5 31.1 12.3778 4.6961] [363. 29. 33.5 12.73 4.4555] [430. 29. 34. 12.444 5.134 ]] . from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(fish_input, fish_target, random_state = 42) . from sklearn.preprocessing import StandardScaler ss = StandardScaler() ss.fit(train_input) train_scaled = ss.transform(train_input) test_scaled = ss.transform(test_input) . K - &#52572;&#44540;&#51217; &#51060;&#50883; &#48516;&#47448;&#44592; &#51060;&#50857; . from sklearn.neighbors import KNeighborsClassifier kn = KNeighborsClassifier(n_neighbors = 3) kn.fit(train_scaled, train_target) print(kn.score(train_scaled, train_target)) print(kn.score(test_scaled, test_target)) . 0.8907563025210085 0.85 . print(kn.classes_) # 도미 빙어 분류 --&gt; 이진분류 # 럭키백 --&gt; 다중분류 . [&#39;Bream&#39; &#39;Parkki&#39; &#39;Perch&#39; &#39;Pike&#39; &#39;Roach&#39; &#39;Smelt&#39; &#39;Whitefish&#39;] . print(kn.predict(test_scaled[:5])) . [&#39;Perch&#39; &#39;Smelt&#39; &#39;Pike&#39; &#39;Perch&#39; &#39;Perch&#39;] . # np.round --&gt; 반올림 --&gt; decimals 로 소수점 어디서 반올림 할건지 지정 import numpy as np proba = kn.predict_proba(test_scaled[:5]) print(np.round(proba, decimals = 4)) . [[0. 0. 1. 0. 0. 0. 0. ] [0. 0. 0. 0. 0. 1. 0. ] [0. 0. 0. 1. 0. 0. 0. ] [0. 0. 0.6667 0. 0.3333 0. 0. ] [0. 0. 0.6667 0. 0.3333 0. 0. ]] . distances, indexes = kn.kneighbors(test_scaled[3:4]) # kneighbors 메서드의 입력은 항상 2차원 배열이어야 하므로 슬라이싱으로 해야지 오류가 안남 print(train_target[indexes]) . [[&#39;Roach&#39; &#39;Perch&#39; &#39;Perch&#39;]] . &#50672;&#49845; . import pandas as pd fish = pd.read_csv(&#39;https://bit.ly/fish_csv_data&#39;) fish.head() # 특성 확인 print(pd.unique(fish[&#39;Species&#39;])) # 입력 타깃 나누기 fish_input = fish[[&#39;Weight&#39;, &#39;Length&#39;, &#39;Diagonal&#39;, &#39;Height&#39;, &#39;Width&#39;]].to_numpy() fish_target = fish[&#39;Species&#39;].to_numpy() # train test split from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(fish_input, fish_target, random_state = 42) # standard scaler from sklearn.preprocessing import StandardScaler ss = StandardScaler() ss.fit(train_input) train_scaled = ss.transform(train_input) test_scaled = ss.transform(test_input) # 최근접 이웃 from sklearn.neighbors import KNeighborsClassifier kn = KNeighborsClassifier(n_neighbors = 3) kn.fit(train_scaled, train_target) print(kn.score(train_scaled, train_target)) print(kn.score(test_scaled, test_target)) # 사용가능 타깃값 확인 kn.classes_ print(kn.classes_) # 샘플 5개 예측 print(kn.predict(test_scaled[:5])) # 타깃별 확률값 출력 import numpy as np proba = kn.predict_proba(test_scaled[:5]) print(np.round(proba, decimals = 4)) # 4번째 샘플의 사용 이웃확인 distances, indexes = kn.kneighbors(test_scaled[3: 4]) print(train_target[indexes]) . [&#39;Bream&#39; &#39;Roach&#39; &#39;Whitefish&#39; &#39;Parkki&#39; &#39;Perch&#39; &#39;Pike&#39; &#39;Smelt&#39;] 0.8907563025210085 0.85 [&#39;Bream&#39; &#39;Parkki&#39; &#39;Perch&#39; &#39;Pike&#39; &#39;Roach&#39; &#39;Smelt&#39; &#39;Whitefish&#39;] [&#39;Perch&#39; &#39;Smelt&#39; &#39;Pike&#39; &#39;Perch&#39; &#39;Perch&#39;] [[0. 0. 1. 0. 0. 0. 0. ] [0. 0. 0. 0. 0. 1. 0. ] [0. 0. 0. 1. 0. 0. 0. ] [0. 0. 0.6667 0. 0.3333 0. 0. ] [0. 0. 0.6667 0. 0.3333 0. 0. ]] [[&#39;Roach&#39; &#39;Perch&#39; &#39;Perch&#39;]] . &#47196;&#51648;&#49828;&#54001; &#54924;&#44480; . 이름은 회귀이지만 분류 모델 | 다중회귀를 한 선형 회귀와 동일하게 선형 방정식을 학습 | 시그모이드 0과 1사이의 값만 출력 --&gt; 이진분류에 사용 | 소프트맥스 0과 1사이의 값을 출력 하는데 총합이 1 --&gt; 다중분류에 사용 | . import numpy as np import matplotlib.pyplot as plt z = np.arange(-5, 5, 0.1) phi = 1 / (1 + np.exp(-z)) # 지수 함수의 계산 --&gt; np.exp() plt.plot(z, phi) plt.xlabel(&#39;z&#39;) plt.ylabel(&#39;phi&#39;) plt.show() # 시그모이드 함수 출력이 0.5 보다 크면 양성, 0.5 이하이면 음성 # 도미와 빙어로 이진분류 수행해보기 . # ex --&gt; A, C 만 선택하기 char_arr = np.array([&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;, &#39;E&#39;]) print(char_arr[[True, False, True, False, False]]) # 적용 bream_smelt_indexes = (train_target == &#39;Bream&#39;) | (train_target == &#39;Smelt&#39;) train_bream_smelt = train_scaled[bream_smelt_indexes] target_bream_smelt = train_target[bream_smelt_indexes] . [&#39;A&#39; &#39;C&#39;] . from sklearn.linear_model import LogisticRegression lr = LogisticRegression() lr.fit(train_bream_smelt, target_bream_smelt) . LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=100, multi_class=&#39;auto&#39;, n_jobs=None, penalty=&#39;l2&#39;, random_state=None, solver=&#39;lbfgs&#39;, tol=0.0001, verbose=0, warm_start=False) . lr.predict(train_bream_smelt[:5]) . array([&#39;Bream&#39;, &#39;Smelt&#39;, &#39;Bream&#39;, &#39;Bream&#39;, &#39;Bream&#39;], dtype=object) . print(lr.predict_proba(train_bream_smelt[:5])) # 첫번째 열이 음성클래스 두번째 열이 양성클래스 # 어떤 것이 음성인지 확인 print(lr.classes_) # bream 이 음성 smelt 가 양성 . [[0.99759855 0.00240145] [0.02735183 0.97264817] [0.99486072 0.00513928] [0.98584202 0.01415798] [0.99767269 0.00232731]] [&#39;Bream&#39; &#39;Smelt&#39;] . decisions = lr.decision_function(train_bream_smelt[: 5]) decisions . array([-6.02927744, 3.57123907, -5.26568906, -4.24321775, -6.0607117 ]) . # 시그모이드 함수 --&gt; expit() from scipy.special import expit print(expit(decisions)) # predict_proba 의 두번째 열과 동일하다 --&gt; decision_function 은 양성 클래스에 대한 Z값을 반환한다. # predict_proba 는 음성과 양성 클래스에 대한 확률을 둘다 반환한다. . [0.00240145 0.97264817 0.00513928 0.01415798 0.00232731] . from sklearn.linear_model import LogisticRegression lr = LogisticRegression(C = 20, max_iter = 1000) # C 는 라쏘 릿지의 alpha 값 같은 느낌 --&gt; 규제의 정도를 조절 --&gt; alpha 와는 달리 값이 작을수록 규제의 강도가 큼 lr.fit(train_scaled, train_target) print(lr.score(train_scaled, train_target)) print(lr.score(test_scaled, test_target)) . 0.9327731092436975 0.925 . print(lr.predict(test_scaled[: 5])) . [&#39;Perch&#39; &#39;Smelt&#39; &#39;Pike&#39; &#39;Roach&#39; &#39;Perch&#39;] . import numpy as np proba = lr.predict_proba(test_scaled[: 5]) print(np.round(proba, decimals = 3)) . [[0. 0.014 0.841 0. 0.136 0.007 0.003] [0. 0.003 0.044 0. 0.007 0.946 0. ] [0. 0. 0.034 0.935 0.015 0.016 0. ] [0.011 0.034 0.306 0.007 0.567 0. 0.076] [0. 0. 0.904 0.002 0.089 0.002 0.001]] . print(lr.classes_) . [&#39;Bream&#39; &#39;Parkki&#39; &#39;Perch&#39; &#39;Pike&#39; &#39;Roach&#39; &#39;Smelt&#39; &#39;Whitefish&#39;] . decision = lr.decision_function(test_scaled[: 5]) print(np.round(decision, decimals = 2)) from scipy.special import softmax proba = softmax(decision, axis = 1) print(np.round(proba, decimals = 3)) . [[ -6.5 1.03 5.16 -2.73 3.34 0.33 -0.63] [-10.86 1.93 4.77 -2.4 2.98 7.84 -4.26] [ -4.34 -6.23 3.17 6.49 2.36 2.42 -3.87] [ -0.68 0.45 2.65 -1.19 3.26 -5.75 1.26] [ -6.4 -1.99 5.82 -0.11 3.5 -0.11 -0.71]] [[0. 0.014 0.841 0. 0.136 0.007 0.003] [0. 0.003 0.044 0. 0.007 0.946 0. ] [0. 0. 0.034 0.935 0.015 0.016 0. ] [0.011 0.034 0.306 0.007 0.567 0. 0.076] [0. 0. 0.904 0.002 0.089 0.002 0.001]] . &#50672;&#49845; . import pandas as pd fish = pd.read_csv(&#39;https://bit.ly/fish_csv_data&#39;) fish.head() # 넘파이 전환 fish_input = fish[[&#39;Weight&#39;, &#39;Length&#39;, &#39;Diagonal&#39;, &#39;Height&#39;, &#39;Width&#39;]].to_numpy() fish_target = fish[&#39;Species&#39;].to_numpy() # train test split from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(fish_input, fish_target, random_state = 42) # 스케일링 from sklearn.preprocessing import StandardScaler ss = StandardScaler() ss.fit(train_input) train_scaled = ss.transform(train_input) test_scaled = ss.transform(test_input) # 이진분류 도미 빙어 bream_smelt_indexes = (train_target == &#39;Bream&#39;) | (train_target == &#39;Smelt&#39;) bream_smelt_train = train_scaled[bream_smelt_indexes] bream_smelt_target = train_target[bream_smelt_indexes] # 모델 훈련 from sklearn.linear_model import LogisticRegression lr = LogisticRegression() lr.fit(bream_smelt_train, bream_smelt_target) # 샘플 5개 예측 lr.predict(bream_smelt_train[: 5]) # 샘플 5개 확률 print(lr.predict_proba(bream_smelt_train[: 5])) # Z값 구하기 decision = lr.decision_function(bream_smelt_train[: 5]) print(decision) # 시그모이드 대입 from scipy.special import expit print(expit(decision)) # 다중분류 lr.fit(train_scaled, train_target) # 샘플 5개 예측 print(lr.predict(test_scaled[: 5])) # 샘플 5개 확률 proba = lr.predict_proba(test_scaled[: 5]) import numpy as np print(np.round(proba, decimals=3)) # Z값 구하기 decision = lr.decision_function(test_scaled[: 5]) # 소프트맥스 대입 from scipy.special import softmax softmax = softmax(decision, axis = 1) print(np.round(softmax, decimals=3)) .",
            "url": "https://parkjeongung.github.io/Ung.github.io/2021/12/26/%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1%ED%9A%8C%EA%B7%80_LogisticeRegression_%EC%8B%9C%EA%B7%B8%EB%AA%A8%EC%9D%B4%EB%93%9C_%EC%86%8C%ED%94%84%ED%8A%B8%EB%A7%A5%EC%8A%A4.html",
            "relUrl": "/2021/12/26/%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1%ED%9A%8C%EA%B7%80_LogisticeRegression_%EC%8B%9C%EA%B7%B8%EB%AA%A8%EC%9D%B4%EB%93%9C_%EC%86%8C%ED%94%84%ED%8A%B8%EB%A7%A5%EC%8A%A4.html",
            "date": " • Dec 26, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "데이터 전처리",
            "content": ". fish_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0, 9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0] fish_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0, 6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9] . &#45336;&#54028;&#51060;&#47196; input &#45936;&#51060;&#53552;&#50752; target &#45936;&#51060;&#53552; &#47564;&#46308;&#44592; . import numpy as np fish_data = np.column_stack((fish_length, fish_weight)) # target 데이터 생성 --&gt; np.ones(), np.zeros() # 두개의 배열을 연결한다 --&gt; np.concatenate(()) fish_target = np.concatenate((np.ones(35), np.zeros(14))) . &#49324;&#51060;&#53431;&#47088;&#51004;&#47196; &#54984;&#47144;&#49464;&#53944;&#50752; &#53580;&#49828;&#53944;&#49464;&#53944; &#45208;&#45572;&#44592; . from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(fish_data, fish_target, random_state = 42) # 잘 나누어졌는지 shpae로 데이터크기 출력 print(train_input.shape, test_input.shape) print(train_target.shape, test_target.shape) # 전체 데이터의 도미와 빙어의 비율 = 35 (도미) : 14 (빙어) / 2.5 : 1 # 테스트 세트의 도미와 빙어의 비율 = 10 (도미) : 3 (빙어) / 3.3 : 1 # 샘플링 편향 발생 print(test_target) . (36, 2) (13, 2) (36,) (13,) [1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.] . # stratify 매개변수에 target데이터를 전달하면 클래스 비율에 맞게 데이터를 나눈다. from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(fish_data, fish_target, stratify = fish_target, random_state = 42) # 전체 데이터의 도미와 빙어의 비율 = 35 (도미) : 14 (빙어) / 2.5 : 1 # 테스트 세트의 도미와 빙어의 비율 = 9 (도미) : 4 (빙어) / 2.25 : 1 # 데이터가 작아 비율을 동일하게 맞출 수 없지만 꽤 올바른 샘플링 print(test_target) . [0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1.] . &#49688;&#49345;&#54620; &#46020;&#48120; &#54620; &#47560;&#47532; . [[25, 150]] 이라는 도미 데이터를 예측값으로 넣었더니 빙어로 출력이 된다. | 산점도에 있어서 직관적으로는 [[25, 150]] 데이터가 도미에 가까워 보이지만 x축의 범위와 y축의 범위를 생각하면 [[25, 150]] 데이터가 빙어데이터에 가까운것이 합리적이다. | 두 특성의 스케일이 다르면 알고리즘이 올바른 예측을 하지 못한다. --&gt; 데이터 전처리를 통해 특성값을 일정한 기준으로 맞춰준다. | . from sklearn.neighbors import KNeighborsClassifier kn = KNeighborsClassifier() kn.fit(train_input, train_target) kn.score(test_input, test_target) # [[25, 150]] 이라는 도미 데이터를 예측값으로 넣었더니 빙어[0]로 출력이 된다. print(kn.predict([[25, 150]])) . [0.] . import matplotlib.pyplot as plt plt.scatter(train_input[:, 0], train_input[:, 1]) plt.scatter(25, 150, marker = &#39;^&#39;) # 새로운 데이터를 marker = &#39;^&#39;로 지정하여 삼각형으로 표현함. --&gt; 구분하기 쉬움 plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() . distances, indexes = kn.kneighbors([[25, 150]]) # 주어진 샘플의 이웃 샘플을 따로 구분해서 산점도를 그린다. plt.scatter(train_input[ : , 0], train_input[ : , 1]) plt.scatter(25, 150 , marker = &#39;^&#39;) plt.scatter(train_input[indexes, 0], train_input[indexes, 1], marker = &#39;D&#39;) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() print(train_input[indexes]) print(train_target[indexes]) print(distances) print(indexes) # [[25, 150]]의 샘플은 도미 데이터를 1개밖에 포함하지 않는다. . [[[ 37. 1000. ] [ 41. 975. ] [ 38.5 955. ] [ 39.5 925. ] [ 36. 850. ]]] [[1. 1. 1. 1. 1.]] [[150.24524979 150.25805338 150.35871414 150.43446443 150.71926768]] [[10 35 17 4 7]] . # x축의 길이와 y축의 길이를 같게 해서 산점도를 그려보자. --&gt; xlim() = x축 범위 지정 , ylim() = y축 범위 지정 plt.scatter(train_input[ : , 0], train_input[ : , 1]) plt.scatter(25, 150 , marker = &#39;^&#39;) plt.scatter(train_input[indexes, 0], train_input[indexes, 1], marker = &#39;D&#39;) plt.xlim((0, 1000)) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() # weight 와 length 두 특성의 값이 놓인 범위가 매우 다르다. --&gt; 두 특성의 스케일이 다르다. # 두 특성의 스케일이 다르면 알고리즘이 올바르게 예측할 수 없다. --&gt; 특성값을 일정한 기준으로 맞춰주어야 한다. --&gt; 데이터 전처리를 한다. . &#54364;&#51456;&#51216;&#49688;(standard score) --&gt; &#45936;&#51060;&#53552; &#51204;&#52376;&#47532; . 각 특성값이 평균에서 표준편차의 몇 배만큼 떨어져 있는지를 나타낸다. | 평균을 빼고 표준편차로 나눈다. | 특성마다 값의 스케일이 다르므로 평균과 표준편차는 각 특성별로 계산해야 한다. | 테스트 세트를 스케일할때에도 훈련 세트의 평균과 표준편차를 이용하여 스케일 해야 한다. | . # 평균을 구하는 넘파이 함수 --&gt; np.mean() mean = np.mean(train_input, axis = 0) # 표준편차를 구하는 넘파이 함수 --&gt; np.std() std = np.std(train_input, axis = 0) mean, std . (array([ 27.29722222, 454.09722222]), array([ 9.98244253, 323.29893931])) . train_scaled = (train_input - mean) / std . plt.scatter(train_scaled[ :, 0], train_scaled[ :, 1]) plt.scatter(25, 150, marker = &#39;^&#39;) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() # 새로운 데이터는 전처리가 안돼있어서 따로 멀리 떨어져나옴 --&gt; 새로운 데이터도 전처리 . new = ([25, 150] - mean) / std plt.scatter(train_scaled[ :, 0], train_scaled[ :, 1]) plt.scatter(new[0], new[1], marker = &#39;^&#39;) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() #새로운 데이터도 올바르게 표현됨 . kn.fit(train_scaled, train_target) . KNeighborsClassifier(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;, metric_params=None, n_jobs=None, n_neighbors=5, p=2, weights=&#39;uniform&#39;) . test_scaled = (test_input - mean) / std . kn.score(test_scaled, test_target) . 1.0 . print(kn.predict([new])) # 수상한 도미 한 마리와 가장 근접한 이웃 데이터 확인하기 distances, indexes = kn.kneighbors([new]) plt.scatter(train_scaled[ :, 0], train_scaled[ :, 1]) plt.scatter(new[0], new[1], marker = &#39;^&#39;) plt.scatter(train_scaled[indexes, 0], train_scaled[indexes, 1], marker = &#39;D&#39;) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() . [1.] . &#50672;&#49845; . fish_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0, 9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0] fish_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0, 6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9] # train 세트 test 세트 분류 import numpy as np fish_data = np.column_stack((fish_length, fish_weight)) fish_target = np.concatenate((np.ones(35), np.zeros(14))) from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(fish_data, fish_target, stratify = fish_target, random_state = 42) # 데이터 전처리 mean = np.mean(train_input, axis = 0) std = np.std(train_input, axis = 0) train_scaled = (train_input - mean) / std test_scaled = (test_input - mean) / std # 모델링 from sklearn.neighbors import KNeighborsClassifier kn = KNeighborsClassifier() kn.fit(train_scaled, train_target) kn.score(test_scaled, test_target) # 수상한 도미 한 마리 --&gt; new = [25,150] new = [25, 150] scaled_new = ([25, 150] - mean) / std distances, indexes = kn.kneighbors([scaled_new]) import matplotlib.pyplot as plt plt.scatter(train_scaled[ :, 0], train_scaled[ :, 1]) plt.scatter(scaled_new[0], scaled_new[1], marker = &#39;^&#39;) plt.scatter(train_scaled[indexes, 0], train_scaled[indexes, 1]) plt.show() kn.predict([scaled_new]) . array([1.]) .",
            "url": "https://parkjeongung.github.io/Ung.github.io/2021/12/26/%EB%8D%B0%EC%9D%B4%ED%84%B0_%EC%A0%84%EC%B2%98%EB%A6%AC.html",
            "relUrl": "/2021/12/26/%EB%8D%B0%EC%9D%B4%ED%84%B0_%EC%A0%84%EC%B2%98%EB%A6%AC.html",
            "date": " • Dec 26, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "PolynomialFeatures // 릿지 Ridge // 라쏘 Lasso",
            "content": ". import pandas as pd df = pd.read_csv(&#39;https://bit.ly/perch_csv_data&#39;) # 판다스로 준비된 데이터 넘파이 배열로 바꾸기 --&gt; to_numpy() perch_full = df.to_numpy() import numpy as np perch_weight = np.array([5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0, 115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0, 150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0, 218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0, 556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0, 850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0, 1000.0]) . from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(perch_full, perch_weight, random_state = 42) . PolynomialFeatures . 사이킷런은 특성을 만들거나 전처리하기 위한 다양한 클래스를 제공한다. | . from sklearn.preprocessing import PolynomialFeatures poly = PolynomialFeatures() poly.fit([[2, 3]]) print(poly.transform([[2, 3]])) poly = PolynomialFeatures(include_bias = False) # include_bias = False 로 배열에서 1을 제거한다. 사실 굳이 지정하지 않아도 사이킷런 모델은 1을 무시함. 문과라 평면을 안배워서 뭔말인지 잘모르겠으니까 스킵 poly.fit([[2, 3]]) print(poly.transform([[2, 3]])) # PolynomialFeatures 적용 poly = PolynomialFeatures(include_bias = False) poly.fit(train_input) train_poly = poly.transform(train_input) print(train_poly.shape) print(train_input.shape) . [[1. 2. 3. 4. 6. 9.]] [[2. 3. 4. 6. 9.]] (42, 9) (42, 3) . poly.get_feature_names() . [&#39;x0&#39;, &#39;x1&#39;, &#39;x2&#39;, &#39;x0^2&#39;, &#39;x0 x1&#39;, &#39;x0 x2&#39;, &#39;x1^2&#39;, &#39;x1 x2&#39;, &#39;x2^2&#39;] . test_poly = poly.transform(test_input) # 다중 회귀 모델 훈련 from sklearn.linear_model import LinearRegression lr = LinearRegression() lr.fit(train_poly, train_target) print(lr.score(train_poly, train_target)) print(lr.score(test_poly, test_target)) # 여러개의 특성을 사용하니 과소적합도 해결되고 점수도 좋게 나온다. . 0.9903183436982124 0.9714559911594132 . poly = PolynomialFeatures(degree = 5, include_bias = False) poly.fit(train_input) train_poly = poly.transform(train_input) test_poly = poly.transform(test_input) print(train_poly.shape, test_poly.shape) # 특성이 55개 # 추가된 특성으로 모델 훈련 from sklearn.linear_model import LinearRegression lr = LinearRegression() lr.fit(train_poly, train_target) print(lr.score(train_poly, train_target)) # 거의 완벽한 점수 print(lr.score(test_poly, test_target)) # 음수? --&gt; 특성을 너무 많이 늘리면 train 세트에 과대적합 되므로 테스트 세트의 점수가 굉장히 낮게 나온다. . (42, 55) (14, 55) 0.9999999999991096 -144.40579242335605 . &#44508;&#51228; . 모델이 훈련 세트를 너무 과도하게 학습하지 못하도록 하는 것 --&gt; 훈련세트에 과대적합되지 않도록 하는 것 | 선형 회귀 모델의 경우 특성에 곱해지는 계수의 크기를 작게 만드는 일 | . from sklearn.preprocessing import StandardScaler ss = StandardScaler() ss.fit(train_poly) train_scaled = ss.transform(train_poly) test_scaled = ss.transform(test_poly) . &#47551;&#51648; &#46972;&#50136; // &#49440;&#54805; &#54924;&#44480; &#47784;&#45944;&#50640; &#44508;&#51228;&#47484; &#52628;&#44032;&#54620; &#47784;&#45944; . 릿지 회귀 --&gt; 계수를 제곱한 값을 기준으로 규제를 적용 | 라쏘 회귀 --&gt; 계수의 절댓값을 기준으로 규제를 적용 | from sklearn.linear_model import Ridge ridge = Ridge() ridge.fit(train_scaled, train_target) print(ridge.score(train_scaled, train_target)) print(ridge.score(test_scaled, test_target)) # 많은 특성을 사용했는데도 train세트에 너무 과대적합되지 않음 . 0.9896101671037343 0.9790693977615398 . # alpha 와 같이 사람이 직접 알려줘야 하는 파라미터를 &quot;하이퍼파라미터&quot; 라고 한다. # 적절한 alpha 값 찾는 방법 --&gt; alpha 값에 대한 R^2 그래프 그리기 --&gt; train세트와 test세트의 점수가 가장 가까운 지점이 최적의 alpha값 import matplotlib.pyplot as plt # 리스트 만들기 train_score = [] test_score = [] alpha_list = [0.001, 0.01, 0.1, 1, 10, 100] # 반복문으로 alpha 값 돌리기 for alpha in alpha_list: ridge = Ridge(alpha = alpha) ridge.fit(train_scaled, train_target) train_score.append(ridge.score(train_scaled, train_target)) test_score.append(ridge.score(test_scaled, test_target)) # 그래프 그리기 plt.plot(np.log10(alpha_list), train_score) plt.plot(np.log10(alpha_list), test_score) plt.xlabel(&#39;alpha&#39;) plt.ylabel(&#39;R^2&#39;) plt.show() # train 스코어가 가장 높고 test 세트의 점수가 가장 높은 -1 = 10^-1 = 0.1 --&gt; alpha = 0.1로 하여 최종 모델을 훈련해야 가장 좋은 모델 . from sklearn.linear_model import Ridge ridge = Ridge(alpha = 0.1) ridge.fit(train_scaled, train_target) print(ridge.score(train_scaled, train_target)) print(ridge.score(test_scaled, test_target)) . 0.9903815817570366 0.9827976465386927 . from sklearn.linear_model import Lasso lasso = Lasso() lasso.fit(train_scaled, train_target) print(lasso.score(train_scaled, train_target)) print(lasso.score(test_scaled, test_target)) . 0.9897898972080961 0.9800593698421883 . import matplotlib.pyplot as plt from sklearn.linear_model import Lasso train_score = [] test_score = [] alpha_list = [0.001, 0.01, 0.1, 1, 10, 100] for alpha in alpha_list: lasso = Lasso(alpha = alpha) lasso.fit(train_scaled, train_target) train_score.append(lasso.score(train_scaled, train_target)) test_score.append(lasso.score(test_scaled, test_target)) . /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 23364.075969939808, tolerance: 518.2793833333334 positive) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 20251.975097475122, tolerance: 518.2793833333334 positive) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 806.2370926333242, tolerance: 518.2793833333334 positive) . import matplotlib.pyplot as plt from sklearn.linear_model import Lasso train_score = [] test_score = [] alpha_list = [0.001, 0.01, 0.1, 1, 10, 100] for alpha in alpha_list: lasso = Lasso(alpha = alpha, max_iter = 10000) # 라쏘 모델은 최적의 계수를 찾기 위해 반복적인 계산을 수행하는데 지정한 횟수가 부족할 때 이런 경고가 발생 --&gt; max_iter 를 늘려줌으로써 해결 lasso.fit(train_scaled, train_target) train_score.append(lasso.score(train_scaled, train_target)) test_score.append(lasso.score(test_scaled, test_target)) plt.plot(np.log10(alpha_list), train_score) plt.plot(np.log10(alpha_list), test_score) plt.xlabel(&#39;alpha&#39;) plt.ylabel(&#39;R^2&#39;) plt.show() # 최적의 알파값은 1 = 10 --&gt; alpha = 10 으로 하여 모델 훈련 . /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 18778.697957792876, tolerance: 518.2793833333334 positive) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 12972.821345404844, tolerance: 518.2793833333334 positive) . from sklearn.linear_model import Lasso lasso = Lasso(alpha = 10) lasso.fit(train_scaled, train_target) print(lasso.score(train_scaled, train_target)) print(lasso.score(test_scaled, test_target)) . 0.9888067471131867 0.9824470598706695 . # 라쏘 모델의 계수는 coef_ 에 저장되어 있다. print(lasso.coef_) # np.sum 은 배열을 모두 더한 값을 반환한다 # 넘파이 배열에 비교연산자를 사용하면 각 원소는 True = 1 아니면 False = 0 가 된다. print(np.sum(lasso.coef_ == 0)) # 총 55개의 특성 중 40개의 특성의 계수를 0으로 만듬 --&gt; 15개의 특성만 사용 . [ 0. 0. 0. 12.14852453 55.44856399 42.23100799 0. 0. 13.70596191 0. 43.2185952 5.7033775 47.46254536 7.42309425 11.85823365 0. 0. 13.53038193 21.22111356 0. 0. 0. 0. 0. 0. 18.66993032 0. 0. 0. 15.81041778 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 18.14672268 0. 0. 0. 0. 15.51272953 0. 0. 0. 0. 0. ] 40 . &#50672;&#49845; . import pandas as pd df = pd.read_csv(&#39;https://bit.ly/perch_csv_data&#39;) perch_full = df.to_numpy() import numpy as np perch_weight = np.array([5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0, 115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0, 150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0, 218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0, 556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0, 850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0, 1000.0]) from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(perch_full, perch_weight, random_state = 42) from sklearn.preprocessing import PolynomialFeatures poly = PolynomialFeatures(degree = 5, include_bias = False) poly.fit(train_input) train_poly = poly.transform(train_input) test_poly = poly.transform(test_input) from sklearn.linear_model import LinearRegression lr = LinearRegression() lr.fit(train_poly, train_target) print(lr.score(train_poly, train_target)) print(lr.score(test_poly, test_target)) from sklearn.preprocessing import StandardScaler ss = StandardScaler() ss.fit(train_poly) train_scaled = ss.transform(train_poly) test_scaled = ss.transform(test_poly) from sklearn.linear_model import Ridge ridge = Ridge() ridge.fit(train_scaled, train_target) print(ridge.score(train_scaled, train_target)) print(ridge.score(test_scaled, test_target)) train_score = [] test_score = [] import matplotlib.pyplot as plt alpha_list = [0.001, 0.01, 0.1, 1, 10, 100] for alpha in alpha_list: ridge = Ridge(alpha = alpha) ridge.fit(train_scaled, train_target) train_score.append(ridge.score(train_scaled, train_target)) test_score.append(ridge.score(test_scaled, test_target)) plt.plot(np.log10(alpha_list), train_score) plt.plot(np.log10(alpha_list), test_score) plt.xlabel(&#39;alpha&#39;) plt.ylabel(&#39;R^2&#39;) plt.show() from sklearn.linear_model import Lasso lasso = Lasso() lasso.fit(train_scaled, train_target) print(lasso.score(train_scaled, train_target)) print(lasso.score(test_scaled, test_target)) train_score = [] test_score = [] alpha_list = [0.001, 0.01, 0.1, 1, 10, 100] for alpha in alpha_list: lasso = Lasso(alpha = alpha) lasso.fit(train_scaled, train_target) train_score.append(lasso.score(train_scaled, train_target)) test_score.append(lasso.score(test_scaled, test_target)) plt.plot(np.log10(alpha_list), train_score) plt.plot(np.log10(alpha_list), test_score) plt.xlabel(&#39;alpha&#39;) plt.ylabel(&#39;R^2&#39;) plt.show() . 0.9999999999991096 -144.40579242335605 0.9896101671037343 0.9790693977615398 . 0.9897898972080961 0.9800593698421883 . /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 23364.075969939808, tolerance: 518.2793833333334 positive) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 20251.975097475122, tolerance: 518.2793833333334 positive) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 806.2370926333242, tolerance: 518.2793833333334 positive) .",
            "url": "https://parkjeongung.github.io/Ung.github.io/2021/12/26/PolynomialFeatures_%EB%A6%BF%EC%A7%80_Ridge_%EB%9D%BC%EC%8F%98_Lasso.html",
            "relUrl": "/2021/12/26/PolynomialFeatures_%EB%A6%BF%EC%A7%80_Ridge_%EB%9D%BC%EC%8F%98_Lasso.html",
            "date": " • Dec 26, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Title",
            "content": ". K - &#52572;&#44540;&#51217;&#51060;&#50883; // from sklearn.neighbors import KNeighborsClassifier . 내가 지정한 데이터와 그 주변에 있는 데이터를 비교해서 결과를 예측한다. | 새로운 데이터를 예측할 때 그 주변 직선거리로 가장 가까운 데이터와 비교한다. | 데이터가 아주 많은 경우 사용하기 어렵다. 직선거리를 계산하는데 많은 시간이 들기 때문이다. | 참고 데이터 기본값은 5이다. n_neighbors로 변경 가능하다 | bream_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0] bream_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0] smelt_length = [9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0] smelt_weight = [6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9] import matplotlib.pyplot as plt plt.scatter(bream_length, bream_weight) plt.scatter(smelt_length, smelt_weight) plt.xlabel(&#39;bream_length&#39;) plt.ylabel(&#39;bream_weight&#39;) plt.show() . bream_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0] bream_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0] smelt_length = [9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0] smelt_weight = [6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9] length = bream_length + smelt_length weight = bream_weight + smelt_weight . fish_data = [[l, w] for l, w in zip(length, weight)] . fish_target = [1] * 35 + [0] * 14 . from sklearn.neighbors import KNeighborsClassifier kn = KNeighborsClassifier() . kn.fit(fish_data, fish_target) . KNeighborsClassifier(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;, metric_params=None, n_jobs=None, n_neighbors=5, p=2, weights=&#39;uniform&#39;) . kn.score(fish_data, fish_target) . kn.predict([[30, 600]]) . kn49 = KNeighborsClassifier(n_neighbors=49) kn49.fit(fish_data, fish_target) kn49.score(fish_data, fish_target) . 0.7142857142857143 . &#50672;&#49845; . bream_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0] bream_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0] smelt_length = [9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0] smelt_weight = [6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9] length = bream_length + smelt_length weight = bream_weight + smelt_weight fish_data = [[l,w] for l,w in zip(length, weight)] fish_target = [1] * 35 + [0] * 14 from sklearn.neighbors import KNeighborsClassifier kn = KNeighborsClassifier() kn.fit(fish_data, fish_target) kn.score(fish_data, fish_target) kn.predict([[30, 600]]) . array([1]) .",
            "url": "https://parkjeongung.github.io/Ung.github.io/2021/12/26/K_%EC%B5%9C%EA%B7%BC%EC%A0%91%EC%9D%B4%EC%9B%83_KNeighborsClassifier.html",
            "relUrl": "/2021/12/26/K_%EC%B5%9C%EA%B7%BC%EC%A0%91%EC%9D%B4%EC%9B%83_KNeighborsClassifier.html",
            "date": " • Dec 26, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "K - 최근접 이웃 회귀 // KNeighborsRegressor",
            "content": ". &#54924;&#44480; . 임의의 어떤 숫자를 예측 | 두 변수 사이의 상관관계를 분석하는 방법 | . import numpy as np # 농어의 특성 = 길이 perch_length = np.array([8.4, 13.7, 15.0, 16.2, 17.4, 18.0, 18.7, 19.0, 19.6, 20.0, 21.0, 21.0, 21.0, 21.3, 22.0, 22.0, 22.0, 22.0, 22.0, 22.5, 22.5, 22.7, 23.0, 23.5, 24.0, 24.0, 24.6, 25.0, 25.6, 26.5, 27.3, 27.5, 27.5, 27.5, 28.0, 28.7, 30.0, 32.8, 34.5, 35.0, 36.5, 36.0, 37.0, 37.0, 39.0, 39.0, 39.0, 40.0, 40.0, 40.0, 40.0, 42.0, 43.0, 43.0, 43.5, 44.0]) # 농어의 타겟 = 무게 perch_weight = np.array([5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0, 115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0, 150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0, 218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0, 556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0, 850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0, 1000.0]) . import matplotlib.pyplot as plt plt.scatter(perch_length, perch_weight) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() # 농어의 길이가 길어질수록 무게가 늘어난다. . from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(perch_length, perch_weight, random_state = 42) . # reshape 메서드는 크기에 -1 을 지정하면 남은 원소 개수로 모두 채우라는 의미 train_input = train_input.reshape(-1, 1) # == train_input.reshape(42, 1) print(train_input.shape) test_input = test_input.reshape(-1, 1) print(test_input.shape) . (42, 1) (14, 1) . from sklearn.neighbors import KNeighborsRegressor knr = KNeighborsRegressor() knr.fit(train_input, train_target) print(knr.score(test_input, test_target)) # 스코어가 1이 안나왔다 --&gt; 회귀에서는 정확한 숫자를 맞힌다는 것이 거의 불가능하다. --&gt; 예측하는 값이 모두 임의의 수치이기 때문 . 0.9928094061010639 . from sklearn.metrics import mean_absolute_error # 테스트 세트의 예측값을 만든다. test_prediction = knr.predict(test_input) # 테스트 세트에 대한 절댓값 오차의 평균을 구한다. mae = mean_absolute_error(test_target, test_prediction) mae # 19그람 정도의 오차가 발생한다. . 19.157142857142862 . &#54984;&#47144; &#49464;&#53944;&#47484; &#49324;&#50857;&#54644; &#47784;&#45944; &#54217;&#44032;&#54616;&#44592; // &#44284;&#49548;&#51201;&#54633;, &#44284;&#45824;&#51201;&#54633; . score(train) &gt; score(test) --&gt; 모델이 훈련세트에 과대적합 되었다. --&gt; 실전에 투입하면 예측이 잘 안됨 --&gt; 모델을 덜 복잡하게 만들어 해결한다. --&gt; 이웃의 개수를 늘린다 | score(train) &lt; score(test) --&gt; 모델이 훈련세트에 과소적합 되었다. --&gt; 모델이 너무 단순하다. --&gt; 모델을 더 복잡하게 만들어 해결한다. --&gt; 이웃의 개수를 줄인다. | . print(knr.score(train_input, train_target)) # 테스트 세트로 점수 확인하기 print(knr.score(test_input, test_target)) # score(train) &lt; score(test) --&gt; 과소적합 --&gt; 데이터가 작을경우 과소적합이 발생할 수 있다. . 0.9698823289099255 0.9928094061010639 . knr.n_neighbors = 3 knr.fit(train_input, train_target) print(knr.score(train_input, train_target)) print(knr.score(test_input, test_target)) # 이웃의 개수를 줄였더니 train 세트와 test 세트의 score가 비슷하게 나온다. . 0.9804899950518966 0.974645996398761 . &#50672;&#49845; . import numpy as np perch_length = np.array([8.4, 13.7, 15.0, 16.2, 17.4, 18.0, 18.7, 19.0, 19.6, 20.0, 21.0, 21.0, 21.0, 21.3, 22.0, 22.0, 22.0, 22.0, 22.0, 22.5, 22.5, 22.7, 23.0, 23.5, 24.0, 24.0, 24.6, 25.0, 25.6, 26.5, 27.3, 27.5, 27.5, 27.5, 28.0, 28.7, 30.0, 32.8, 34.5, 35.0, 36.5, 36.0, 37.0, 37.0, 39.0, 39.0, 39.0, 40.0, 40.0, 40.0, 40.0, 42.0, 43.0, 43.0, 43.5, 44.0]) # 농어의 타겟 = 무게 perch_weight = np.array([5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0, 115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0, 150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0, 218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0, 556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0, 850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0, 1000.0]) # train test split from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(perch_length, perch_weight, random_state = 42) # 2차원 배열로 변형 train_input = train_input.reshape(-1, 1) test_input = test_input.reshape(-1, 1) # 모델링 from sklearn.neighbors import KNeighborsRegressor knr = KNeighborsRegressor() knr.fit(train_input, train_target) # 스코어 knr.score(test_input, test_target) # 오차확인 mean absoulte error from sklearn.metrics import mean_absolute_error test_prediction = knr.predict(test_input) mae = mean_absolute_error(test_target, test_prediction) # 과소적합 --&gt; 테스트세트 스코어가 트레인세트 스코어보다 높다 print(knr.score(train_input, train_target)) print(knr.score(test_input, test_target)) # 과소적합 해결 --&gt; 모델을 복잡하게 만든다. --&gt; 이웃수 줄이기 knr.n_neighbors = 3 knr.fit(train_input, train_target) print(knr.score(train_input, train_target)) print(knr.score(test_input, test_target)) . 0.9698823289099255 0.9928094061010639 0.9804899950518966 0.974645996398761 . # n_neighbors = [1, 5, 10] # 농어의 길이를 5에서 45까지 바꿔가며 모델링 결과 확인하기 from sklearn.neighbors import KNeighborsRegressor import matplotlib.pyplot as plt knr = KNeighborsRegressor() x = np.arange(5, 45).reshape(-1, 1) for n in [1, 5, 10]: knr.n_neighbors = n knr.fit(train_input, train_target) predict = knr.predict(x) plt.scatter(train_input, train_target) plt.scatter(x, predict, marker = &#39;^&#39;) plt.title(&quot;n_neighbors = {}&quot;.format(n)) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() .",
            "url": "https://parkjeongung.github.io/Ung.github.io/2021/12/26/K_%EC%B5%9C%EA%B7%BC%EC%A0%91_%EC%9D%B4%EC%9B%83_%ED%9A%8C%EA%B7%80_KNeighborsRegressor.html",
            "relUrl": "/2021/12/26/K_%EC%B5%9C%EA%B7%BC%EC%A0%91_%EC%9D%B4%EC%9B%83_%ED%9A%8C%EA%B7%80_KNeighborsRegressor.html",
            "date": " • Dec 26, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://parkjeongung.github.io/Ung.github.io/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://parkjeongung.github.io/Ung.github.io/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://parkjeongung.github.io/Ung.github.io/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}