{
  
    
        "post0": {
            "title": "주성분 분석 // PCA",
            "content": ". &#52264;&#50896; &#52629;&#49548; . 3차원을 2차원으로 줄이는 개념이 아니라 특성의 갯수를 줄인다는 의미 | . &#51452;&#49457;&#48516; . 데이터를 가장 잘 표현하는 벡터를 찾는다 --&gt; 분산이 큰방향을 찾는다 | 찾은 벡터(주성분)를 원점에 맞춘다 | 샘플을 주성분에 투영한다 --&gt; S(4, 2) x1 = 4, x2 = 2 인 샘플을 P(4.5) 하나로 나타내어 차원을 축소시킴 | 찾은 벡터에 수직이고 분산이 가장 큰 다음 벡터(주성분)를 찾는다 --&gt; 수직인 벡터를 찾는 이유는 처음 찾았던 주성분이 표현하지 못하는 주성분을 찾기위해서임 | . !wget https://bit.ly/fruits_300_data -O fruits_300.npy import numpy as np fruits = np.load(&#39;fruits_300.npy&#39;) fruits_2d = fruits.reshape(-1, 100 * 100) . --2021-12-16 06:48:09-- https://bit.ly/fruits_300_data Resolving bit.ly (bit.ly)... 67.199.248.10, 67.199.248.11 Connecting to bit.ly (bit.ly)|67.199.248.10|:443... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://github.com/rickiepark/hg-mldl/raw/master/fruits_300.npy [following] --2021-12-16 06:48:09-- https://github.com/rickiepark/hg-mldl/raw/master/fruits_300.npy Resolving github.com (github.com)... 140.82.114.4 Connecting to github.com (github.com)|140.82.114.4|:443... connected. HTTP request sent, awaiting response... 302 Found Location: https://raw.githubusercontent.com/rickiepark/hg-mldl/master/fruits_300.npy [following] --2021-12-16 06:48:09-- https://raw.githubusercontent.com/rickiepark/hg-mldl/master/fruits_300.npy Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 3000128 (2.9M) [application/octet-stream] Saving to: ‘fruits_300.npy’ fruits_300.npy 100%[===================&gt;] 2.86M --.-KB/s in 0.07s 2021-12-16 06:48:09 (41.8 MB/s) - ‘fruits_300.npy’ saved [3000128/3000128] . from sklearn.decomposition import PCA pca = PCA(n_components = 50) # n_components 로 주성분의 개수를 지정 pca.fit(fruits_2d) . PCA(n_components=50) . print(pca.components_.shape) . (50, 10000) . import matplotlib.pyplot as plt def draw_fruits(arr, ratio = 1): n = len(arr) rows = int(np.ceil(n / 10)) cols = n if rows &lt; 2 else 10 fig, axs = plt.subplots(rows, cols, figsize = (cols * ratio, rows * ratio), squeeze = False) for i in range(rows): for j in range(cols): if i * 10 + j &lt; n: axs[i, j].imshow(arr[i * 10 + j], cmap = &#39;gray_r&#39;) axs[i, j].axis(&#39;off&#39;) plt.show() . draw_fruits(pca.components_.reshape(-1, 100, 100)) . # 픽셀이 총 10000개 가 있었으니 이 데이터의 특성은 총 10000개 print(fruits_2d.shape) # 투영 --&gt; transform() fruits_pca = pca.transform(fruits_2d) print(fruits_pca.shape) # 10000개였던 특성을 50개로 줄임 --&gt; 차원 축소 . (300, 10000) (300, 50) . # 하지만 상당 부분 원본데이터를 재구성 할 수 있다 # 축소시킨 차원을 다시 복원한다 --&gt; inverse_transform fruits_inverse = pca.inverse_transform(fruits_pca) print(fruits_inverse.shape) # 50개였던 특성을 다시 10000개로 늘림 . (300, 10000) . fruits_reconstruct = fruits_inverse.reshape(-1, 100, 100) for start in [0, 100, 200]: draw_fruits(fruits_reconstruct[start : 100 + start]) print(&quot; n&quot;) # 조금 번지긴 했지만 그래도 잘 복원됨 . . &#49444;&#47749;&#46108; &#48516;&#49328; . 주성분이 원본 데이터를 얼마나 잘 나타내는지를 기록한 값 | . # 이 분산비율을 모두더하면 주성분으로 표현하는 총 분산비율 print(np.sum(pca.explained_variance_ratio_)) # 92퍼센트가 넘는 분산을 유지하고 있기에 데이터를 복원했을 때 이미지가 잘 나온거임 . 0.9215667058771523 . import matplotlib.pyplot as plt plt.plot(pca.explained_variance_ratio_) plt.show() # 이 그래프는 적절한 주성분의 개수를 찾는 데 도움이 됨 # 이 데이터 같은 경우는 처음 10개의 주성분이 대부분의 분산을 표현함 . &#45796;&#47480; &#50508;&#44256;&#47532;&#51608;&#44284; &#54632;&#44760; &#49324;&#50857;&#54616;&#44592; . PCA 로 축소시킨 데이터를 지도 학습에 적용하기 | . from sklearn.linear_model import LogisticRegression lr = LogisticRegression() . # 사과(0), 파인애플(1), 바나나(2) 각각 100개씩 import numpy as np target = np.array([0] * 100 + [1] * 100 + [2] * 100) target . array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) . from sklearn.model_selection import cross_validate scores = cross_validate(lr, fruits_2d, target) print(np.mean(scores[&#39;test_score&#39;])) print(np.mean(scores[&#39;fit_time&#39;])) # 원본 데이터는 특성이 10000개이기 때문에 과대적합되기 쉬움 # fit_time 에는 교차 검증 훈련 시간이 기록됨 . 0.9966666666666667 1.886031198501587 . scores = cross_validate(lr, fruits_pca, target) print(np.mean(scores[&#39;test_score&#39;])) print(np.mean(scores[&#39;fit_time&#39;])) # 50개의 특성으로 정확도 100% 만들어냄 # 또한 훈련시간도 줄고 데이터를 줄였기에 저장 공간까지 확보함 . 1.0 0.030258846282958985 . pca = PCA(n_components = 0.5) # 설명된 분산이 50%에 달하는 주성분을 찾는다 pca.fit(fruits_2d) print(pca.n_components_) # 2개의 주성분으로 원본 데이터에 있는 분산의 50%를 표현할 수 있다 . 2 . fruits_pca = pca.transform(fruits_2d) print(fruits_pca.shape) # 주성분이 2개이므로 특성이 2개 . (300, 2) . scores = cross_validate(lr, fruits_pca, target) print(np.mean(scores[&#39;test_score&#39;])) print(np.mean(scores[&#39;fit_time&#39;])) # 2개의 특성으로 좋은 정확도 . /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG, /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG, . 0.9933333333333334 0.17764811515808104 . /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG, . from sklearn.cluster import KMeans km = KMeans(n_clusters = 3, random_state = 42) km.fit(fruits_pca) print(np.unique(km.labels_, return_counts = True)) # 원본 데이터를 K-Means 에 적용 했을때 --&gt; [111, 98, 91] . (array([0, 1, 2], dtype=int32), array([110, 99, 91])) . for label in range(0, 3): draw_fruits(fruits[km.labels_ == label]) . # PCA 한 데이터는 특성이 2개이므로 2차원으로 표현이 가능하다 # 산점도 그리기 for label in range(0, 3): data = fruits_pca[km.labels_ == label] plt.scatter(data[:, 0], data[:, 1]) plt.legend([&#39;pineapple&#39;,&#39;banana&#39;,&#39;apple&#39;]) plt.show() # 바나나는 확실히 분리되어있지만 # 파인애플과 사과의 경계는 가까움 몇 개의 샘플이 혼동을 일으킬 가능성이 큼 .",
            "url": "https://parkjeongung.github.io/Ung.github.io/2022/01/06/%EC%A3%BC%EC%84%B1%EB%B6%84_%EB%B6%84%EC%84%9D_PCA.html",
            "relUrl": "/2022/01/06/%EC%A3%BC%EC%84%B1%EB%B6%84_%EB%B6%84%EC%84%9D_PCA.html",
            "date": " • Jan 6, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "앙상블 // 랜덤 포레스트 RandomForest // 엑스트라 트리 ExtraTreesClassifier",
            "content": ". &#47004;&#45924; &#54252;&#47112;&#49828;&#53944; // RandomForest . 랜덤하게 만들어진 여러개의 결정트리가 모인 숲 | 랜덤 포레스트는 각 트리마다 훈련 세트가 다름 --&gt; 각 트리의 훈련 세트를 만들때 전체 훈련세트에서 복원추출을 시행하여 만듬 --&gt; 부트스트랩 샘플 | 랜덤하게 샘플링된 훈련 세트 덕분에 과대적합을 방지해줌 | . import numpy as np import pandas as pd from sklearn.model_selection import train_test_split wine = pd.read_csv(&#39;https://bit.ly/wine_csv_data&#39;) wine.head() . alcohol sugar pH class . 0 9.4 | 1.9 | 3.51 | 0.0 | . 1 9.8 | 2.6 | 3.20 | 0.0 | . 2 9.8 | 2.3 | 3.26 | 0.0 | . 3 9.8 | 1.9 | 3.16 | 0.0 | . 4 9.4 | 1.9 | 3.51 | 0.0 | . wine_input = wine[[&#39;alcohol&#39;, &#39;sugar&#39;, &#39;pH&#39;]].to_numpy() wine_target = wine[&#39;class&#39;].to_numpy() train_input, test_input, train_target, test_target = train_test_split(wine_input, wine_target, test_size = 0.2, random_state = 42) . from sklearn.model_selection import cross_validate from sklearn.ensemble import RandomForestClassifier rf = RandomForestClassifier(n_jobs = -1, random_state = 42) score = cross_validate(rf, train_input, train_target ,n_jobs = -1, return_train_score = True) # return_train_score = True --&gt; 훈련 세트에 대한 점수도 반환 --&gt; 과대적합을 확인할때 편함 score . {&#39;fit_time&#39;: array([0.68797898, 0.68438673, 0.69149828, 0.68736911, 0.44117284]), &#39;score_time&#39;: array([0.10288453, 0.1029191 , 0.10267925, 0.10335732, 0.10253263]), &#39;test_score&#39;: array([0.88461538, 0.88942308, 0.90279115, 0.88931665, 0.88642926]), &#39;train_score&#39;: array([0.9971133 , 0.99663219, 0.9978355 , 0.9973545 , 0.9978355 ])} . print(np.mean(score[&#39;test_score&#39;]), np.mean(score[&#39;train_score&#39;])) # 과대적합이 나타남 --&gt; 랜덤포레스트도 결정트리를 사용하는 거기 때문에 가지치기가 필요할 것으로 보임 . 0.8905151032797809 0.9973541965122431 . rf.fit(train_input, train_target) print(rf.feature_importances_) # 결정트리만 사용했을 때의 feature_importances_ 결과 값 --&gt; [0.12345626 0.86862934 0.0079144] # 랜덤포레스트는 각 트리마다 전체 특성 개수의 제곱근만큼의 특성을 선택함 ex) 1번 트리 alcohol, sugar / 2번 트리 sugar,pH # 각 트리마다 특성의 일부만 훈련에 사용하기 때문에 하나의 특성에 과도하게 집중하지 않고 많은 특성을 사용할 수 있음 --&gt; 과대적합을 줄임 . [0.23167441 0.50039841 0.26792718] . # 복원추출을 했기 때문에 부트스트랩 샘플로 뽑히지 않은 샘플들이 존재 --&gt; OOB 샘플 # 이런 OOB 샘플을 이용해 부트스트랩 샘플로 훈련한 결정트리를 평가 할 수 있음 --&gt; oob_score = True --&gt; OOB 샘플이 검증세트가 되어 교차검증을 하는 느낌 rf = RandomForestClassifier(oob_score = True, random_state = 42, n_jobs = -1) rf.fit(train_input, train_target) print(rf.oob_score_) # cross_validate 로 확인한 점수와 비슷하게 나옴 . 0.8934000384837406 . &#50672;&#49845; . import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import cross_validate wine = pd.read_csv(&#39;https://bit.ly/wine_csv_data&#39;) wine.head() wine_input = wine[[&#39;alcohol&#39;, &#39;sugar&#39;, &#39;pH&#39;]].to_numpy() wine_target = wine[&#39;class&#39;].to_numpy() train_input, test_input, train_target, test_target = train_test_split(wine_input, wine_target, test_size = 0.2, random_state = 42) score = cross_validate(RandomForestClassifier(random_state = 42), train_input, train_target, n_jobs = -1, return_train_score = True) print(np.mean(score[&#39;test_score&#39;]), np.mean(score[&#39;train_score&#39;])) rf = RandomForestClassifier(random_state = 42, n_jobs = -1, oob_score = True) rf.fit(train_input, train_target) print(rf.oob_score_) . 0.8905151032797809 0.9973541965122431 0.8934000384837406 . &#50641;&#49828;&#53944;&#46972; &#53944;&#47532; . 부트스트랩 샘플을 사용하지 않는다 --&gt; 알고리즘 훈련 때 전체훈련세트 이용 | 노드를 분할할 때 무작위로 분할한다 | 무작위성 때문에 속도가 빠르다 | . from sklearn.ensemble import ExtraTreesClassifier et = ExtraTreesClassifier(n_jobs = -1, random_state = 42) scores = cross_validate(et, train_input, train_target, return_train_score = True, n_jobs = -1) print(np.mean(score[&#39;train_score&#39;]), np.mean(score[&#39;test_score&#39;])) . 0.9973541965122431 0.8905151032797809 . et.fit(train_input, train_target) print(et.feature_importances_) . [0.20183568 0.52242907 0.27573525] .",
            "url": "https://parkjeongung.github.io/Ung.github.io/2022/01/06/%EC%95%99%EC%83%81%EB%B8%94_%EB%9E%9C%EB%8D%A4_%ED%8F%AC%EB%A0%88%EC%8A%A4%ED%8A%B8_RandomForest_%EC%97%91%EC%8A%A4%ED%8A%B8%EB%9D%BC_%ED%8A%B8%EB%A6%AC_ExtraTreesClassifier.html",
            "relUrl": "/2022/01/06/%EC%95%99%EC%83%81%EB%B8%94_%EB%9E%9C%EB%8D%A4_%ED%8F%AC%EB%A0%88%EC%8A%A4%ED%8A%B8_RandomForest_%EC%97%91%EC%8A%A4%ED%8A%B8%EB%9D%BC_%ED%8A%B8%EB%A6%AC_ExtraTreesClassifier.html",
            "date": " • Jan 6, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "따릉이 대여량 예측 // 모델링 연습",
            "content": ". &#45936;&#51060;&#53552; &#48520;&#47084;&#50724;&#44592; . 학습에 필요한 데이터 불러오기 . date_time : 일별 날짜 | wind_direction: 풍향 (degree) | sky_condition : 하늘 상태 (하단 설명 참조) | precipitation_form : 강수 형태 (하단 설명 참조) | wind_speed : 풍속 (m/s) | humidity : 습도 (%) | low_temp : 최저 기온 ( `C) | high_temp : 최고 기온 ( `C) | Precipitation_Probability : 강수 확률 (%) | 기상 데이터는 하루에 총 8번 3시간 간격으로 발표되는 기상단기예보(SHRT) 데이터를 1일 평균으로 변환한 데이터입니다. | sky_condition (하늘 상태) 코드 : 맑음(1), 구름많음(3), 흐림(4) | precipitation_form (강수 형태) 코드 : 없음(0), 비(1), 진눈깨비(2), 눈(3), 소나기(4) | 위 데이터는 4월~6월의 기상 데이터만을 추출한 것이기 때문에 없음(0), 비(1)만 등장했습니다. 따라서 precipitation_form 이 0.5인 경우: &quot;하루의 절반은 비가 올 것으로 예측하고, 나머지 절반은 맑을 것으로 예측했다&quot;는 의미로 해석해주시기 바랍니다. | . import pandas as pd import numpy as np train = pd.read_csv(&#39;/content/drive/MyDrive/mydata/train.csv&#39;) train.head().T . 0 1 2 3 4 . date_time 2018-04-01 | 2018-04-02 | 2018-04-03 | 2018-04-04 | 2018-04-05 | . wind_direction 207.5 | 208.317 | 213.516 | 143.836 | 95.905 | . sky_condition 4 | 2.95 | 2.911 | 3.692 | 4 | . precipitation_form 0 | 0 | 0 | 0.425 | 0.723 | . wind_speed 3.05 | 3.278 | 2.69 | 3.138 | 3.186 | . humidity 75 | 69.833 | 74.879 | 71.849 | 73.784 | . low_temp 12.6 | 12.812 | 10.312 | 8.312 | 5.875 | . high_temp 21 | 19 | 15.316 | 12.368 | 10.421 | . Precipitation_Probability 30 | 19.5 | 19.113 | 43.493 | 63.378 | . number_of_rentals 22994 | 28139 | 26817 | 26034 | 2833 | . test = pd.read_csv(&#39;/content/drive/MyDrive/mydata/test.csv&#39;) submission = pd.read_csv(&#39;/content/drive/MyDrive/mydata/sample_submission.csv&#39;) # 테스트 데이터 결측치 확인 print(test.info()) print(test.shape) . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 91 entries, 0 to 90 Data columns (total 9 columns): # Column Non-Null Count Dtype -- -- 0 date_time 91 non-null object 1 wind_direction 91 non-null float64 2 sky_condition 91 non-null float64 3 precipitation_form 91 non-null float64 4 wind_speed 91 non-null float64 5 humidity 91 non-null float64 6 low_temp 91 non-null float64 7 high_temp 91 non-null float64 8 Precipitation_Probability 91 non-null float64 dtypes: float64(8), object(1) memory usage: 6.5+ KB None (91, 9) . &#45936;&#51060;&#53552; &#54869;&#51064; . train.info() print(train.shape) . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 273 entries, 0 to 272 Data columns (total 9 columns): # Column Non-Null Count Dtype -- -- 0 date_time 273 non-null object 1 sky_condition 273 non-null float64 2 precipitation_form 273 non-null float64 3 wind_speed 273 non-null float64 4 humidity 273 non-null float64 5 low_temp 273 non-null float64 6 high_temp 273 non-null float64 7 Precipitation_Probability 273 non-null float64 8 number_of_rentals 273 non-null int64 dtypes: float64(7), int64(1), object(1) memory usage: 19.3+ KB (273, 9) . def check_missing_col(dataframe) : counted_missing_col = 0 for i, col in enumerate(train.columns) : missing_values = sum(train[col].isna()) is_missing = True if missing_values &gt;= 1 else False if is_missing: counted_missing_col += 1 . train.describe().T . count mean std min 25% 50% 75% max . sky_condition 273.0 | 2.288256 | 0.961775 | 1.000 | 1.405 | 2.167 | 3.000 | 4.000 | . precipitation_form 273.0 | 0.100963 | 0.203193 | 0.000 | 0.000 | 0.000 | 0.088 | 1.000 | . wind_speed 273.0 | 2.480963 | 0.884397 | 0.753 | 1.820 | 2.411 | 2.924 | 5.607 | . humidity 273.0 | 56.745491 | 12.351268 | 24.831 | 47.196 | 55.845 | 66.419 | 88.885 | . low_temp 273.0 | 13.795249 | 5.107711 | 1.938 | 9.938 | 14.375 | 18.000 | 22.312 | . high_temp 273.0 | 23.384733 | 5.204605 | 9.895 | 19.842 | 24.158 | 27.526 | 33.421 | . Precipitation_Probability 273.0 | 16.878103 | 16.643772 | 0.000 | 4.054 | 12.162 | 22.973 | 82.162 | . number_of_rentals 273.0 | 59574.978022 | 27659.575774 | 1037.000 | 36761.000 | 63032.000 | 81515.000 | 110377.000 | . &#51068;&#51088; &#48516;&#47532; . 글자형이나 문자형을 숫자형으로 바꿔준다. . year = [] month = [] day = [] for date in train[&#39;date_time&#39;]: y_point, m_point, d_point = date.split(&#39;-&#39;) year.append(int(y_point)) month.append(int(m_point)) day.append(int(d_point)) train[&#39;year&#39;] = year train[&#39;month&#39;] = month train[&#39;day&#39;] = day train.head() . date_time wind_direction sky_condition precipitation_form wind_speed humidity low_temp high_temp Precipitation_Probability number_of_rentals year month day . 0 2018-04-01 | 207.500 | 4.000 | 0.000 | 3.050 | 75.000 | 12.600 | 21.000 | 30.000 | 22994 | 2018 | 4 | 1 | . 1 2018-04-02 | 208.317 | 2.950 | 0.000 | 3.278 | 69.833 | 12.812 | 19.000 | 19.500 | 28139 | 2018 | 4 | 2 | . 2 2018-04-03 | 213.516 | 2.911 | 0.000 | 2.690 | 74.879 | 10.312 | 15.316 | 19.113 | 26817 | 2018 | 4 | 3 | . 3 2018-04-04 | 143.836 | 3.692 | 0.425 | 3.138 | 71.849 | 8.312 | 12.368 | 43.493 | 26034 | 2018 | 4 | 4 | . 4 2018-04-05 | 95.905 | 4.000 | 0.723 | 3.186 | 73.784 | 5.875 | 10.421 | 63.378 | 2833 | 2018 | 4 | 5 | . year = [] month = [] day = [] for date in test[&#39;date_time&#39;]: y_point, m_point, d_point = date.split(&#39;-&#39;) year.append(int(y_point)) month.append(int(m_point)) day.append(int(d_point)) test[&#39;year&#39;] = year test[&#39;month&#39;] = month test[&#39;day&#39;] = day test.head() . date_time wind_direction sky_condition precipitation_form wind_speed humidity low_temp high_temp Precipitation_Probability year month day . 0 2021-04-01 | 108.833 | 3.000 | 0.000 | 2.900 | 28.333 | 11.800 | 20.667 | 18.333 | 2021 | 4 | 1 | . 1 2021-04-02 | 116.717 | 3.850 | 0.000 | 2.662 | 46.417 | 12.000 | 19.000 | 28.500 | 2021 | 4 | 2 | . 2 2021-04-03 | 82.669 | 4.000 | 0.565 | 2.165 | 77.258 | 8.875 | 16.368 | 52.847 | 2021 | 4 | 3 | . 3 2021-04-04 | 44.123 | 3.466 | 0.466 | 3.747 | 63.288 | 6.250 | 17.368 | 37.671 | 2021 | 4 | 4 | . 4 2021-04-05 | 147.791 | 1.500 | 0.000 | 1.560 | 48.176 | 7.188 | 18.684 | 4.459 | 2021 | 4 | 5 | . train = train.drop([&#39;wind_direction&#39;], axis = 1) test = test.drop([&#39;wind_direction&#39;], axis = 1) . &#47784;&#45944;&#47553; . x_train = train.drop([&#39;date_time&#39;, &#39;number_of_rentals&#39;], axis = 1) y_train = train.number_of_rentals . x_test = test.drop([&#39;date_time&#39;], axis = 1) . LinearRegression // 0.98369 . from sklearn.linear_model import LinearRegression lr = LinearRegression() lr.fit(x_train, y_train) linear_predict = lr.predict(x_train) . import matplotlib.pyplot as plt plt.style.use(&#39;ggplot&#39;) plt.figure(figsize = (20, 10)) plt.plot(linear_predict, label = &#39;prediction&#39;) plt.plot(y_train, label = &#39;real&#39;) plt.legend(fontsize = 20) plt.show() . KNeighborsRegressor . 스케일링 X = 0.82995 | 스케일링 O = 0.70720 | from sklearn.preprocessing import StandardScaler ss = StandardScaler() ss.fit(x_train) x_train_scaled = ss.transform(x_train) x_test_scaled = ss.transform(x_test) . from sklearn.neighbors import KNeighborsRegressor kn = KNeighborsRegressor() params = {&#39;n_neighbors&#39; : range(2, 150)} from sklearn.model_selection import GridSearchCV gs = GridSearchCV(kn,params, n_jobs = -1 ) gs.fit(x_train_scaled, y_train) kn = gs.best_estimator_ kn_predict = kn.predict(x_train_scaled) . import matplotlib.pyplot as plt plt.style.use(&#39;ggplot&#39;) plt.figure(figsize = (20, 10)) plt.plot(kn.predict(x_train), label = &#39;prediction&#39;) plt.plot(y_train, label = &#39;real&#39;) plt.legend(fontsize = 20) plt.show() . DecisionTreeRegressor // 0.62857 . from sklearn.model_selection import GridSearchCV from sklearn.tree import DecisionTreeRegressor params = {&#39;min_samples_split&#39; : range(2, 30), &#39;max_depth&#39; : range(10, 30), &#39;min_impurity_decrease&#39; : np.arange(0.0001, 0.0003, 0.0001) } gs = GridSearchCV(DecisionTreeRegressor(), params, n_jobs = -1) gs.fit(x_train, y_train) dt = gs.best_estimator_ . x_test . sky_condition precipitation_form wind_speed humidity low_temp high_temp Precipitation_Probability year month day . 0 3.000 | 0.000 | 2.900 | 28.333 | 11.800 | 20.667 | 18.333 | 2021 | 4 | 1 | . 1 3.850 | 0.000 | 2.662 | 46.417 | 12.000 | 19.000 | 28.500 | 2021 | 4 | 2 | . 2 4.000 | 0.565 | 2.165 | 77.258 | 8.875 | 16.368 | 52.847 | 2021 | 4 | 3 | . 3 3.466 | 0.466 | 3.747 | 63.288 | 6.250 | 17.368 | 37.671 | 2021 | 4 | 4 | . 4 1.500 | 0.000 | 1.560 | 48.176 | 7.188 | 18.684 | 4.459 | 2021 | 4 | 5 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 86 3.980 | 0.223 | 1.066 | 74.628 | 20.312 | 28.579 | 36.486 | 2021 | 6 | 26 | . 87 2.777 | 0.135 | 1.290 | 70.236 | 20.812 | 29.000 | 18.378 | 2021 | 6 | 27 | . 88 3.338 | 1.270 | 1.692 | 70.338 | 21.000 | 28.789 | 35.946 | 2021 | 6 | 28 | . 89 3.270 | 0.595 | 1.470 | 70.473 | 21.000 | 29.421 | 27.770 | 2021 | 6 | 29 | . 90 3.270 | 0.703 | 1.180 | 75.203 | 21.500 | 30.211 | 29.054 | 2021 | 6 | 30 | . 91 rows × 10 columns . print(dt.feature_importances_) print(gs.best_params_) . [0.00134991 0.01028958 0. 0. 0.01253956 0.12896212 0.18233246 0.66277645 0.00174992 0. ] {&#39;max_depth&#39;: 12, &#39;min_impurity_decrease&#39;: 0.0002, &#39;min_samples_split&#39;: 25} . import matplotlib.pyplot as plt plt.style.use(&#39;ggplot&#39;) plt.figure(figsize = (20, 10)) plt.plot(dt.predict(x_train), label = &#39;prediction&#39;) plt.plot(y_train, label = &#39;real&#39;) plt.legend(fontsize = 20) plt.show() . RandomForest // 0.64282 . from sklearn.ensemble import RandomForestRegressor rf = RandomForestRegressor(random_state = 42, n_jobs = -1) rf.fit(x_train, y_train) print(rf.feature_importances_) . [0.02715464 0.0636719 0.02191772 0.01226784 0.04654751 0.12911862 0.10436298 0.56736246 0.01310199 0.01449433] . import matplotlib.pyplot as plt plt.style.use(&#39;ggplot&#39;) plt.figure(figsize = (20, 10)) plt.plot(rf.predict(x_train), label = &#39;prediction&#39;) plt.plot(y_train, label = &#39;real&#39;) plt.legend(fontsize = 20) plt.show() . Ridge (feat. PolynomialFeatures) // 0.57752 . from sklearn.linear_model import Ridge from sklearn.preprocessing import PolynomialFeatures from sklearn.preprocessing import StandardScaler poly = PolynomialFeatures() poly.fit(x_train) x_train_poly = poly.transform(x_train) x_test_poly = poly.transform(x_test) ss = StandardScaler() ss.fit(x_train_poly) x_train_poly_scaled = ss.transform(x_train_poly) x_test_poly_scaled = ss.transform(x_test_poly) ridge = Ridge(alpha = 0.0000001) ridge.fit(x_train_poly_scaled, y_train) . Ridge(alpha=1e-07, copy_X=True, fit_intercept=True, max_iter=None, normalize=False, random_state=None, solver=&#39;auto&#39;, tol=0.001) . import matplotlib.pyplot as plt # 리스트 만들기 train_score = [] test_score = [] alpha_list = [0.0000000000000001, 0.0000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100] # 반복문으로 alpha 값 돌리기 for alpha in alpha_list: ridge = Ridge(alpha = alpha) ridge.fit(x_train_poly_scaled, y_train) train_score.append(ridge.score(x_train_poly_scaled, y_train)) test_score.append(ridge.score(x_test_poly_scaled, ridge.predict(x_test_poly_scaled))) # 그래프 그리기 plt.plot(np.log10(alpha_list), train_score, ) plt.plot(np.log10(alpha_list), test_score) plt.xlabel(&#39;alpha&#39;) plt.ylabel(&#39;R^2&#39;) plt.show() # train 스코어가 가장 높고 test 세트의 점수가 가장 높은 -1 = 10^-1 = 0.1 --&gt; alpha = 0.1로 하여 최종 모델을 훈련해야 가장 좋은 모델 . /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_ridge.py:148: LinAlgWarning: Ill-conditioned matrix (rcond=1.01053e-20): result may not be accurate. overwrite_a=True).T . import matplotlib.pyplot as plt plt.style.use(&#39;ggplot&#39;) plt.figure(figsize = (20, 10)) plt.plot(ridge.predict(x_train_poly_scaled), label = &#39;prediction&#39;) plt.plot(y_train, label = &#39;real&#39;) plt.legend(fontsize = 20) plt.show() . Lasso (feat.PolynomialFeatures) // 1.14842 . from sklearn.preprocessing import PolynomialFeatures from sklearn.preprocessing import StandardScaler from sklearn.linear_model import Lasso poly = PolynomialFeatures() poly.fit(x_train) x_train_poly = poly.transform(x_train) x_test_poly = poly.transform(x_test) ss = StandardScaler() ss.fit(x_train_poly) x_train_poly_scaled = ss.transform(x_train_poly) x_test_poly_scaled = ss.transform(x_test_poly) lasso = Lasso(alpha = 0.1) lasso.fit(x_train_poly_scaled, y_train) . /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 11316965287.630852, tolerance: 20809417.990786813 positive) . Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000, normalize=False, positive=False, precompute=False, random_state=None, selection=&#39;cyclic&#39;, tol=0.0001, warm_start=False) . from sklearn.preprocessing import StandardScaler from sklearn.linear_model import Lasso ss = StandardScaler() ss.fit(x_train, y_train) x_train_scaled = ss.transform(x_train) x_test_scaled = ss.transform(x_test) lasso = Lasso(alpha = 0.0001) lasso.fit(x_train_scaled, y_train) . /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 15036150890.694096, tolerance: 20809417.990786813 positive) . Lasso(alpha=0.0001, copy_X=True, fit_intercept=True, max_iter=1000, normalize=False, positive=False, precompute=False, random_state=None, selection=&#39;cyclic&#39;, tol=0.0001, warm_start=False) . import matplotlib.pyplot as plt from sklearn.linear_model import Lasso # 리스트 만들기 train_score = [] test_score = [] alpha_list = [0.0000000000000001, 0.0000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100] # 반복문으로 alpha 값 돌리기 for alpha in alpha_list: lasso = Lasso(alpha = alpha) lasso.fit(x_train_poly_scaled, y_train) train_score.append(lasso.score(x_train_poly_scaled, y_train)) test_score.append(lasso.score(x_test_poly_scaled, lasso.predict(x_test_poly_scaled))) # 그래프 그리기 plt.plot(np.log10(alpha_list), train_score, ) plt.plot(np.log10(alpha_list), test_score) plt.xlabel(&#39;alpha&#39;) plt.ylabel(&#39;R^2&#39;) plt.show() # train 스코어가 가장 높고 test 세트의 점수가 가장 높은 -1 = 10^-1 = 0.1 --&gt; alpha = 0.1로 하여 최종 모델을 훈련해야 가장 좋은 모델 . /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 11460544544.452034, tolerance: 20809417.990786813 positive) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 11460544403.169, tolerance: 20809417.990786813 positive) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 11460530416.141394, tolerance: 20809417.990786813 positive) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 11460403259.290476, tolerance: 20809417.990786813 positive) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 11459131489.683096, tolerance: 20809417.990786813 positive) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 11446393544.570705, tolerance: 20809417.990786813 positive) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 11316965287.630852, tolerance: 20809417.990786813 positive) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 9861513758.318222, tolerance: 20809417.990786813 positive) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4289118912.9155006, tolerance: 20809417.990786813 positive) . import matplotlib.pyplot as plt plt.style.use(&#39;ggplot&#39;) plt.figure(figsize = (20, 10)) plt.plot(lasso.predict(x_train), label = &#39;prediction&#39;) plt.plot(y_train, label = &#39;real&#39;) plt.legend(fontsize = 20) plt.show() . &#51228;&#52636; . test_predict = lr.predict(x_test) submission[&#39;number_of_rentals&#39;] = test_predict submission.to_csv(&#39;sample_submission.csv&#39;, index = False) . test_predict = kn.predict(x_test_scaled) submission[&#39;number_of_rentals&#39;] = test_predict submission.to_csv(&#39;sample_submission.csv&#39;, index = False) . test_predict = dt.predict(x_test) submission[&#39;number_of_rentals&#39;] = test_predict submission.to_csv(&#39;sample_submission.csv&#39;, index = False) . test_predict = rf.predict(x_test) submission[&#39;number_of_rentals&#39;] = test_predict submission.to_csv(&#39;sample_submission.csv&#39;, index = False) . test_predict = ridge.predict(x_test_poly_scaled) submission[&#39;number_of_rentals&#39;] = test_predict submission.to_csv(&#39;sample_submission.csv&#39;, index = False) . test_predict = lasso.predict(x_test_scaled) submission[&#39;number_of_rentals&#39;] = test_predict submission.to_csv(&#39;sample_submission.csv&#39;, index = False) . &#45936;&#51060;&#53552; &#48520;&#47084;&#50724;&#44592; . import pandas as pd import numpy as np train = pd.read_csv(&#39;/content/drive/MyDrive/mydata/train.csv&#39;) test = pd.read_csv(&#39;/content/drive/MyDrive/mydata/test.csv&#39;) train.head().T . 0 1 2 3 4 . index 0 | 1 | 2 | 3 | 4 | . gender F | F | M | F | F | . car N | N | Y | N | Y | . reality N | Y | Y | Y | Y | . child_num 0 | 1 | 0 | 0 | 0 | . income_total 202500 | 247500 | 450000 | 202500 | 157500 | . income_type Commercial associate | Commercial associate | Working | Commercial associate | State servant | . edu_type Higher education | Secondary / secondary special | Higher education | Secondary / secondary special | Higher education | . family_type Married | Civil marriage | Married | Married | Married | . house_type Municipal apartment | House / apartment | House / apartment | House / apartment | House / apartment | . DAYS_BIRTH -13899 | -11380 | -19087 | -15088 | -15037 | . DAYS_EMPLOYED -4709 | -1540 | -4434 | -2092 | -2105 | . FLAG_MOBIL 1 | 1 | 1 | 1 | 1 | . work_phone 0 | 0 | 0 | 0 | 0 | . phone 0 | 0 | 1 | 1 | 0 | . email 0 | 1 | 0 | 0 | 0 | . occyp_type NaN | Laborers | Managers | Sales staff | Managers | . family_size 2 | 3 | 2 | 2 | 2 | . begin_month -6 | -5 | -22 | -37 | -26 | . credit 1 | 1 | 2 | 0 | 2 | . # 카드 등록 개월 보기편하게 절대값 train[&#39;DAYS_BIRTH&#39;] = (abs(train[&#39;DAYS_BIRTH&#39;]) / 365) train[&#39;DAYS_EMPLOYED&#39;] = (abs(train[&#39;DAYS_EMPLOYED&#39;]) / 365) train[&#39;begin_month&#39;] = abs(train[&#39;begin_month&#39;]) train.head().T . 0 1 2 3 4 . index 0 | 1 | 2 | 3 | 4 | . gender F | F | M | F | F | . car N | N | Y | N | Y | . reality N | Y | Y | Y | Y | . child_num 0 | 1 | 0 | 0 | 0 | . income_total 202500 | 247500 | 450000 | 202500 | 157500 | . income_type Commercial associate | Commercial associate | Working | Commercial associate | State servant | . edu_type Higher education | Secondary / secondary special | Higher education | Secondary / secondary special | Higher education | . family_type Married | Civil marriage | Married | Married | Married | . house_type Municipal apartment | House / apartment | House / apartment | House / apartment | House / apartment | . DAYS_BIRTH 38.0795 | 31.1781 | 52.2932 | 41.337 | 41.1973 | . DAYS_EMPLOYED 12.9014 | 4.21918 | 12.1479 | 5.73151 | 5.76712 | . FLAG_MOBIL 1 | 1 | 1 | 1 | 1 | . work_phone 0 | 0 | 0 | 0 | 0 | . phone 0 | 0 | 1 | 1 | 0 | . email 0 | 1 | 0 | 0 | 0 | . occyp_type NaN | Laborers | Managers | Sales staff | Managers | . family_size 2 | 3 | 2 | 2 | 2 | . begin_month 6 | 5 | 22 | 37 | 26 | . credit 1 | 1 | 2 | 0 | 2 | .",
            "url": "https://parkjeongung.github.io/Ung.github.io/2022/01/06/%EB%94%B0%EB%A6%89%EC%9D%B4_%EB%8C%80%EC%97%AC%EB%9F%89_%EC%98%88%EC%B8%A1_%EB%AA%A8%EB%8D%B8%EB%A7%81_%EC%97%B0%EC%8A%B5.html",
            "relUrl": "/2022/01/06/%EB%94%B0%EB%A6%89%EC%9D%B4_%EB%8C%80%EC%97%AC%EB%9F%89_%EC%98%88%EC%B8%A1_%EB%AA%A8%EB%8D%B8%EB%A7%81_%EC%97%B0%EC%8A%B5.html",
            "date": " • Jan 6, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "PolynomialFeatures // 릿지 Ridge // 라쏘 Lasso",
            "content": ". import pandas as pd df = pd.read_csv(&#39;https://bit.ly/perch_csv_data&#39;) # 판다스로 준비된 데이터 넘파이 배열로 바꾸기 --&gt; to_numpy() perch_full = df.to_numpy() import numpy as np perch_weight = np.array([5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0, 115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0, 150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0, 218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0, 556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0, 850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0, 1000.0]) . from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(perch_full, perch_weight, random_state = 42) . PolynomialFeatures . 사이킷런은 특성을 만들거나 전처리하기 위한 다양한 클래스를 제공한다. | . from sklearn.preprocessing import PolynomialFeatures poly = PolynomialFeatures() poly.fit([[2, 3]]) print(poly.transform([[2, 3]])) poly = PolynomialFeatures(include_bias = False) # include_bias = False 로 배열에서 1을 제거한다. 사실 굳이 지정하지 않아도 사이킷런 모델은 1을 무시함. 문과라 평면을 안배워서 뭔말인지 잘모르겠으니까 스킵 poly.fit([[2, 3]]) print(poly.transform([[2, 3]])) # PolynomialFeatures 적용 poly = PolynomialFeatures(include_bias = False) poly.fit(train_input) train_poly = poly.transform(train_input) print(train_poly.shape) print(train_input.shape) . [[1. 2. 3. 4. 6. 9.]] [[2. 3. 4. 6. 9.]] (42, 9) (42, 3) . poly.get_feature_names() . [&#39;x0&#39;, &#39;x1&#39;, &#39;x2&#39;, &#39;x0^2&#39;, &#39;x0 x1&#39;, &#39;x0 x2&#39;, &#39;x1^2&#39;, &#39;x1 x2&#39;, &#39;x2^2&#39;] . test_poly = poly.transform(test_input) # 다중 회귀 모델 훈련 from sklearn.linear_model import LinearRegression lr = LinearRegression() lr.fit(train_poly, train_target) print(lr.score(train_poly, train_target)) print(lr.score(test_poly, test_target)) # 여러개의 특성을 사용하니 과소적합도 해결되고 점수도 좋게 나온다. . 0.9903183436982124 0.9714559911594132 . poly = PolynomialFeatures(degree = 5, include_bias = False) poly.fit(train_input) train_poly = poly.transform(train_input) test_poly = poly.transform(test_input) print(train_poly.shape, test_poly.shape) # 특성이 55개 # 추가된 특성으로 모델 훈련 from sklearn.linear_model import LinearRegression lr = LinearRegression() lr.fit(train_poly, train_target) print(lr.score(train_poly, train_target)) # 거의 완벽한 점수 print(lr.score(test_poly, test_target)) # 음수? --&gt; 특성을 너무 많이 늘리면 train 세트에 과대적합 되므로 테스트 세트의 점수가 굉장히 낮게 나온다. . (42, 55) (14, 55) 0.9999999999991096 -144.40579242335605 . &#44508;&#51228; . 모델이 훈련 세트를 너무 과도하게 학습하지 못하도록 하는 것 --&gt; 훈련세트에 과대적합되지 않도록 하는 것 | 선형 회귀 모델의 경우 특성에 곱해지는 계수의 크기를 작게 만드는 일 | . from sklearn.preprocessing import StandardScaler ss = StandardScaler() ss.fit(train_poly) train_scaled = ss.transform(train_poly) test_scaled = ss.transform(test_poly) . &#47551;&#51648; &#46972;&#50136; // &#49440;&#54805; &#54924;&#44480; &#47784;&#45944;&#50640; &#44508;&#51228;&#47484; &#52628;&#44032;&#54620; &#47784;&#45944; . 릿지 회귀 --&gt; 계수를 제곱한 값을 기준으로 규제를 적용 | 라쏘 회귀 --&gt; 계수의 절댓값을 기준으로 규제를 적용 | from sklearn.linear_model import Ridge ridge = Ridge() ridge.fit(train_scaled, train_target) print(ridge.score(train_scaled, train_target)) print(ridge.score(test_scaled, test_target)) # 많은 특성을 사용했는데도 train세트에 너무 과대적합되지 않음 . 0.9896101671037343 0.9790693977615398 . # alpha 와 같이 사람이 직접 알려줘야 하는 파라미터를 &quot;하이퍼파라미터&quot; 라고 한다. # 적절한 alpha 값 찾는 방법 --&gt; alpha 값에 대한 R^2 그래프 그리기 --&gt; train세트와 test세트의 점수가 가장 가까운 지점이 최적의 alpha값 import matplotlib.pyplot as plt # 리스트 만들기 train_score = [] test_score = [] alpha_list = [0.001, 0.01, 0.1, 1, 10, 100] # 반복문으로 alpha 값 돌리기 for alpha in alpha_list: ridge = Ridge(alpha = alpha) ridge.fit(train_scaled, train_target) train_score.append(ridge.score(train_scaled, train_target)) test_score.append(ridge.score(test_scaled, test_target)) # 그래프 그리기 plt.plot(np.log10(alpha_list), train_score) plt.plot(np.log10(alpha_list), test_score) plt.xlabel(&#39;alpha&#39;) plt.ylabel(&#39;R^2&#39;) plt.show() # train 스코어가 가장 높고 test 세트의 점수가 가장 높은 -1 = 10^-1 = 0.1 --&gt; alpha = 0.1로 하여 최종 모델을 훈련해야 가장 좋은 모델 . from sklearn.linear_model import Ridge ridge = Ridge(alpha = 0.1) ridge.fit(train_scaled, train_target) print(ridge.score(train_scaled, train_target)) print(ridge.score(test_scaled, test_target)) . 0.9903815817570366 0.9827976465386927 . from sklearn.linear_model import Lasso lasso = Lasso() lasso.fit(train_scaled, train_target) print(lasso.score(train_scaled, train_target)) print(lasso.score(test_scaled, test_target)) . 0.9897898972080961 0.9800593698421883 . import matplotlib.pyplot as plt from sklearn.linear_model import Lasso train_score = [] test_score = [] alpha_list = [0.001, 0.01, 0.1, 1, 10, 100] for alpha in alpha_list: lasso = Lasso(alpha = alpha) lasso.fit(train_scaled, train_target) train_score.append(lasso.score(train_scaled, train_target)) test_score.append(lasso.score(test_scaled, test_target)) . /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 23364.075969939808, tolerance: 518.2793833333334 positive) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 20251.975097475122, tolerance: 518.2793833333334 positive) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 806.2370926333242, tolerance: 518.2793833333334 positive) . import matplotlib.pyplot as plt from sklearn.linear_model import Lasso train_score = [] test_score = [] alpha_list = [0.001, 0.01, 0.1, 1, 10, 100] for alpha in alpha_list: lasso = Lasso(alpha = alpha, max_iter = 10000) # 라쏘 모델은 최적의 계수를 찾기 위해 반복적인 계산을 수행하는데 지정한 횟수가 부족할 때 이런 경고가 발생 --&gt; max_iter 를 늘려줌으로써 해결 lasso.fit(train_scaled, train_target) train_score.append(lasso.score(train_scaled, train_target)) test_score.append(lasso.score(test_scaled, test_target)) plt.plot(np.log10(alpha_list), train_score) plt.plot(np.log10(alpha_list), test_score) plt.xlabel(&#39;alpha&#39;) plt.ylabel(&#39;R^2&#39;) plt.show() # 최적의 알파값은 1 = 10 --&gt; alpha = 10 으로 하여 모델 훈련 . /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 18778.697957792876, tolerance: 518.2793833333334 positive) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 12972.821345404844, tolerance: 518.2793833333334 positive) . from sklearn.linear_model import Lasso lasso = Lasso(alpha = 10) lasso.fit(train_scaled, train_target) print(lasso.score(train_scaled, train_target)) print(lasso.score(test_scaled, test_target)) . 0.9888067471131867 0.9824470598706695 . # 라쏘 모델의 계수는 coef_ 에 저장되어 있다. print(lasso.coef_) # np.sum 은 배열을 모두 더한 값을 반환한다 # 넘파이 배열에 비교연산자를 사용하면 각 원소는 True = 1 아니면 False = 0 가 된다. print(np.sum(lasso.coef_ == 0)) # 총 55개의 특성 중 40개의 특성의 계수를 0으로 만듬 --&gt; 15개의 특성만 사용 . [ 0. 0. 0. 12.14852453 55.44856399 42.23100799 0. 0. 13.70596191 0. 43.2185952 5.7033775 47.46254536 7.42309425 11.85823365 0. 0. 13.53038193 21.22111356 0. 0. 0. 0. 0. 0. 18.66993032 0. 0. 0. 15.81041778 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 18.14672268 0. 0. 0. 0. 15.51272953 0. 0. 0. 0. 0. ] 40 . &#50672;&#49845; . import pandas as pd df = pd.read_csv(&#39;https://bit.ly/perch_csv_data&#39;) perch_full = df.to_numpy() import numpy as np perch_weight = np.array([5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0, 115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0, 150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0, 218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0, 556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0, 850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0, 1000.0]) from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(perch_full, perch_weight, random_state = 42) from sklearn.preprocessing import PolynomialFeatures poly = PolynomialFeatures(degree = 5, include_bias = False) poly.fit(train_input) train_poly = poly.transform(train_input) test_poly = poly.transform(test_input) from sklearn.linear_model import LinearRegression lr = LinearRegression() lr.fit(train_poly, train_target) print(lr.score(train_poly, train_target)) print(lr.score(test_poly, test_target)) from sklearn.preprocessing import StandardScaler ss = StandardScaler() ss.fit(train_poly) train_scaled = ss.transform(train_poly) test_scaled = ss.transform(test_poly) from sklearn.linear_model import Ridge ridge = Ridge() ridge.fit(train_scaled, train_target) print(ridge.score(train_scaled, train_target)) print(ridge.score(test_scaled, test_target)) train_score = [] test_score = [] import matplotlib.pyplot as plt alpha_list = [0.001, 0.01, 0.1, 1, 10, 100] for alpha in alpha_list: ridge = Ridge(alpha = alpha) ridge.fit(train_scaled, train_target) train_score.append(ridge.score(train_scaled, train_target)) test_score.append(ridge.score(test_scaled, test_target)) plt.plot(np.log10(alpha_list), train_score) plt.plot(np.log10(alpha_list), test_score) plt.xlabel(&#39;alpha&#39;) plt.ylabel(&#39;R^2&#39;) plt.show() from sklearn.linear_model import Lasso lasso = Lasso() lasso.fit(train_scaled, train_target) print(lasso.score(train_scaled, train_target)) print(lasso.score(test_scaled, test_target)) train_score = [] test_score = [] alpha_list = [0.001, 0.01, 0.1, 1, 10, 100] for alpha in alpha_list: lasso = Lasso(alpha = alpha) lasso.fit(train_scaled, train_target) train_score.append(lasso.score(train_scaled, train_target)) test_score.append(lasso.score(test_scaled, test_target)) plt.plot(np.log10(alpha_list), train_score) plt.plot(np.log10(alpha_list), test_score) plt.xlabel(&#39;alpha&#39;) plt.ylabel(&#39;R^2&#39;) plt.show() . 0.9999999999991096 -144.40579242335605 0.9896101671037343 0.9790693977615398 . 0.9897898972080961 0.9800593698421883 . /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 23364.075969939808, tolerance: 518.2793833333334 positive) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 20251.975097475122, tolerance: 518.2793833333334 positive) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 806.2370926333242, tolerance: 518.2793833333334 positive) .",
            "url": "https://parkjeongung.github.io/Ung.github.io/2022/01/06/PolynomialFeatures_%EB%A6%BF%EC%A7%80_Ridge_%EB%9D%BC%EC%8F%98_Lasso.html",
            "relUrl": "/2022/01/06/PolynomialFeatures_%EB%A6%BF%EC%A7%80_Ridge_%EB%9D%BC%EC%8F%98_Lasso.html",
            "date": " • Jan 6, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "K - 평균 // K - Means",
            "content": ". &#50508;&#44256;&#47532;&#51608; &#51089;&#46041;&#50896;&#47532; . !wget https://bit.ly/fruits_300_data -O fruits_300.npy . --2021-12-12 10:02:39-- https://bit.ly/fruits_300_data Resolving bit.ly (bit.ly)... 67.199.248.10, 67.199.248.11 Connecting to bit.ly (bit.ly)|67.199.248.10|:443... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://github.com/rickiepark/hg-mldl/raw/master/fruits_300.npy [following] --2021-12-12 10:02:39-- https://github.com/rickiepark/hg-mldl/raw/master/fruits_300.npy Resolving github.com (github.com)... 140.82.113.3 Connecting to github.com (github.com)|140.82.113.3|:443... connected. HTTP request sent, awaiting response... 302 Found Location: https://raw.githubusercontent.com/rickiepark/hg-mldl/master/fruits_300.npy [following] --2021-12-12 10:02:40-- https://raw.githubusercontent.com/rickiepark/hg-mldl/master/fruits_300.npy Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 3000128 (2.9M) [application/octet-stream] Saving to: ‘fruits_300.npy’ fruits_300.npy 100%[===================&gt;] 2.86M --.-KB/s in 0.07s 2021-12-12 10:02:40 (43.0 MB/s) - ‘fruits_300.npy’ saved [3000128/3000128] . import numpy as np fruits = np.load(&#39;fruits_300.npy&#39;) # 2차원 배열로 변형 fruits_2d = fruits.reshape(-1, 100 * 100) . from sklearn.cluster import KMeans km = KMeans(n_clusters = 3, random_state = 42) # n_clusters 로 만들어질 클러스터의 갯수 설정 km.fit(fruits_2d) . KMeans(n_clusters=3, random_state=42) . print(km.labels_) . [2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 0 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 0 0 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] . print(np.unique(km.labels_, return_counts = True)) . (array([0, 1, 2], dtype=int32), array([111, 98, 91])) . import matplotlib.pyplot as plt def draw_fruits(arr, ratio = 1): n = len(arr) rows = int(np.ceil(n / 10)) cols = n if rows &lt; 2 else 10 fig, axs = plt.subplots(rows, cols, figsize = (cols * ratio, rows * ratio), squeeze = False) for i in range(rows): for j in range(cols): if i * 10 + j &lt; n: axs[i, j].imshow(arr[i * 10 + j], cmap = &#39;gray_r&#39;) axs[i, j].axis(&#39;off&#39;) plt.show() . draw_fruits(fruits[km.labels_ == 0]) . draw_fruits(fruits[km.labels_ == 1]) . draw_fruits(fruits[km.labels_ == 2]) . &#45824;&#54364;&#51060;&#48120;&#51648; &#52636;&#47141;&#54644;&#48372;&#44592; . # 훈련 데이터가 2차원 배열로 변형된 데이터 였기 때문에 2차원 배열로나옴 print(km.cluster_centers_.shape) # 이미지를 그리기 위해 3차원 배열로 다시 바꿔준다 cluster_centers_ = km.cluster_centers_.reshape(-1, 100, 100) draw_fruits(cluster_centers_, ratio = 3) # 이전에 픽셀값 평균으로 찾았던 대표이미지와 유사하다 . (3, 10000) . print(km.transform(fruits_2d[100: 101])) # 0번째 클러스터 까지의 거리가 가장 가까우므로 이 샘플은 0번째 클러스터에 속해있다. . [[3393.8136117 8837.37750892 5267.70439881]] . print(km.predict(fruits_2d[100: 101])) # 0번째 클러스터에 속해있다. 위의 transform 으로 확인한 결과와 일치한다 . [0] . # 그림을 그려 확인해보자 draw_fruits(fruits[100: 101], ratio = 3) . # 최적의 클러스터 중심을 찾기위해 알고리즘이 반복한 횟수 확인 --&gt; n_iter_ print(km.n_iter_) . 4 . &#52572;&#51201;&#51032; K(n_cluster) &#44050; &#52286;&#44592; . 알고리즘을 훈련할 때 타겟값을 사용하지 않긴 했지만 클러스터를 총 3개(사과, 파인애플, 바나나)를 써야한다는 것을 알고 있었음 --&gt; &quot;n_cluster = 3&quot; 으로 지정 | 실전에서는 클러스터 개수를 알 수 없다 --&gt; 엘보우 방법으로 해결 | . 클러스터 중심과 그 클러스터에 속한 샘플 사이의 거리 제곱 합을 이너셔 라고한다. | 이너셔는 클러스터에 속한 샘플이 얼마나 가깝게 모여있는지를 나타내는 값으로 생각할 수 있다. | 일반적으로 클러스터 갯수가 늘어나면 클러스터 각각의 크기가 줄어들기 때문에 이너셔도 줄어든다 | 클러스터 개수를 늘려가며 이너셔의 변화를 관찰한다. 최적의 클러스터 갯수를 찾는다 | . # 클러스터 갯수를 늘려가면서 이너셔를 확인하는 그래프를 그린다 inertia = [] for k in range(2, 7): km = KMeans(n_clusters = k, random_state = 42) km.fit(fruits_2d) inertia.append(km.inertia_) plt.plot(range(2, 7), inertia) plt.xlabel(&#39;cluster&#39;) plt.ylabel(&#39;inertia&#39;) plt.show() # 그래프의 모양이 팔꿈치 처럼 생겼대서 엘보우래요 ㅋㅋ # K = 3 에서 이너셔의 변화가 줄어들기 때문에 최적의 K 값은 3 이다 .",
            "url": "https://parkjeongung.github.io/Ung.github.io/2022/01/06/K_%ED%8F%89%EA%B7%A0_K_Means.html",
            "relUrl": "/2022/01/06/K_%ED%8F%89%EA%B7%A0_K_Means.html",
            "date": " • Jan 6, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "K - 최근접 이웃 회귀 // KNeighborsRegressor",
            "content": ". &#54924;&#44480; . 임의의 어떤 숫자를 예측 | 두 변수 사이의 상관관계를 분석하는 방법 | . import numpy as np # 농어의 특성 = 길이 perch_length = np.array([8.4, 13.7, 15.0, 16.2, 17.4, 18.0, 18.7, 19.0, 19.6, 20.0, 21.0, 21.0, 21.0, 21.3, 22.0, 22.0, 22.0, 22.0, 22.0, 22.5, 22.5, 22.7, 23.0, 23.5, 24.0, 24.0, 24.6, 25.0, 25.6, 26.5, 27.3, 27.5, 27.5, 27.5, 28.0, 28.7, 30.0, 32.8, 34.5, 35.0, 36.5, 36.0, 37.0, 37.0, 39.0, 39.0, 39.0, 40.0, 40.0, 40.0, 40.0, 42.0, 43.0, 43.0, 43.5, 44.0]) # 농어의 타겟 = 무게 perch_weight = np.array([5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0, 115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0, 150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0, 218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0, 556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0, 850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0, 1000.0]) . import matplotlib.pyplot as plt plt.scatter(perch_length, perch_weight) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() # 농어의 길이가 길어질수록 무게가 늘어난다. . from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(perch_length, perch_weight, random_state = 42) . # reshape 메서드는 크기에 -1 을 지정하면 남은 원소 개수로 모두 채우라는 의미 train_input = train_input.reshape(-1, 1) # == train_input.reshape(42, 1) print(train_input.shape) test_input = test_input.reshape(-1, 1) print(test_input.shape) . (42, 1) (14, 1) . from sklearn.neighbors import KNeighborsRegressor knr = KNeighborsRegressor() knr.fit(train_input, train_target) print(knr.score(test_input, test_target)) # 스코어가 1이 안나왔다 --&gt; 회귀에서는 정확한 숫자를 맞힌다는 것이 거의 불가능하다. --&gt; 예측하는 값이 모두 임의의 수치이기 때문 . 0.9928094061010639 . from sklearn.metrics import mean_absolute_error # 테스트 세트의 예측값을 만든다. test_prediction = knr.predict(test_input) # 테스트 세트에 대한 절댓값 오차의 평균을 구한다. mae = mean_absolute_error(test_target, test_prediction) mae # 19그람 정도의 오차가 발생한다. . 19.157142857142862 . &#54984;&#47144; &#49464;&#53944;&#47484; &#49324;&#50857;&#54644; &#47784;&#45944; &#54217;&#44032;&#54616;&#44592; // &#44284;&#49548;&#51201;&#54633;, &#44284;&#45824;&#51201;&#54633; . score(train) &gt; score(test) --&gt; 모델이 훈련세트에 과대적합 되었다. --&gt; 실전에 투입하면 예측이 잘 안됨 --&gt; 모델을 덜 복잡하게 만들어 해결한다. --&gt; 이웃의 개수를 늘린다 | score(train) &lt; score(test) --&gt; 모델이 훈련세트에 과소적합 되었다. --&gt; 모델이 너무 단순하다. --&gt; 모델을 더 복잡하게 만들어 해결한다. --&gt; 이웃의 개수를 줄인다. | . print(knr.score(train_input, train_target)) # 테스트 세트로 점수 확인하기 print(knr.score(test_input, test_target)) # score(train) &lt; score(test) --&gt; 과소적합 --&gt; 데이터가 작을경우 과소적합이 발생할 수 있다. . 0.9698823289099255 0.9928094061010639 . knr.n_neighbors = 3 knr.fit(train_input, train_target) print(knr.score(train_input, train_target)) print(knr.score(test_input, test_target)) # 이웃의 개수를 줄였더니 train 세트와 test 세트의 score가 비슷하게 나온다. . 0.9804899950518966 0.974645996398761 . &#50672;&#49845; . import numpy as np perch_length = np.array([8.4, 13.7, 15.0, 16.2, 17.4, 18.0, 18.7, 19.0, 19.6, 20.0, 21.0, 21.0, 21.0, 21.3, 22.0, 22.0, 22.0, 22.0, 22.0, 22.5, 22.5, 22.7, 23.0, 23.5, 24.0, 24.0, 24.6, 25.0, 25.6, 26.5, 27.3, 27.5, 27.5, 27.5, 28.0, 28.7, 30.0, 32.8, 34.5, 35.0, 36.5, 36.0, 37.0, 37.0, 39.0, 39.0, 39.0, 40.0, 40.0, 40.0, 40.0, 42.0, 43.0, 43.0, 43.5, 44.0]) # 농어의 타겟 = 무게 perch_weight = np.array([5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0, 115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0, 150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0, 218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0, 556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0, 850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0, 1000.0]) # train test split from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(perch_length, perch_weight, random_state = 42) # 2차원 배열로 변형 train_input = train_input.reshape(-1, 1) test_input = test_input.reshape(-1, 1) # 모델링 from sklearn.neighbors import KNeighborsRegressor knr = KNeighborsRegressor() knr.fit(train_input, train_target) # 스코어 knr.score(test_input, test_target) # 오차확인 mean absoulte error from sklearn.metrics import mean_absolute_error test_prediction = knr.predict(test_input) mae = mean_absolute_error(test_target, test_prediction) # 과소적합 --&gt; 테스트세트 스코어가 트레인세트 스코어보다 높다 print(knr.score(train_input, train_target)) print(knr.score(test_input, test_target)) # 과소적합 해결 --&gt; 모델을 복잡하게 만든다. --&gt; 이웃수 줄이기 knr.n_neighbors = 3 knr.fit(train_input, train_target) print(knr.score(train_input, train_target)) print(knr.score(test_input, test_target)) . 0.9698823289099255 0.9928094061010639 0.9804899950518966 0.974645996398761 . # n_neighbors = [1, 5, 10] # 농어의 길이를 5에서 45까지 바꿔가며 모델링 결과 확인하기 from sklearn.neighbors import KNeighborsRegressor import matplotlib.pyplot as plt knr = KNeighborsRegressor() x = np.arange(5, 45).reshape(-1, 1) for n in [1, 5, 10]: knr.n_neighbors = n knr.fit(train_input, train_target) predict = knr.predict(x) plt.scatter(train_input, train_target) plt.scatter(x, predict, marker = &#39;^&#39;) plt.title(&quot;n_neighbors = {}&quot;.format(n)) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() .",
            "url": "https://parkjeongung.github.io/Ung.github.io/2022/01/06/K_%EC%B5%9C%EA%B7%BC%EC%A0%91_%EC%9D%B4%EC%9B%83_%ED%9A%8C%EA%B7%80_KNeighborsRegressor.html",
            "relUrl": "/2022/01/06/K_%EC%B5%9C%EA%B7%BC%EC%A0%91_%EC%9D%B4%EC%9B%83_%ED%9A%8C%EA%B7%80_KNeighborsRegressor.html",
            "date": " • Jan 6, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Group Product Classification // 인코딩 // Confusion Matrix",
            "content": ". &#45936;&#51060;&#53552; &#48520;&#47084;&#50724;&#44592; . pip install kaggle --upgrade . Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12) Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0) Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.3) Requirement already satisfied: six&gt;=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0) Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2) Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3) Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.10.8) Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2) Requirement already satisfied: text-unidecode&gt;=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify-&gt;kaggle) (1.3) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (3.0.4) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (2.10) . !pip install kaggle from google.colab import files files.upload() . Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12) Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.10.8) Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0) Requirement already satisfied: six&gt;=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0) Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3) Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2) Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2) Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.3) Requirement already satisfied: text-unidecode&gt;=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify-&gt;kaggle) (1.3) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (2.10) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (3.0.4) . Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable. Saving kaggle (1).json to kaggle (1).json . {&#39;kaggle (1).json&#39;: b&#39;{&#34;username&#34;:&#34;chobocoder&#34;,&#34;key&#34;:&#34;89866a08becb23b3c536081d68f0c29b&#34;}&#39;} . !mkdir -p ~/.kaggle !cp kaggle.json ~/.kaggle/ !chmod 600 ~/.kaggle/kaggle.json . !kaggle competitions download -c otto-group-product-classification-challenge . 401 - Unauthorized . import pandas as pd train = pd.read_csv(&#39;/content/drive/MyDrive/mydata/train.csv&#39;) test = pd.read_csv(&#39;/content/drive/MyDrive/mydata/test.csv&#39;) . train.head().T . 0 1 2 3 4 . id 1 | 2 | 3 | 4 | 5 | . feat_1 1 | 0 | 0 | 1 | 0 | . feat_2 0 | 0 | 0 | 0 | 0 | . feat_3 0 | 0 | 0 | 0 | 0 | . feat_4 0 | 0 | 0 | 1 | 0 | . ... ... | ... | ... | ... | ... | . feat_90 0 | 0 | 0 | 0 | 1 | . feat_91 0 | 0 | 0 | 0 | 0 | . feat_92 0 | 0 | 0 | 0 | 0 | . feat_93 0 | 0 | 0 | 0 | 0 | . target Class_1 | Class_1 | Class_1 | Class_1 | Class_1 | . 95 rows × 5 columns . &#44208;&#52769;&#52824; &#54869;&#51064; . sum(train.isnull().sum()) . id 0 feat_1 0 feat_2 0 feat_3 0 feat_4 0 .. feat_90 0 feat_91 0 feat_92 0 feat_93 0 target 0 Length: 95, dtype: int64 . &#53440;&#44191; &#48516;&#47532; . x_train = train.drop(&#39;target&#39;, axis = 1) y_train = train[&#39;target&#39;] . &#51064;&#53076;&#46377; . 사이킷런 알고리즘은 문자열 값을 입력값으로 허용하지 않는다 | 모든 문자열 값은 인코딩 되어 숫자 형으로 변환해야 한다. | . &#46972;&#48296; &#51064;&#53076;&#46377; (Label Encoding) . 카테코리 피쳐를 숫자 값으로 변환한다. | 숫자 값으로 변환 되었기 때문에 특정 알고리즘에서는 이를 가중치로 인식할 수 있다. | 트리 계열의 알고리즘을 사용할 때 주로 사용된다. | . from sklearn.preprocessing import LabelEncoder items=[&#39;TV&#39;, &#39;냉장고&#39;, &#39;전자레인지&#39;, &#39;컴퓨터&#39;, &#39;선풍기&#39;, &#39;믹서&#39;, &#39;믹서&#39;] encoder = LabelEncoder() encoder.fit(items) label = encoder.transform(items) label . array([0, 1, 4, 5, 3, 2, 2]) . # 데이터가 많은 경우에는 classes_ 를 이용해서 확인한다 encoder.classes_ # 처음부터 0, 1 , 2 ... 순서대로 . array([&#39;TV&#39;, &#39;냉장고&#39;, &#39;믹서&#39;, &#39;선풍기&#39;, &#39;전자레인지&#39;, &#39;컴퓨터&#39;], dtype=&#39;&lt;U5&#39;) . print(encoder.inverse_transform([0, 1, 4, 5, 3, 2, 2])) print(items) . [&#39;TV&#39; &#39;냉장고&#39; &#39;전자레인지&#39; &#39;컴퓨터&#39; &#39;선풍기&#39; &#39;믹서&#39; &#39;믹서&#39;] [&#39;TV&#39;, &#39;냉장고&#39;, &#39;전자레인지&#39;, &#39;컴퓨터&#39;, &#39;선풍기&#39;, &#39;믹서&#39;, &#39;믹서&#39;] . &#50896;&#54635; &#51064;&#53076;&#46377; (One-Hot Encoding) . # 원핫 인코딩은 입력값으로 2차원 데이터가 필요하다 from sklearn.preprocessing import OneHotEncoder import numpy as np items=[&#39;TV&#39;, &#39;냉장고&#39;, &#39;전자레인지&#39;, &#39;컴퓨터&#39;, &#39;선풍기&#39;, &#39;믹서&#39;, &#39;믹서&#39;] # 숫자형 값으로 변환 (feat.라벨인코딩) encoder = LabelEncoder() encoder.fit(items) label = encoder.transform(items) # 2차원 데이터로 변환 label = label.reshape(-1, 1) # 원핫인코딩 encoder = OneHotEncoder() encoder.fit(label) label = encoder.transform(label) print(label.toarray()) . [[1. 0. 0. 0. 0. 0.] [0. 1. 0. 0. 0. 0.] [0. 0. 0. 0. 1. 0.] [0. 0. 0. 0. 0. 1.] [0. 0. 0. 1. 0. 0.] [0. 0. 1. 0. 0. 0.] [0. 0. 1. 0. 0. 0.]] . import pandas as pd items = [&#39;TV&#39;, &#39;냉장고&#39;, &#39;전자레인지&#39;, &#39;컴퓨터&#39;, &#39;선풍기&#39;, &#39;믹서&#39;, &#39;믹서&#39;] df = pd.DataFrame({&#39;item&#39;:items}) df . item . 0 TV | . 1 냉장고 | . 2 전자레인지 | . 3 컴퓨터 | . 4 선풍기 | . 5 믹서 | . 6 믹서 | . pd.get_dummies(df) . item_TV item_냉장고 item_믹서 item_선풍기 item_전자레인지 item_컴퓨터 . 0 1 | 0 | 0 | 0 | 0 | 0 | . 1 0 | 1 | 0 | 0 | 0 | 0 | . 2 0 | 0 | 0 | 0 | 1 | 0 | . 3 0 | 0 | 0 | 0 | 0 | 1 | . 4 0 | 0 | 0 | 1 | 0 | 0 | . 5 0 | 0 | 1 | 0 | 0 | 0 | . 6 0 | 0 | 1 | 0 | 0 | 0 | . KNN . from sklearn.neighbors import KNeighborsClassifier kn = KNeighborsClassifier( algorithm = &#39;auto&#39;, # 가장 가까운 이웃을 계산하는데 사용할 알고리즘을 선택 (ball_tree, kd_tree 등이 있음) metric = &#39;minkowski&#39;, # 거리 측정방식 선택 // minkowski 는 맨하튼 과 유클리드 거리공식을 하나로 나타낸 것 p = 2, # p값이 1일때는 맨하튼 거리공식, 2일때는 유클리드 거리공식을 사용 (defalt = 2) weights = &#39;distance&#39; # 예측에 사용하는 가중치 --&gt; &quot;distance&quot; 는 가까운 이웃에 가중치를 더 주고 &quot;uniform&quot; 은 이웃에 모두 동일한 가중치를 준다 ) . . Confusion Matrix // &#48516;&#47448;&#47784;&#45944; &#54217;&#44032; . from sklearn.model_selection import train_test_split x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size = 0.2, random_state = 42, stratify = y_train) x = [x_train, y_train, x_test, y_test] for i in x: print(i.shape) . (49502, 94) (49502,) (12376, 94) (12376,) . from sklearn.preprocessing import StandardScaler ss = StandardScaler() ss.fit(x_train) x_train_scaled = ss.transform(x_train) x_test_scaled = ss.transform(x_test) # 모델링 from sklearn.neighbors import KNeighborsClassifier kn = KNeighborsClassifier( algorithm = &#39;auto&#39;, # 가장 가까운 이웃을 계산하는데 사용할 알고리즘을 선택 (ball_tree, kd_tree 등이 있음) metric = &#39;minkowski&#39;, # 거리 측정방식 선택 // minkowski 는 맨하튼 과 유클리드 거리공식을 하나로 나타낸 것 p = 2, # p값이 1일때는 맨하튼 거리공식, 2일때는 유클리드 거리공식을 사용 (defalt = 2) weights = &#39;distance&#39; # 예측에 사용하는 가중치 --&gt; &quot;distance&quot; 는 가까운 이웃에 가중치를 더 주고 &quot;uniform&quot; 은 이웃에 모두 동일한 가중치를 준다 ) kn.fit(x_train_scaled, y_train) kn_predict = kn.predict(x_test_scaled) # confusion matrix from sklearn.metrics import confusion_matrix, accuracy_score, classification_report cm = confusion_matrix(y_test, kn_predict) print(cm) print(accuracy_score(y_test, kn_predict)) . [[ 308 30 6 0 1 14 4 10 13] [ 7 2840 333 25 5 2 11 1 0] [ 0 456 1058 70 2 1 9 3 2] [ 0 80 182 253 6 15 2 0 0] [ 0 5 8 0 532 3 0 0 0] [ 2 7 12 10 8 2654 56 52 26] [ 0 5 25 15 2 45 423 42 11] [ 3 2 2 2 0 61 29 1552 42] [ 1 0 1 1 0 30 7 38 913]] 0.8510827407886231 . Confusion Matrix &#49884;&#44033;&#54868; . import seaborn as sns import matplotlib.pyplot as plt plt.figure(figsize = (10, 7)) sns.heatmap(cm, annot = True) # 정규화를 하는데 귀찮음이 있다 . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f2fb0ad95d0&gt; . from sklearn.metrics import plot_confusion_matrix figure, ax = plt.subplots(figsize = (15, 10)) plot = plot_confusion_matrix(kn, x_test_scaled, y_test, ax = ax) plt.show() . /usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator. warnings.warn(msg, category=FutureWarning) . from sklearn.metrics import plot_confusion_matrix figure , ax = plt.subplots(figsize = (15, 10)) plot_confusion_matrix(kn, x_test_scaled, y_test, normalize = &#39;true&#39;, ax= ax) plt.show() . /usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator. warnings.warn(msg, category=FutureWarning) . RandomForest . from sklearn.ensemble import RandomForestClassifier rf = RandomForestClassifier( n_estimator = 100, # 앙상블을 구성할 트리의 개수를 지정 criterion = &#39;gini&#39;, # 어떤 불순도를 사용하여 트리 분할 할것인지 지정 &#39;gini&#39; or &#39;entropy&#39; max_features = &#39;auto&#39;, # 각 트리를 구성할 피처의 갯수 선택 bootstrap = &#39;True&#39;, # 부트스트랩 샘플을 사용할 것인지 여부 결정 // False 면 전체 데이터 샘플이 각 트리의 훈련데이터가 됨 warm_start = &#39;False&#39;, # 이전에 훈련된 모델을 불러와 추가훈련을 시킬 것인지를 결정 class_weight = &#39;None&#39;) # &quot;balanced&quot; --&gt; n_samples / (n_classes * np.bincount(y)) 각 클래스의 빈도수에 반비례하게 가중치를 조절한다 # &quot;balanced_subsample&quot; --&gt; 부트스트랩 샘플로 각 트리마다 빈도수를 결정해서 가중치를 조절한다 .",
            "url": "https://parkjeongung.github.io/Ung.github.io/2022/01/06/Group_Product_Classification_%EC%9D%B8%EC%BD%94%EB%94%A9_Confusion_Matrix.html",
            "relUrl": "/2022/01/06/Group_Product_Classification_%EC%9D%B8%EC%BD%94%EB%94%A9_Confusion_Matrix.html",
            "date": " • Jan 6, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://parkjeongung.github.io/Ung.github.io/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://parkjeongung.github.io/Ung.github.io/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://parkjeongung.github.io/Ung.github.io/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}