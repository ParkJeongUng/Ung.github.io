{
  
    
        "post0": {
            "title": "지도학습 비지도학습 // 훈련세트 테스트세트",
            "content": ". &#54984;&#47144;&#49464;&#53944;&#50752; &#53580;&#49828;&#53944;&#49464;&#53944; . 샘플링편향 발생 . fish_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0, 9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0] fish_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0, 6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9] . fish_data = [[l, w] for l, w in zip(fish_length, fish_weight)] fish_target = [1] * 35 + [0] * 14 . from sklearn.neighbors import KNeighborsClassifier kn = KNeighborsClassifier() . train_input = fish_data[: 35] train_target = fish_target[: 35] test_input = fish_data[35: ] test_target = fish_target[35: ] . # 테스트세트 --&gt; score()함수로 모델 평가 kn = kn.fit(train_input, train_target) kn.score(test_input, test_target) #샘플링 편향 발생 --&gt; 마지막 14개(빙어 특성 14개)를 test_input 으로 넣어놔서 훈련에 사용된 train_input 에는 빙어가 하나도 없음 --&gt; 데이터를 섞든지 골고루 샘플을 뽑아야 함 --&gt; numpy 사용 . 0.0 . NUMPY&#47484; &#51060;&#50857;&#54620; &#49368;&#54540;&#47553; &#54200;&#54693; &#54644;&#44208; . 파이썬의 대표적인 배열 라이브러리 | 고차원의 배열을 쉽게 만들고 조작가능 | . import numpy as np . input_arr = np.array(fish_data) target_arr = np.array(fish_target) . print(input_arr.shape) . (49, 2) . # input 과 target 에서 같은 인덱스는 함께 선택되어야 한다. input의 2번은 train으로 target의 2번은 test로 가면 안된다. # 넘파이의 random 함수들은 실행할 때마다 다른 결과를 만든다. --&gt; random.seed()를 지정하면 항상 일정한 결과를 얻을 수 있다. np.random.seed(42) index = np.arange(49) np.random.shuffle(index) index . array([13, 45, 47, 44, 17, 27, 26, 25, 31, 19, 12, 4, 34, 8, 3, 6, 40, 41, 46, 15, 9, 16, 24, 33, 30, 0, 43, 32, 5, 29, 11, 36, 1, 21, 2, 37, 35, 23, 39, 10, 22, 18, 48, 20, 7, 42, 14, 28, 38]) . print(input_arr[[1,3]]) # input_arr의 2번째와 4번째 원소 출력 # numpy 배열을 인덱스로 전하기 --&gt; 훈련세트 생성 train_input = input_arr[index [ : 35]] train_target = input_arr[index [ : 35]] #랜덤으로 만들어진 index의 첫번째 원소는 13 --&gt; input_arr의 14번째 원소(index = 13)가 train_input의 1번째 원소(index = 0)에 들어감 print(input_arr[13], train_input[0]) # numpy 배열을 인덱스로 전하기 --&gt; 테스트세트 생성 test_input = input_arr[index [35 : ]] test_target = input_arr[index [35 : ]] #랜덤으로 만들어진 index의 35번째 원소는 37 --&gt; input_arr의 38번째 원소(indexe = 37)가 test_input의 1번째 원소(index = 0)에 들어감 print(input_arr[37], test_input[0]) . [[ 26.3 290. ] [ 29. 363. ]] [ 32. 340.] [ 32. 340.] [10.6 7. ] [10.6 7. ] . import matplotlib.pyplot as plt plt.scatter(train_input[ :, 0], train_input[ :, 1]) plt.scatter(test_input[ :, 0], test_input[ :, 1]) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() . &#50672;&#49845; . fish_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0, 9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0] fish_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0, 6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9] fish_data = [[l, w] for l, w in zip(fish_length, fish_weight)] fish_target = [1] * 35 + [0] * 14 import numpy as np input_arr = np.array(fish_data) target_arr = np.array(fish_target) np.random.seed(42) index = np.arange(49) np.random.shuffle(index) train_input = input_arr[index[ : 35]] train_target = target_arr[index [ : 35]] test_input = input_arr[index[35 :]] test_target = target_arr[index[35 :]] from sklearn.neighbors import KNeighborsClassifier kn = KNeighborsClassifier() kn.fit(train_input, train_target) kn.score(test_input, test_target) . 1.0 .",
            "url": "https://parkjeongung.github.io/Ung.github.io/2021/12/26/%EC%A7%80%EB%8F%84%ED%95%99%EC%8A%B5_%EB%B9%84%EC%A7%80%EB%8F%84%ED%95%99%EC%8A%B5_%ED%9B%88%EB%A0%A8%EC%84%B8%ED%8A%B8_%ED%85%8C%EC%8A%A4%ED%8A%B8%EC%84%B8%ED%8A%B8.html",
            "relUrl": "/2021/12/26/%EC%A7%80%EB%8F%84%ED%95%99%EC%8A%B5_%EB%B9%84%EC%A7%80%EB%8F%84%ED%95%99%EC%8A%B5_%ED%9B%88%EB%A0%A8%EC%84%B8%ED%8A%B8_%ED%85%8C%EC%8A%A4%ED%8A%B8%EC%84%B8%ED%8A%B8.html",
            "date": " • Dec 26, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "데이터 전처리",
            "content": ". fish_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0, 9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0] fish_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0, 6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9] . &#45336;&#54028;&#51060;&#47196; input &#45936;&#51060;&#53552;&#50752; target &#45936;&#51060;&#53552; &#47564;&#46308;&#44592; . import numpy as np fish_data = np.column_stack((fish_length, fish_weight)) # target 데이터 생성 --&gt; np.ones(), np.zeros() # 두개의 배열을 연결한다 --&gt; np.concatenate(()) fish_target = np.concatenate((np.ones(35), np.zeros(14))) . &#49324;&#51060;&#53431;&#47088;&#51004;&#47196; &#54984;&#47144;&#49464;&#53944;&#50752; &#53580;&#49828;&#53944;&#49464;&#53944; &#45208;&#45572;&#44592; . from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(fish_data, fish_target, random_state = 42) # 잘 나누어졌는지 shpae로 데이터크기 출력 print(train_input.shape, test_input.shape) print(train_target.shape, test_target.shape) # 전체 데이터의 도미와 빙어의 비율 = 35 (도미) : 14 (빙어) / 2.5 : 1 # 테스트 세트의 도미와 빙어의 비율 = 10 (도미) : 3 (빙어) / 3.3 : 1 # 샘플링 편향 발생 print(test_target) . (36, 2) (13, 2) (36,) (13,) [1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.] . # stratify 매개변수에 target데이터를 전달하면 클래스 비율에 맞게 데이터를 나눈다. from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(fish_data, fish_target, stratify = fish_target, random_state = 42) # 전체 데이터의 도미와 빙어의 비율 = 35 (도미) : 14 (빙어) / 2.5 : 1 # 테스트 세트의 도미와 빙어의 비율 = 9 (도미) : 4 (빙어) / 2.25 : 1 # 데이터가 작아 비율을 동일하게 맞출 수 없지만 꽤 올바른 샘플링 print(test_target) . [0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1.] . &#49688;&#49345;&#54620; &#46020;&#48120; &#54620; &#47560;&#47532; . [[25, 150]] 이라는 도미 데이터를 예측값으로 넣었더니 빙어로 출력이 된다. | 산점도에 있어서 직관적으로는 [[25, 150]] 데이터가 도미에 가까워 보이지만 x축의 범위와 y축의 범위를 생각하면 [[25, 150]] 데이터가 빙어데이터에 가까운것이 합리적이다. | 두 특성의 스케일이 다르면 알고리즘이 올바른 예측을 하지 못한다. --&gt; 데이터 전처리를 통해 특성값을 일정한 기준으로 맞춰준다. | . from sklearn.neighbors import KNeighborsClassifier kn = KNeighborsClassifier() kn.fit(train_input, train_target) kn.score(test_input, test_target) # [[25, 150]] 이라는 도미 데이터를 예측값으로 넣었더니 빙어[0]로 출력이 된다. print(kn.predict([[25, 150]])) . [0.] . import matplotlib.pyplot as plt plt.scatter(train_input[:, 0], train_input[:, 1]) plt.scatter(25, 150, marker = &#39;^&#39;) # 새로운 데이터를 marker = &#39;^&#39;로 지정하여 삼각형으로 표현함. --&gt; 구분하기 쉬움 plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() . distances, indexes = kn.kneighbors([[25, 150]]) # 주어진 샘플의 이웃 샘플을 따로 구분해서 산점도를 그린다. plt.scatter(train_input[ : , 0], train_input[ : , 1]) plt.scatter(25, 150 , marker = &#39;^&#39;) plt.scatter(train_input[indexes, 0], train_input[indexes, 1], marker = &#39;D&#39;) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() print(train_input[indexes]) print(train_target[indexes]) print(distances) print(indexes) # [[25, 150]]의 샘플은 도미 데이터를 1개밖에 포함하지 않는다. . [[[ 37. 1000. ] [ 41. 975. ] [ 38.5 955. ] [ 39.5 925. ] [ 36. 850. ]]] [[1. 1. 1. 1. 1.]] [[150.24524979 150.25805338 150.35871414 150.43446443 150.71926768]] [[10 35 17 4 7]] . # x축의 길이와 y축의 길이를 같게 해서 산점도를 그려보자. --&gt; xlim() = x축 범위 지정 , ylim() = y축 범위 지정 plt.scatter(train_input[ : , 0], train_input[ : , 1]) plt.scatter(25, 150 , marker = &#39;^&#39;) plt.scatter(train_input[indexes, 0], train_input[indexes, 1], marker = &#39;D&#39;) plt.xlim((0, 1000)) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() # weight 와 length 두 특성의 값이 놓인 범위가 매우 다르다. --&gt; 두 특성의 스케일이 다르다. # 두 특성의 스케일이 다르면 알고리즘이 올바르게 예측할 수 없다. --&gt; 특성값을 일정한 기준으로 맞춰주어야 한다. --&gt; 데이터 전처리를 한다. . &#54364;&#51456;&#51216;&#49688;(standard score) --&gt; &#45936;&#51060;&#53552; &#51204;&#52376;&#47532; . 각 특성값이 평균에서 표준편차의 몇 배만큼 떨어져 있는지를 나타낸다. | 평균을 빼고 표준편차로 나눈다. | 특성마다 값의 스케일이 다르므로 평균과 표준편차는 각 특성별로 계산해야 한다. | 테스트 세트를 스케일할때에도 훈련 세트의 평균과 표준편차를 이용하여 스케일 해야 한다. | . # 평균을 구하는 넘파이 함수 --&gt; np.mean() mean = np.mean(train_input, axis = 0) # 표준편차를 구하는 넘파이 함수 --&gt; np.std() std = np.std(train_input, axis = 0) mean, std . (array([ 27.29722222, 454.09722222]), array([ 9.98244253, 323.29893931])) . train_scaled = (train_input - mean) / std . plt.scatter(train_scaled[ :, 0], train_scaled[ :, 1]) plt.scatter(25, 150, marker = &#39;^&#39;) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() # 새로운 데이터는 전처리가 안돼있어서 따로 멀리 떨어져나옴 --&gt; 새로운 데이터도 전처리 . new = ([25, 150] - mean) / std plt.scatter(train_scaled[ :, 0], train_scaled[ :, 1]) plt.scatter(new[0], new[1], marker = &#39;^&#39;) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() #새로운 데이터도 올바르게 표현됨 . kn.fit(train_scaled, train_target) . KNeighborsClassifier(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;, metric_params=None, n_jobs=None, n_neighbors=5, p=2, weights=&#39;uniform&#39;) . test_scaled = (test_input - mean) / std . kn.score(test_scaled, test_target) . 1.0 . print(kn.predict([new])) # 수상한 도미 한 마리와 가장 근접한 이웃 데이터 확인하기 distances, indexes = kn.kneighbors([new]) plt.scatter(train_scaled[ :, 0], train_scaled[ :, 1]) plt.scatter(new[0], new[1], marker = &#39;^&#39;) plt.scatter(train_scaled[indexes, 0], train_scaled[indexes, 1], marker = &#39;D&#39;) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() . [1.] . &#50672;&#49845; . fish_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0, 9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0] fish_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0, 6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9] # train 세트 test 세트 분류 import numpy as np fish_data = np.column_stack((fish_length, fish_weight)) fish_target = np.concatenate((np.ones(35), np.zeros(14))) from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(fish_data, fish_target, stratify = fish_target, random_state = 42) # 데이터 전처리 mean = np.mean(train_input, axis = 0) std = np.std(train_input, axis = 0) train_scaled = (train_input - mean) / std test_scaled = (test_input - mean) / std # 모델링 from sklearn.neighbors import KNeighborsClassifier kn = KNeighborsClassifier() kn.fit(train_scaled, train_target) kn.score(test_scaled, test_target) # 수상한 도미 한 마리 --&gt; new = [25,150] new = [25, 150] scaled_new = ([25, 150] - mean) / std distances, indexes = kn.kneighbors([scaled_new]) import matplotlib.pyplot as plt plt.scatter(train_scaled[ :, 0], train_scaled[ :, 1]) plt.scatter(scaled_new[0], scaled_new[1], marker = &#39;^&#39;) plt.scatter(train_scaled[indexes, 0], train_scaled[indexes, 1]) plt.show() kn.predict([scaled_new]) . array([1.]) .",
            "url": "https://parkjeongung.github.io/Ung.github.io/2021/12/26/%EB%8D%B0%EC%9D%B4%ED%84%B0_%EC%A0%84%EC%B2%98%EB%A6%AC.html",
            "relUrl": "/2021/12/26/%EB%8D%B0%EC%9D%B4%ED%84%B0_%EC%A0%84%EC%B2%98%EB%A6%AC.html",
            "date": " • Dec 26, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "인공지능 머신러닝 딥러닝",
            "content": ". K - &#52572;&#44540;&#51217;&#51060;&#50883; // from sklearn.neighbors import KNeighborsClassifier . 내가 지정한 데이터와 그 주변에 있는 데이터를 비교해서 결과를 예측한다. | 새로운 데이터를 예측할 때 그 주변 직선거리로 가장 가까운 데이터와 비교한다. | 데이터가 아주 많은 경우 사용하기 어렵다. 직선거리를 계산하는데 많은 시간이 들기 때문이다. | 참고 데이터 기본값은 5이다. n_neighbors로 변경 가능하다 | bream_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0] bream_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0] smelt_length = [9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0] smelt_weight = [6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9] import matplotlib.pyplot as plt plt.scatter(bream_length, bream_weight) plt.scatter(smelt_length, smelt_weight) plt.xlabel(&#39;bream_length&#39;) plt.ylabel(&#39;bream_weight&#39;) plt.show() . bream_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0] bream_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0] smelt_length = [9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0] smelt_weight = [6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9] length = bream_length + smelt_length weight = bream_weight + smelt_weight . fish_data = [[l, w] for l, w in zip(length, weight)] . fish_target = [1] * 35 + [0] * 14 . from sklearn.neighbors import KNeighborsClassifier kn = KNeighborsClassifier() . kn.fit(fish_data, fish_target) . KNeighborsClassifier(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;, metric_params=None, n_jobs=None, n_neighbors=5, p=2, weights=&#39;uniform&#39;) . kn.score(fish_data, fish_target) . kn.predict([[30, 600]]) . kn49 = KNeighborsClassifier(n_neighbors=49) kn49.fit(fish_data, fish_target) kn49.score(fish_data, fish_target) . 0.7142857142857143 . &#50672;&#49845; . bream_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0] bream_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0] smelt_length = [9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0] smelt_weight = [6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9] length = bream_length + smelt_length weight = bream_weight + smelt_weight fish_data = [[l,w] for l,w in zip(length, weight)] fish_target = [1] * 35 + [0] * 14 from sklearn.neighbors import KNeighborsClassifier kn = KNeighborsClassifier() kn.fit(fish_data, fish_target) kn.score(fish_data, fish_target) kn.predict([[30, 600]]) . array([1]) .",
            "url": "https://parkjeongung.github.io/Ung.github.io/2021/12/26/K_%EC%B5%9C%EA%B7%BC%EC%A0%91%EC%9D%B4%EC%9B%83_KNeighborsClassifier.html",
            "relUrl": "/2021/12/26/K_%EC%B5%9C%EA%B7%BC%EC%A0%91%EC%9D%B4%EC%9B%83_KNeighborsClassifier.html",
            "date": " • Dec 26, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://parkjeongung.github.io/Ung.github.io/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://parkjeongung.github.io/Ung.github.io/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://parkjeongung.github.io/Ung.github.io/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}