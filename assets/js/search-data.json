{
  
    
        "post0": {
            "title": "인공지능 머신러닝 딥러닝",
            "content": ". K - &#52572;&#44540;&#51217;&#51060;&#50883; // from sklearn.neighbors import KNeighborsClassifier . 내가 지정한 데이터와 그 주변에 있는 데이터를 비교해서 결과를 예측한다. | 새로운 데이터를 예측할 때 그 주변 직선거리로 가장 가까운 데이터와 비교한다. | 데이터가 아주 많은 경우 사용하기 어렵다. 직선거리를 계산하는데 많은 시간이 들기 때문이다. | 참고 데이터 기본값은 5이다. n_neighbors로 변경 가능하다 | bream_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0] bream_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0] smelt_length = [9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0] smelt_weight = [6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9] import matplotlib.pyplot as plt plt.scatter(bream_length, bream_weight) plt.scatter(smelt_length, smelt_weight) plt.xlabel(&#39;bream_length&#39;) plt.ylabel(&#39;bream_weight&#39;) plt.show() . bream_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0] bream_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0] smelt_length = [9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0] smelt_weight = [6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9] length = bream_length + smelt_length weight = bream_weight + smelt_weight . fish_data = [[l, w] for l, w in zip(length, weight)] . fish_target = [1] * 35 + [0] * 14 . from sklearn.neighbors import KNeighborsClassifier kn = KNeighborsClassifier() . kn.fit(fish_data, fish_target) . KNeighborsClassifier(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;, metric_params=None, n_jobs=None, n_neighbors=5, p=2, weights=&#39;uniform&#39;) . kn.score(fish_data, fish_target) . kn.predict([[30, 600]]) . kn49 = KNeighborsClassifier(n_neighbors=49) kn49.fit(fish_data, fish_target) kn49.score(fish_data, fish_target) . 0.7142857142857143 . &#50672;&#49845; . bream_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0] bream_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0] smelt_length = [9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0] smelt_weight = [6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9] length = bream_length + smelt_length weight = bream_weight + smelt_weight fish_data = [[l,w] for l,w in zip(length, weight)] fish_target = [1] * 35 + [0] * 14 from sklearn.neighbors import KNeighborsClassifier kn = KNeighborsClassifier() kn.fit(fish_data, fish_target) kn.score(fish_data, fish_target) kn.predict([[30, 600]]) . array([1]) . &#45936;&#51060;&#53552; &#45796;&#47336;&#44592; . 지도 학습과 비지도 학습 | 지도학습 --&gt; 입력(input)데이터와 타깃(target)데이터 2가지를 합친 훈련 데이터를 가지고있다. | 비지도학습 --&gt; 타깃데이터 없이 입력데이터만 사용한다. | . 훈련 세트와 테스트 세트 | 훈련 세트 --&gt; 모델의 훈련을 위해 사용되는 데이터 | 테스트 세트 --&gt; 훈련된 모델의 평가에 사용되는 데이터 --&gt; 일반적으로 이미 준비된 데이터의 일부를 떼어서 사용 | . &#54984;&#47144;&#49464;&#53944;&#50752; &#53580;&#49828;&#53944;&#49464;&#53944; . 샘플링편향 발생 . fish_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0, 9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0] fish_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0, 6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9] . fish_data = [[l, w] for l, w in zip(fish_length, fish_weight)] fish_target = [1] * 35 + [0] * 14 . from sklearn.neighbors import KNeighborsClassifier kn = KNeighborsClassifier() . train_input = fish_data[: 35] train_target = fish_target[: 35] test_input = fish_data[35: ] test_target = fish_target[35: ] . # 테스트세트 --&gt; score()함수로 모델 평가 kn = kn.fit(train_input, train_target) kn.score(test_input, test_target) #샘플링 편향 발생 --&gt; 마지막 14개(빙어 특성 14개)를 test_input 으로 넣어놔서 훈련에 사용된 train_input 에는 빙어가 하나도 없음 --&gt; 데이터를 섞든지 골고루 샘플을 뽑아야 함 --&gt; numpy 사용 . 0.0 . NUMPY&#47484; &#51060;&#50857;&#54620; &#49368;&#54540;&#47553; &#54200;&#54693; &#54644;&#44208; . 파이썬의 대표적인 배열 라이브러리 | 고차원의 배열을 쉽게 만들고 조작가능 | . import numpy as np . input_arr = np.array(fish_data) target_arr = np.array(fish_target) . print(input_arr.shape) . (49, 2) . # input 과 target 에서 같은 인덱스는 함께 선택되어야 한다. input의 2번은 train으로 target의 2번은 test로 가면 안된다. # 넘파이의 random 함수들은 실행할 때마다 다른 결과를 만든다. --&gt; random.seed()를 지정하면 항상 일정한 결과를 얻을 수 있다. np.random.seed(42) index = np.arange(49) np.random.shuffle(index) index . array([13, 45, 47, 44, 17, 27, 26, 25, 31, 19, 12, 4, 34, 8, 3, 6, 40, 41, 46, 15, 9, 16, 24, 33, 30, 0, 43, 32, 5, 29, 11, 36, 1, 21, 2, 37, 35, 23, 39, 10, 22, 18, 48, 20, 7, 42, 14, 28, 38]) . print(input_arr[[1,3]]) # input_arr의 2번째와 4번째 원소 출력 # numpy 배열을 인덱스로 전하기 --&gt; 훈련세트 생성 train_input = input_arr[index [ : 35]] train_target = input_arr[index [ : 35]] #랜덤으로 만들어진 index의 첫번째 원소는 13 --&gt; input_arr의 14번째 원소(index = 13)가 train_input의 1번째 원소(index = 0)에 들어감 print(input_arr[13], train_input[0]) # numpy 배열을 인덱스로 전하기 --&gt; 테스트세트 생성 test_input = input_arr[index [35 : ]] test_target = input_arr[index [35 : ]] #랜덤으로 만들어진 index의 35번째 원소는 37 --&gt; input_arr의 38번째 원소(indexe = 37)가 test_input의 1번째 원소(index = 0)에 들어감 print(input_arr[37], test_input[0]) . [[ 26.3 290. ] [ 29. 363. ]] [ 32. 340.] [ 32. 340.] [10.6 7. ] [10.6 7. ] . import matplotlib.pyplot as plt plt.scatter(train_input[ :, 0], train_input[ :, 1]) plt.scatter(test_input[ :, 0], test_input[ :, 1]) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() . &#50672;&#49845; . fish_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0, 9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0] fish_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0, 6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9] fish_data = [[l, w] for l, w in zip(fish_length, fish_weight)] fish_target = [1] * 35 + [0] * 14 import numpy as np input_arr = np.array(fish_data) target_arr = np.array(fish_target) np.random.seed(42) index = np.arange(49) np.random.shuffle(index) train_input = input_arr[index[ : 35]] train_target = target_arr[index [ : 35]] test_input = input_arr[index[35 :]] test_target = target_arr[index[35 :]] from sklearn.neighbors import KNeighborsClassifier kn = KNeighborsClassifier() kn.fit(train_input, train_target) kn.score(test_input, test_target) . 1.0 . &#45936;&#51060;&#53552; &#51204;&#52376;&#47532; . fish_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0, 9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0] fish_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0, 6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9] . &#45336;&#54028;&#51060;&#47196; input &#45936;&#51060;&#53552;&#50752; target &#45936;&#51060;&#53552; &#47564;&#46308;&#44592; . import numpy as np fish_data = np.column_stack((fish_length, fish_weight)) # target 데이터 생성 --&gt; np.ones(), np.zeros() # 두개의 배열을 연결한다 --&gt; np.concatenate(()) fish_target = np.concatenate((np.ones(35), np.zeros(14))) . &#49324;&#51060;&#53431;&#47088;&#51004;&#47196; &#54984;&#47144;&#49464;&#53944;&#50752; &#53580;&#49828;&#53944;&#49464;&#53944; &#45208;&#45572;&#44592; . from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(fish_data, fish_target, random_state = 42) # 잘 나누어졌는지 shpae로 데이터크기 출력 print(train_input.shape, test_input.shape) print(train_target.shape, test_target.shape) # 전체 데이터의 도미와 빙어의 비율 = 35 (도미) : 14 (빙어) / 2.5 : 1 # 테스트 세트의 도미와 빙어의 비율 = 10 (도미) : 3 (빙어) / 3.3 : 1 # 샘플링 편향 발생 print(test_target) . (36, 2) (13, 2) (36,) (13,) [1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.] . # stratify 매개변수에 target데이터를 전달하면 클래스 비율에 맞게 데이터를 나눈다. from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(fish_data, fish_target, stratify = fish_target, random_state = 42) # 전체 데이터의 도미와 빙어의 비율 = 35 (도미) : 14 (빙어) / 2.5 : 1 # 테스트 세트의 도미와 빙어의 비율 = 9 (도미) : 4 (빙어) / 2.25 : 1 # 데이터가 작아 비율을 동일하게 맞출 수 없지만 꽤 올바른 샘플링 print(test_target) . [0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1.] . &#49688;&#49345;&#54620; &#46020;&#48120; &#54620; &#47560;&#47532; . [[25, 150]] 이라는 도미 데이터를 예측값으로 넣었더니 빙어로 출력이 된다. | 산점도에 있어서 직관적으로는 [[25, 150]] 데이터가 도미에 가까워 보이지만 x축의 범위와 y축의 범위를 생각하면 [[25, 150]] 데이터가 빙어데이터에 가까운것이 합리적이다. | 두 특성의 스케일이 다르면 알고리즘이 올바른 예측을 하지 못한다. --&gt; 데이터 전처리를 통해 특성값을 일정한 기준으로 맞춰준다. | . from sklearn.neighbors import KNeighborsClassifier kn = KNeighborsClassifier() kn.fit(train_input, train_target) kn.score(test_input, test_target) # [[25, 150]] 이라는 도미 데이터를 예측값으로 넣었더니 빙어[0]로 출력이 된다. print(kn.predict([[25, 150]])) . [0.] . import matplotlib.pyplot as plt plt.scatter(train_input[:, 0], train_input[:, 1]) plt.scatter(25, 150, marker = &#39;^&#39;) # 새로운 데이터를 marker = &#39;^&#39;로 지정하여 삼각형으로 표현함. --&gt; 구분하기 쉬움 plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() . distances, indexes = kn.kneighbors([[25, 150]]) # 주어진 샘플의 이웃 샘플을 따로 구분해서 산점도를 그린다. plt.scatter(train_input[ : , 0], train_input[ : , 1]) plt.scatter(25, 150 , marker = &#39;^&#39;) plt.scatter(train_input[indexes, 0], train_input[indexes, 1], marker = &#39;D&#39;) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() print(train_input[indexes]) print(train_target[indexes]) print(distances) print(indexes) # [[25, 150]]의 샘플은 도미 데이터를 1개밖에 포함하지 않는다. . [[[ 37. 1000. ] [ 41. 975. ] [ 38.5 955. ] [ 39.5 925. ] [ 36. 850. ]]] [[1. 1. 1. 1. 1.]] [[150.24524979 150.25805338 150.35871414 150.43446443 150.71926768]] [[10 35 17 4 7]] . # x축의 길이와 y축의 길이를 같게 해서 산점도를 그려보자. --&gt; xlim() = x축 범위 지정 , ylim() = y축 범위 지정 plt.scatter(train_input[ : , 0], train_input[ : , 1]) plt.scatter(25, 150 , marker = &#39;^&#39;) plt.scatter(train_input[indexes, 0], train_input[indexes, 1], marker = &#39;D&#39;) plt.xlim((0, 1000)) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() # weight 와 length 두 특성의 값이 놓인 범위가 매우 다르다. --&gt; 두 특성의 스케일이 다르다. # 두 특성의 스케일이 다르면 알고리즘이 올바르게 예측할 수 없다. --&gt; 특성값을 일정한 기준으로 맞춰주어야 한다. --&gt; 데이터 전처리를 한다. . &#54364;&#51456;&#51216;&#49688;(standard score) --&gt; &#45936;&#51060;&#53552; &#51204;&#52376;&#47532; . 각 특성값이 평균에서 표준편차의 몇 배만큼 떨어져 있는지를 나타낸다. | 평균을 빼고 표준편차로 나눈다. | 특성마다 값의 스케일이 다르므로 평균과 표준편차는 각 특성별로 계산해야 한다. | 테스트 세트를 스케일할때에도 훈련 세트의 평균과 표준편차를 이용하여 스케일 해야 한다. | . # 평균을 구하는 넘파이 함수 --&gt; np.mean() mean = np.mean(train_input, axis = 0) # 표준편차를 구하는 넘파이 함수 --&gt; np.std() std = np.std(train_input, axis = 0) mean, std . (array([ 27.29722222, 454.09722222]), array([ 9.98244253, 323.29893931])) . train_scaled = (train_input - mean) / std . plt.scatter(train_scaled[ :, 0], train_scaled[ :, 1]) plt.scatter(25, 150, marker = &#39;^&#39;) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() # 새로운 데이터는 전처리가 안돼있어서 따로 멀리 떨어져나옴 --&gt; 새로운 데이터도 전처리 . new = ([25, 150] - mean) / std plt.scatter(train_scaled[ :, 0], train_scaled[ :, 1]) plt.scatter(new[0], new[1], marker = &#39;^&#39;) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() #새로운 데이터도 올바르게 표현됨 . kn.fit(train_scaled, train_target) . KNeighborsClassifier(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;, metric_params=None, n_jobs=None, n_neighbors=5, p=2, weights=&#39;uniform&#39;) . test_scaled = (test_input - mean) / std . kn.score(test_scaled, test_target) . 1.0 . print(kn.predict([new])) # 수상한 도미 한 마리와 가장 근접한 이웃 데이터 확인하기 distances, indexes = kn.kneighbors([new]) plt.scatter(train_scaled[ :, 0], train_scaled[ :, 1]) plt.scatter(new[0], new[1], marker = &#39;^&#39;) plt.scatter(train_scaled[indexes, 0], train_scaled[indexes, 1], marker = &#39;D&#39;) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() . [1.] . &#50672;&#49845; . fish_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0, 9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0] fish_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0, 6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9] # train 세트 test 세트 분류 import numpy as np fish_data = np.column_stack((fish_length, fish_weight)) fish_target = np.concatenate((np.ones(35), np.zeros(14))) from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(fish_data, fish_target, stratify = fish_target, random_state = 42) # 데이터 전처리 mean = np.mean(train_input, axis = 0) std = np.std(train_input, axis = 0) train_scaled = (train_input - mean) / std test_scaled = (test_input - mean) / std # 모델링 from sklearn.neighbors import KNeighborsClassifier kn = KNeighborsClassifier() kn.fit(train_scaled, train_target) kn.score(test_scaled, test_target) # 수상한 도미 한 마리 --&gt; new = [25,150] new = [25, 150] scaled_new = ([25, 150] - mean) / std distances, indexes = kn.kneighbors([scaled_new]) import matplotlib.pyplot as plt plt.scatter(train_scaled[ :, 0], train_scaled[ :, 1]) plt.scatter(scaled_new[0], scaled_new[1], marker = &#39;^&#39;) plt.scatter(train_scaled[indexes, 0], train_scaled[indexes, 1]) plt.show() kn.predict([scaled_new]) . array([1.]) . K - &#52572;&#44540;&#51217; &#51060;&#50883; &#54924;&#44480; // KNeighborsRegressor . &#54924;&#44480; . 임의의 어떤 숫자를 예측 | 두 변수 사이의 상관관계를 분석하는 방법 | . import numpy as np # 농어의 특성 = 길이 perch_length = np.array([8.4, 13.7, 15.0, 16.2, 17.4, 18.0, 18.7, 19.0, 19.6, 20.0, 21.0, 21.0, 21.0, 21.3, 22.0, 22.0, 22.0, 22.0, 22.0, 22.5, 22.5, 22.7, 23.0, 23.5, 24.0, 24.0, 24.6, 25.0, 25.6, 26.5, 27.3, 27.5, 27.5, 27.5, 28.0, 28.7, 30.0, 32.8, 34.5, 35.0, 36.5, 36.0, 37.0, 37.0, 39.0, 39.0, 39.0, 40.0, 40.0, 40.0, 40.0, 42.0, 43.0, 43.0, 43.5, 44.0]) # 농어의 타겟 = 무게 perch_weight = np.array([5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0, 115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0, 150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0, 218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0, 556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0, 850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0, 1000.0]) . import matplotlib.pyplot as plt plt.scatter(perch_length, perch_weight) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() # 농어의 길이가 길어질수록 무게가 늘어난다. . from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(perch_length, perch_weight, random_state = 42) . # reshape 메서드는 크기에 -1 을 지정하면 남은 원소 개수로 모두 채우라는 의미 train_input = train_input.reshape(-1, 1) # == train_input.reshape(42, 1) print(train_input.shape) test_input = test_input.reshape(-1, 1) print(test_input.shape) . (42, 1) (14, 1) . from sklearn.neighbors import KNeighborsRegressor knr = KNeighborsRegressor() knr.fit(train_input, train_target) print(knr.score(test_input, test_target)) # 스코어가 1이 안나왔다 --&gt; 회귀에서는 정확한 숫자를 맞힌다는 것이 거의 불가능하다. --&gt; 예측하는 값이 모두 임의의 수치이기 때문 . 0.9928094061010639 . from sklearn.metrics import mean_absolute_error # 테스트 세트의 예측값을 만든다. test_prediction = knr.predict(test_input) # 테스트 세트에 대한 절댓값 오차의 평균을 구한다. mae = mean_absolute_error(test_target, test_prediction) mae # 19그람 정도의 오차가 발생한다. . 19.157142857142862 . &#54984;&#47144; &#49464;&#53944;&#47484; &#49324;&#50857;&#54644; &#47784;&#45944; &#54217;&#44032;&#54616;&#44592; // &#44284;&#49548;&#51201;&#54633;, &#44284;&#45824;&#51201;&#54633; . score(train) &gt; score(test) --&gt; 모델이 훈련세트에 과대적합 되었다. --&gt; 실전에 투입하면 예측이 잘 안됨 --&gt; 모델을 덜 복잡하게 만들어 해결한다. --&gt; 이웃의 개수를 늘린다 | score(train) &lt; score(test) --&gt; 모델이 훈련세트에 과소적합 되었다. --&gt; 모델이 너무 단순하다. --&gt; 모델을 더 복잡하게 만들어 해결한다. --&gt; 이웃의 개수를 줄인다. | . print(knr.score(train_input, train_target)) # 테스트 세트로 점수 확인하기 print(knr.score(test_input, test_target)) # score(train) &lt; score(test) --&gt; 과소적합 --&gt; 데이터가 작을경우 과소적합이 발생할 수 있다. . 0.9698823289099255 0.9928094061010639 . knr.n_neighbors = 3 knr.fit(train_input, train_target) print(knr.score(train_input, train_target)) print(knr.score(test_input, test_target)) # 이웃의 개수를 줄였더니 train 세트와 test 세트의 score가 비슷하게 나온다. . 0.9804899950518966 0.974645996398761 . &#50672;&#49845; . import numpy as np perch_length = np.array([8.4, 13.7, 15.0, 16.2, 17.4, 18.0, 18.7, 19.0, 19.6, 20.0, 21.0, 21.0, 21.0, 21.3, 22.0, 22.0, 22.0, 22.0, 22.0, 22.5, 22.5, 22.7, 23.0, 23.5, 24.0, 24.0, 24.6, 25.0, 25.6, 26.5, 27.3, 27.5, 27.5, 27.5, 28.0, 28.7, 30.0, 32.8, 34.5, 35.0, 36.5, 36.0, 37.0, 37.0, 39.0, 39.0, 39.0, 40.0, 40.0, 40.0, 40.0, 42.0, 43.0, 43.0, 43.5, 44.0]) # 농어의 타겟 = 무게 perch_weight = np.array([5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0, 115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0, 150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0, 218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0, 556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0, 850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0, 1000.0]) # train test split from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(perch_length, perch_weight, random_state = 42) # 2차원 배열로 변형 train_input = train_input.reshape(-1, 1) test_input = test_input.reshape(-1, 1) # 모델링 from sklearn.neighbors import KNeighborsRegressor knr = KNeighborsRegressor() knr.fit(train_input, train_target) # 스코어 knr.score(test_input, test_target) # 오차확인 mean absoulte error from sklearn.metrics import mean_absolute_error test_prediction = knr.predict(test_input) mae = mean_absolute_error(test_target, test_prediction) # 과소적합 --&gt; 테스트세트 스코어가 트레인세트 스코어보다 높다 print(knr.score(train_input, train_target)) print(knr.score(test_input, test_target)) # 과소적합 해결 --&gt; 모델을 복잡하게 만든다. --&gt; 이웃수 줄이기 knr.n_neighbors = 3 knr.fit(train_input, train_target) print(knr.score(train_input, train_target)) print(knr.score(test_input, test_target)) . 0.9698823289099255 0.9928094061010639 0.9804899950518966 0.974645996398761 . # n_neighbors = [1, 5, 10] # 농어의 길이를 5에서 45까지 바꿔가며 모델링 결과 확인하기 from sklearn.neighbors import KNeighborsRegressor import matplotlib.pyplot as plt knr = KNeighborsRegressor() x = np.arange(5, 45).reshape(-1, 1) for n in [1, 5, 10]: knr.n_neighbors = n knr.fit(train_input, train_target) predict = knr.predict(x) plt.scatter(train_input, train_target) plt.scatter(x, predict, marker = &#39;^&#39;) plt.title(&quot;n_neighbors = {}&quot;.format(n)) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() . &#49440;&#54805; &#54924;&#44480; // LinearRegression . import numpy as np perch_length = np.array([8.4, 13.7, 15.0, 16.2, 17.4, 18.0, 18.7, 19.0, 19.6, 20.0, 21.0, 21.0, 21.0, 21.3, 22.0, 22.0, 22.0, 22.0, 22.0, 22.5, 22.5, 22.7, 23.0, 23.5, 24.0, 24.0, 24.6, 25.0, 25.6, 26.5, 27.3, 27.5, 27.5, 27.5, 28.0, 28.7, 30.0, 32.8, 34.5, 35.0, 36.5, 36.0, 37.0, 37.0, 39.0, 39.0, 39.0, 40.0, 40.0, 40.0, 40.0, 42.0, 43.0, 43.0, 43.5, 44.0]) # 농어의 타겟 = 무게 perch_weight = np.array([5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0, 115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0, 150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0, 218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0, 556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0, 850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0, 1000.0]) # train test split from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(perch_length, perch_weight, random_state = 42) # 2차원 변형 train_input = train_input.reshape(-1, 1) test_input = test_input.reshape(-1, 1) # 모델링 --&gt; 이웃개수 3개인 최근접회귀 from sklearn.neighbors import KNeighborsRegressor knr = KNeighborsRegressor(n_neighbors = 3) knr.fit(train_input, train_target) . KNeighborsRegressor(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;, metric_params=None, n_jobs=None, n_neighbors=3, p=2, weights=&#39;uniform&#39;) . 50cm &#51064; &#45453;&#50612;&#51032; &#47924;&#44172; &#50696;&#52769; . print(knr.predict([[50]])) # 그러나 실제 50cm인 농어의 무게와 오차가 심하다는 예시 . [1033.33333333] . distancecs, indexes = knr.kneighbors([[50]]) predict = knr.predict([[50]]) import matplotlib.pyplot as plt plt.scatter(train_input, train_target) plt.scatter(50, predict, marker = &#39;^&#39;) plt.scatter(train_input[indexes], train_target[indexes], marker = &#39;D&#39;) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() # 훈련세트의 범위를 벗어났기 때문에 엉뚱한 값을 출력함 --&gt; 100cm 농어의 경우도 똑같은 무게를 출력 print(knr.predict([[100]])) . [1033.33333333] . &#49440;&#54805; &#54924;&#44480; &#47784;&#45944;&#47553; . KNeighborsRegressor로 위 문제를 해결하려면 train 세트를 길이가 긴 농어가 포함되도록 다시 만들어야 한다. | 선형 회귀를 이용하면 train 세트의 범위를 벗어나도 길이가 긴 농어의 무게를 예측할 수 있다. | 선형 회귀에서 알고리즘이 찾아낸 직선의 기울기나 y절편 값을 &quot;모델 파라미터&quot; 라고 한다. | . from sklearn.linear_model import LinearRegression lr = LinearRegression() lr.fit(train_input, train_target) # 50cm 농어의 무게 예측값과 100cm 농어의 무게 예측값 print(lr.predict([[50], [100]])) . [1241.83860323 3192.69585141] . # 이러한 파라미터들은 coef_(기울기) 와 intercept(y절편) 에 저장되어 있다. # 머신러닝에서 &quot;기울기&quot;는 &quot;계수&quot; 또는 &quot;가중치&quot; 라고도 한다. print(lr.coef_, lr.intercept_) # 훈련세트와 농어의 길이 15 ~ 50를 기준으로 알고리즘이 찾아낸 직선 그리기 import matplotlib.pyplot as plt plt.scatter(train_input, train_target) # 15 ~ 50까지 직선 그리기 plt.plot([15, 50], [15 * lr.coef_ + lr.intercept_, 50 * lr.coef_ + lr.intercept_]) # 50cm 농어 데이터 predict = lr.predict([[50]]) plt.scatter(50, predict, marker = &#39;^&#39;) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() . [39.01714496] -709.0186449535477 . print(lr.score(train_input, train_target)) print(lr.score(test_input, test_target)) # 테스트세트의 점수가 현저히 낮은 과대적합 # 훈련세트의 점수 또한 높은 편이 아니기에 전체적으로 과소적합 되었다. # 직선을 확인했을 때 X = 15 인 구간에서 Y 값이 0 이하가 나온다. --&gt; 현실에서는 불가능 . 0.9398463339976039 0.8247503123313558 . &#45796;&#54637; &#54924;&#44480; . import matplotlib.pyplot as plt plt.scatter(train_input, train_target) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() # train세트의 산점도는 일직선 이라기 보다는 곡선의 형태에 가깝다 --&gt; 2차방정식의 그래프를 그려보자 --&gt; y = ax^2 + bx + c . # np.column_stack() 을 이용해서 train세트의 제곱과 train세트 2배열을 나란히 붙인다. --&gt; test세트도 마찬가지 train_poly = np.column_stack((train_input ** 2, train_input)) test_poly = np.column_stack((test_input ** 2, test_input)) print(train_poly.shape, test_poly.shape) # 모든 원소 제곱이 하나의 열을 이루었기 때문에 열이 2개로 늘어났다. . (42, 2) (14, 2) . from sklearn.linear_model import LinearRegression lr = LinearRegression() lr.fit(train_poly, train_target) print(lr.predict([[50 ** 2, 50]])) . [1573.98423528] . print(lr.coef_, lr.intercept_) # trian 세트 산점도를 그리고 그 위에 계수와 절편을 이용해 모델링에 사용된 곡선 그리기 x = np.arange(15, 50) # 15cm ~ 50cm길이의 농어 배열 생성 import matplotlib.pyplot as plt plt.scatter(train_input, train_target) plt.plot(x, lr.coef_[0] * x ** 2 + lr.coef_[1] * x + lr.intercept_) plt.scatter(50, lr.predict([[50 ** 2, 50]]), marker = &#39;^&#39;) plt.show() . [ 1.01433211 -21.55792498] 116.05021078278276 . print(lr.score(train_poly, train_target)) print(lr.score(test_poly, test_target)) # 이정도 점수차이면 양호하다고 볼 수 있는건가요? # 과소적합이 아직 조금은 남아있음 --&gt; 조금 더 복잡한 모델 생성 . 0.9706807451768623 0.9775935108325122 .",
            "url": "https://parkjeongung.github.io/Ung.github.io/2021/12/26/%ED%98%BC%EC%9E%90_%EA%B3%B5%EB%B6%80%ED%95%98%EA%B8%B0.html",
            "relUrl": "/2021/12/26/%ED%98%BC%EC%9E%90_%EA%B3%B5%EB%B6%80%ED%95%98%EA%B8%B0.html",
            "date": " • Dec 26, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://parkjeongung.github.io/Ung.github.io/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://parkjeongung.github.io/Ung.github.io/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://parkjeongung.github.io/Ung.github.io/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://parkjeongung.github.io/Ung.github.io/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}