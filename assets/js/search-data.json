{
  
    
        "post0": {
            "title": "그래프 한글 깨짐 현상 해결",
            "content": ". &#45936;&#51060;&#53552; &#47196;&#46300; . import pandas as pd path = &#39;/content/drive/MyDrive/mydata/&#39; # 변수지정 train = pd.read_csv(path+&#39;movies_train.csv&#39;) test = pd.read_csv(path+&#39;movies_test.csv&#39;) submission = pd.read_csv(path+&#39;submission.csv&#39;) train.head() . title distributor genre release_time time screening_rat director dir_prev_bfnum dir_prev_num num_staff num_actor box_off_num . 0 개들의 전쟁 | 롯데엔터테인먼트 | 액션 | 2012-11-22 | 96 | 청소년 관람불가 | 조병옥 | NaN | 0 | 91 | 2 | 23398 | . 1 내부자들 | (주)쇼박스 | 느와르 | 2015-11-19 | 130 | 청소년 관람불가 | 우민호 | 1161602.50 | 2 | 387 | 3 | 7072501 | . 2 은밀하게 위대하게 | (주)쇼박스 | 액션 | 2013-06-05 | 123 | 15세 관람가 | 장철수 | 220775.25 | 4 | 343 | 4 | 6959083 | . 3 나는 공무원이다 | (주)NEW | 코미디 | 2012-07-12 | 101 | 전체 관람가 | 구자홍 | 23894.00 | 2 | 20 | 6 | 217866 | . 4 불량남녀 | 쇼박스(주)미디어플렉스 | 코미디 | 2010-11-04 | 108 | 15세 관람가 | 신근호 | 1.00 | 1 | 251 | 2 | 483387 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; &#44536;&#47000;&#54532; &#44536;&#47532;&#44592; . import warnings warnings.filterwarnings(&quot;ignore&quot;) import matplotlib.pyplot as plt import seaborn as sns plt.rc(&#39;figure&#39;, facecolor = &#39;eaeaf2&#39;) def show_countplot(column): plt.figure(figsize=(12,4)) sns.countplot(data=train, x=column).set_title(column) plt.show() def show_distplot(column): plt.figure(figsize=(12, 4)) sns.distplot(train[column], bins=50) plt.show() . show_countplot(&#39;genre&#39;) # 한글이 깨져서 나오는걸 확인할 수 있음 . !sudo apt-get install -y fonts-nanum !sudo fc-cache -fv !rm ~/.cache/matplotlib -rf # STEP 2. 코랩 런타임 재시작 (단축키 CTRL + M + .) . Reading package lists... Done Building dependency tree Reading state information... Done The following NEW packages will be installed: fonts-nanum 0 upgraded, 1 newly installed, 0 to remove and 37 not upgraded. Need to get 9,604 kB of archives. After this operation, 29.5 MB of additional disk space will be used. Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 fonts-nanum all 20170925-1 [9,604 kB] Fetched 9,604 kB in 3s (3,215 kB/s) debconf: unable to initialize frontend: Dialog debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, &lt;&gt; line 1.) debconf: falling back to frontend: Readline debconf: unable to initialize frontend: Readline debconf: (This frontend requires a controlling tty.) debconf: falling back to frontend: Teletype dpkg-preconfigure: unable to re-open stdin: Selecting previously unselected package fonts-nanum. (Reading database ... 155229 files and directories currently installed.) Preparing to unpack .../fonts-nanum_20170925-1_all.deb ... Unpacking fonts-nanum (20170925-1) ... Setting up fonts-nanum (20170925-1) ... Processing triggers for fontconfig (2.12.6-0ubuntu2) ... /usr/share/fonts: caching, new cache contents: 0 fonts, 1 dirs /usr/share/fonts/truetype: caching, new cache contents: 0 fonts, 3 dirs /usr/share/fonts/truetype/humor-sans: caching, new cache contents: 1 fonts, 0 dirs /usr/share/fonts/truetype/liberation: caching, new cache contents: 16 fonts, 0 dirs /usr/share/fonts/truetype/nanum: caching, new cache contents: 10 fonts, 0 dirs /usr/local/share/fonts: caching, new cache contents: 0 fonts, 0 dirs /root/.local/share/fonts: skipping, no such directory /root/.fonts: skipping, no such directory /var/cache/fontconfig: cleaning cache directory /root/.cache/fontconfig: not cleaning non-existent cache directory /root/.fontconfig: not cleaning non-existent cache directory fc-cache: succeeded . plt.rc(&#39;font&#39;, family=&#39;NanumGothic&#39;) . show_countplot(&#39;genre&#39;) # 해결!! .",
            "url": "https://parkjeongung.github.io/Ung.github.io/2022/01/16/%EA%B7%B8%EB%9E%98%ED%94%84_%ED%95%9C%EA%B8%80_%EA%B9%A8%EC%A7%90_%ED%98%84%EC%83%81_%ED%95%B4%EA%B2%B0.html",
            "relUrl": "/2022/01/16/%EA%B7%B8%EB%9E%98%ED%94%84_%ED%95%9C%EA%B8%80_%EA%B9%A8%EC%A7%90_%ED%98%84%EC%83%81_%ED%95%B4%EA%B2%B0.html",
            "date": " • Jan 16, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "따릉이 대여량 예측 // 모델링 연습",
            "content": ". &#45936;&#51060;&#53552; &#48520;&#47084;&#50724;&#44592; . 학습에 필요한 데이터 불러오기 . date_time : 일별 날짜 | wind_direction: 풍향 (degree) | sky_condition : 하늘 상태 (하단 설명 참조) | precipitation_form : 강수 형태 (하단 설명 참조) | wind_speed : 풍속 (m/s) | humidity : 습도 (%) | low_temp : 최저 기온 ( `C) | high_temp : 최고 기온 ( `C) | Precipitation_Probability : 강수 확률 (%) | 기상 데이터는 하루에 총 8번 3시간 간격으로 발표되는 기상단기예보(SHRT) 데이터를 1일 평균으로 변환한 데이터입니다. | sky_condition (하늘 상태) 코드 : 맑음(1), 구름많음(3), 흐림(4) | precipitation_form (강수 형태) 코드 : 없음(0), 비(1), 진눈깨비(2), 눈(3), 소나기(4) | 위 데이터는 4월~6월의 기상 데이터만을 추출한 것이기 때문에 없음(0), 비(1)만 등장했습니다. 따라서 precipitation_form 이 0.5인 경우: &quot;하루의 절반은 비가 올 것으로 예측하고, 나머지 절반은 맑을 것으로 예측했다&quot;는 의미로 해석해주시기 바랍니다. | . import pandas as pd import numpy as np train = pd.read_csv(&#39;/content/drive/MyDrive/mydata/train.csv&#39;) train.head().T . 0 1 2 3 4 . date_time 2018-04-01 | 2018-04-02 | 2018-04-03 | 2018-04-04 | 2018-04-05 | . wind_direction 207.5 | 208.317 | 213.516 | 143.836 | 95.905 | . sky_condition 4 | 2.95 | 2.911 | 3.692 | 4 | . precipitation_form 0 | 0 | 0 | 0.425 | 0.723 | . wind_speed 3.05 | 3.278 | 2.69 | 3.138 | 3.186 | . humidity 75 | 69.833 | 74.879 | 71.849 | 73.784 | . low_temp 12.6 | 12.812 | 10.312 | 8.312 | 5.875 | . high_temp 21 | 19 | 15.316 | 12.368 | 10.421 | . Precipitation_Probability 30 | 19.5 | 19.113 | 43.493 | 63.378 | . number_of_rentals 22994 | 28139 | 26817 | 26034 | 2833 | . test = pd.read_csv(&#39;/content/drive/MyDrive/mydata/test.csv&#39;) submission = pd.read_csv(&#39;/content/drive/MyDrive/mydata/sample_submission.csv&#39;) # 테스트 데이터 결측치 확인 print(test.info()) print(test.shape) . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 91 entries, 0 to 90 Data columns (total 9 columns): # Column Non-Null Count Dtype -- -- 0 date_time 91 non-null object 1 wind_direction 91 non-null float64 2 sky_condition 91 non-null float64 3 precipitation_form 91 non-null float64 4 wind_speed 91 non-null float64 5 humidity 91 non-null float64 6 low_temp 91 non-null float64 7 high_temp 91 non-null float64 8 Precipitation_Probability 91 non-null float64 dtypes: float64(8), object(1) memory usage: 6.5+ KB None (91, 9) . &#45936;&#51060;&#53552; &#54869;&#51064; . train.info() print(train.shape) . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 273 entries, 0 to 272 Data columns (total 9 columns): # Column Non-Null Count Dtype -- -- 0 date_time 273 non-null object 1 sky_condition 273 non-null float64 2 precipitation_form 273 non-null float64 3 wind_speed 273 non-null float64 4 humidity 273 non-null float64 5 low_temp 273 non-null float64 6 high_temp 273 non-null float64 7 Precipitation_Probability 273 non-null float64 8 number_of_rentals 273 non-null int64 dtypes: float64(7), int64(1), object(1) memory usage: 19.3+ KB (273, 9) . def check_missing_col(dataframe) : counted_missing_col = 0 for i, col in enumerate(train.columns) : missing_values = sum(train[col].isna()) is_missing = True if missing_values &gt;= 1 else False if is_missing: counted_missing_col += 1 . train.describe().T . count mean std min 25% 50% 75% max . sky_condition 273.0 | 2.288256 | 0.961775 | 1.000 | 1.405 | 2.167 | 3.000 | 4.000 | . precipitation_form 273.0 | 0.100963 | 0.203193 | 0.000 | 0.000 | 0.000 | 0.088 | 1.000 | . wind_speed 273.0 | 2.480963 | 0.884397 | 0.753 | 1.820 | 2.411 | 2.924 | 5.607 | . humidity 273.0 | 56.745491 | 12.351268 | 24.831 | 47.196 | 55.845 | 66.419 | 88.885 | . low_temp 273.0 | 13.795249 | 5.107711 | 1.938 | 9.938 | 14.375 | 18.000 | 22.312 | . high_temp 273.0 | 23.384733 | 5.204605 | 9.895 | 19.842 | 24.158 | 27.526 | 33.421 | . Precipitation_Probability 273.0 | 16.878103 | 16.643772 | 0.000 | 4.054 | 12.162 | 22.973 | 82.162 | . number_of_rentals 273.0 | 59574.978022 | 27659.575774 | 1037.000 | 36761.000 | 63032.000 | 81515.000 | 110377.000 | . &#51068;&#51088; &#48516;&#47532; . 글자형이나 문자형을 숫자형으로 바꿔준다. . year = [] month = [] day = [] for date in train[&#39;date_time&#39;]: y_point, m_point, d_point = date.split(&#39;-&#39;) year.append(int(y_point)) month.append(int(m_point)) day.append(int(d_point)) train[&#39;year&#39;] = year train[&#39;month&#39;] = month train[&#39;day&#39;] = day train.head() . date_time wind_direction sky_condition precipitation_form wind_speed humidity low_temp high_temp Precipitation_Probability number_of_rentals year month day . 0 2018-04-01 | 207.500 | 4.000 | 0.000 | 3.050 | 75.000 | 12.600 | 21.000 | 30.000 | 22994 | 2018 | 4 | 1 | . 1 2018-04-02 | 208.317 | 2.950 | 0.000 | 3.278 | 69.833 | 12.812 | 19.000 | 19.500 | 28139 | 2018 | 4 | 2 | . 2 2018-04-03 | 213.516 | 2.911 | 0.000 | 2.690 | 74.879 | 10.312 | 15.316 | 19.113 | 26817 | 2018 | 4 | 3 | . 3 2018-04-04 | 143.836 | 3.692 | 0.425 | 3.138 | 71.849 | 8.312 | 12.368 | 43.493 | 26034 | 2018 | 4 | 4 | . 4 2018-04-05 | 95.905 | 4.000 | 0.723 | 3.186 | 73.784 | 5.875 | 10.421 | 63.378 | 2833 | 2018 | 4 | 5 | . year = [] month = [] day = [] for date in test[&#39;date_time&#39;]: y_point, m_point, d_point = date.split(&#39;-&#39;) year.append(int(y_point)) month.append(int(m_point)) day.append(int(d_point)) test[&#39;year&#39;] = year test[&#39;month&#39;] = month test[&#39;day&#39;] = day test.head() . date_time wind_direction sky_condition precipitation_form wind_speed humidity low_temp high_temp Precipitation_Probability year month day . 0 2021-04-01 | 108.833 | 3.000 | 0.000 | 2.900 | 28.333 | 11.800 | 20.667 | 18.333 | 2021 | 4 | 1 | . 1 2021-04-02 | 116.717 | 3.850 | 0.000 | 2.662 | 46.417 | 12.000 | 19.000 | 28.500 | 2021 | 4 | 2 | . 2 2021-04-03 | 82.669 | 4.000 | 0.565 | 2.165 | 77.258 | 8.875 | 16.368 | 52.847 | 2021 | 4 | 3 | . 3 2021-04-04 | 44.123 | 3.466 | 0.466 | 3.747 | 63.288 | 6.250 | 17.368 | 37.671 | 2021 | 4 | 4 | . 4 2021-04-05 | 147.791 | 1.500 | 0.000 | 1.560 | 48.176 | 7.188 | 18.684 | 4.459 | 2021 | 4 | 5 | . train = train.drop([&#39;wind_direction&#39;], axis = 1) test = test.drop([&#39;wind_direction&#39;], axis = 1) . &#47784;&#45944;&#47553; . x_train = train.drop([&#39;date_time&#39;, &#39;number_of_rentals&#39;], axis = 1) y_train = train.number_of_rentals . x_test = test.drop([&#39;date_time&#39;], axis = 1) . LinearRegression // 0.98369 . from sklearn.linear_model import LinearRegression lr = LinearRegression() lr.fit(x_train, y_train) linear_predict = lr.predict(x_train) . import matplotlib.pyplot as plt plt.style.use(&#39;ggplot&#39;) plt.figure(figsize = (20, 10)) plt.plot(linear_predict, label = &#39;prediction&#39;) plt.plot(y_train, label = &#39;real&#39;) plt.legend(fontsize = 20) plt.show() . KNeighborsRegressor . 스케일링 X = 0.82995 | 스케일링 O = 0.70720 | from sklearn.preprocessing import StandardScaler ss = StandardScaler() ss.fit(x_train) x_train_scaled = ss.transform(x_train) x_test_scaled = ss.transform(x_test) . from sklearn.neighbors import KNeighborsRegressor kn = KNeighborsRegressor() params = {&#39;n_neighbors&#39; : range(2, 150)} from sklearn.model_selection import GridSearchCV gs = GridSearchCV(kn,params, n_jobs = -1 ) gs.fit(x_train_scaled, y_train) kn = gs.best_estimator_ kn_predict = kn.predict(x_train_scaled) . import matplotlib.pyplot as plt plt.style.use(&#39;ggplot&#39;) plt.figure(figsize = (20, 10)) plt.plot(kn.predict(x_train), label = &#39;prediction&#39;) plt.plot(y_train, label = &#39;real&#39;) plt.legend(fontsize = 20) plt.show() . DecisionTreeRegressor // 0.62857 . from sklearn.model_selection import GridSearchCV from sklearn.tree import DecisionTreeRegressor params = {&#39;min_samples_split&#39; : range(2, 30), &#39;max_depth&#39; : range(10, 30), &#39;min_impurity_decrease&#39; : np.arange(0.0001, 0.0003, 0.0001) } gs = GridSearchCV(DecisionTreeRegressor(), params, n_jobs = -1) gs.fit(x_train, y_train) dt = gs.best_estimator_ . x_test . sky_condition precipitation_form wind_speed humidity low_temp high_temp Precipitation_Probability year month day . 0 3.000 | 0.000 | 2.900 | 28.333 | 11.800 | 20.667 | 18.333 | 2021 | 4 | 1 | . 1 3.850 | 0.000 | 2.662 | 46.417 | 12.000 | 19.000 | 28.500 | 2021 | 4 | 2 | . 2 4.000 | 0.565 | 2.165 | 77.258 | 8.875 | 16.368 | 52.847 | 2021 | 4 | 3 | . 3 3.466 | 0.466 | 3.747 | 63.288 | 6.250 | 17.368 | 37.671 | 2021 | 4 | 4 | . 4 1.500 | 0.000 | 1.560 | 48.176 | 7.188 | 18.684 | 4.459 | 2021 | 4 | 5 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 86 3.980 | 0.223 | 1.066 | 74.628 | 20.312 | 28.579 | 36.486 | 2021 | 6 | 26 | . 87 2.777 | 0.135 | 1.290 | 70.236 | 20.812 | 29.000 | 18.378 | 2021 | 6 | 27 | . 88 3.338 | 1.270 | 1.692 | 70.338 | 21.000 | 28.789 | 35.946 | 2021 | 6 | 28 | . 89 3.270 | 0.595 | 1.470 | 70.473 | 21.000 | 29.421 | 27.770 | 2021 | 6 | 29 | . 90 3.270 | 0.703 | 1.180 | 75.203 | 21.500 | 30.211 | 29.054 | 2021 | 6 | 30 | . 91 rows × 10 columns . print(dt.feature_importances_) print(gs.best_params_) . [0.00134991 0.01028958 0. 0. 0.01253956 0.12896212 0.18233246 0.66277645 0.00174992 0. ] {&#39;max_depth&#39;: 12, &#39;min_impurity_decrease&#39;: 0.0002, &#39;min_samples_split&#39;: 25} . import matplotlib.pyplot as plt plt.style.use(&#39;ggplot&#39;) plt.figure(figsize = (20, 10)) plt.plot(dt.predict(x_train), label = &#39;prediction&#39;) plt.plot(y_train, label = &#39;real&#39;) plt.legend(fontsize = 20) plt.show() . RandomForest // 0.64282 . from sklearn.ensemble import RandomForestRegressor rf = RandomForestRegressor(random_state = 42, n_jobs = -1) rf.fit(x_train, y_train) print(rf.feature_importances_) . [0.02715464 0.0636719 0.02191772 0.01226784 0.04654751 0.12911862 0.10436298 0.56736246 0.01310199 0.01449433] . import matplotlib.pyplot as plt plt.style.use(&#39;ggplot&#39;) plt.figure(figsize = (20, 10)) plt.plot(rf.predict(x_train), label = &#39;prediction&#39;) plt.plot(y_train, label = &#39;real&#39;) plt.legend(fontsize = 20) plt.show() . Ridge (feat. PolynomialFeatures) // 0.57752 . from sklearn.linear_model import Ridge from sklearn.preprocessing import PolynomialFeatures from sklearn.preprocessing import StandardScaler poly = PolynomialFeatures() poly.fit(x_train) x_train_poly = poly.transform(x_train) x_test_poly = poly.transform(x_test) ss = StandardScaler() ss.fit(x_train_poly) x_train_poly_scaled = ss.transform(x_train_poly) x_test_poly_scaled = ss.transform(x_test_poly) ridge = Ridge(alpha = 0.0000001) ridge.fit(x_train_poly_scaled, y_train) . Ridge(alpha=1e-07, copy_X=True, fit_intercept=True, max_iter=None, normalize=False, random_state=None, solver=&#39;auto&#39;, tol=0.001) . import matplotlib.pyplot as plt # 리스트 만들기 train_score = [] test_score = [] alpha_list = [0.0000000000000001, 0.0000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100] # 반복문으로 alpha 값 돌리기 for alpha in alpha_list: ridge = Ridge(alpha = alpha) ridge.fit(x_train_poly_scaled, y_train) train_score.append(ridge.score(x_train_poly_scaled, y_train)) test_score.append(ridge.score(x_test_poly_scaled, ridge.predict(x_test_poly_scaled))) # 그래프 그리기 plt.plot(np.log10(alpha_list), train_score, ) plt.plot(np.log10(alpha_list), test_score) plt.xlabel(&#39;alpha&#39;) plt.ylabel(&#39;R^2&#39;) plt.show() # train 스코어가 가장 높고 test 세트의 점수가 가장 높은 -1 = 10^-1 = 0.1 --&gt; alpha = 0.1로 하여 최종 모델을 훈련해야 가장 좋은 모델 . /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_ridge.py:148: LinAlgWarning: Ill-conditioned matrix (rcond=1.01053e-20): result may not be accurate. overwrite_a=True).T . import matplotlib.pyplot as plt plt.style.use(&#39;ggplot&#39;) plt.figure(figsize = (20, 10)) plt.plot(ridge.predict(x_train_poly_scaled), label = &#39;prediction&#39;) plt.plot(y_train, label = &#39;real&#39;) plt.legend(fontsize = 20) plt.show() . Lasso (feat.PolynomialFeatures) // 1.14842 . from sklearn.preprocessing import PolynomialFeatures from sklearn.preprocessing import StandardScaler from sklearn.linear_model import Lasso poly = PolynomialFeatures() poly.fit(x_train) x_train_poly = poly.transform(x_train) x_test_poly = poly.transform(x_test) ss = StandardScaler() ss.fit(x_train_poly) x_train_poly_scaled = ss.transform(x_train_poly) x_test_poly_scaled = ss.transform(x_test_poly) lasso = Lasso(alpha = 0.1) lasso.fit(x_train_poly_scaled, y_train) . /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 11316965287.630852, tolerance: 20809417.990786813 positive) . Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000, normalize=False, positive=False, precompute=False, random_state=None, selection=&#39;cyclic&#39;, tol=0.0001, warm_start=False) . from sklearn.preprocessing import StandardScaler from sklearn.linear_model import Lasso ss = StandardScaler() ss.fit(x_train, y_train) x_train_scaled = ss.transform(x_train) x_test_scaled = ss.transform(x_test) lasso = Lasso(alpha = 0.0001) lasso.fit(x_train_scaled, y_train) . /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 15036150890.694096, tolerance: 20809417.990786813 positive) . Lasso(alpha=0.0001, copy_X=True, fit_intercept=True, max_iter=1000, normalize=False, positive=False, precompute=False, random_state=None, selection=&#39;cyclic&#39;, tol=0.0001, warm_start=False) . import matplotlib.pyplot as plt from sklearn.linear_model import Lasso # 리스트 만들기 train_score = [] test_score = [] alpha_list = [0.0000000000000001, 0.0000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100] # 반복문으로 alpha 값 돌리기 for alpha in alpha_list: lasso = Lasso(alpha = alpha) lasso.fit(x_train_poly_scaled, y_train) train_score.append(lasso.score(x_train_poly_scaled, y_train)) test_score.append(lasso.score(x_test_poly_scaled, lasso.predict(x_test_poly_scaled))) # 그래프 그리기 plt.plot(np.log10(alpha_list), train_score, ) plt.plot(np.log10(alpha_list), test_score) plt.xlabel(&#39;alpha&#39;) plt.ylabel(&#39;R^2&#39;) plt.show() # train 스코어가 가장 높고 test 세트의 점수가 가장 높은 -1 = 10^-1 = 0.1 --&gt; alpha = 0.1로 하여 최종 모델을 훈련해야 가장 좋은 모델 . /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 11460544544.452034, tolerance: 20809417.990786813 positive) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 11460544403.169, tolerance: 20809417.990786813 positive) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 11460530416.141394, tolerance: 20809417.990786813 positive) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 11460403259.290476, tolerance: 20809417.990786813 positive) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 11459131489.683096, tolerance: 20809417.990786813 positive) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 11446393544.570705, tolerance: 20809417.990786813 positive) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 11316965287.630852, tolerance: 20809417.990786813 positive) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 9861513758.318222, tolerance: 20809417.990786813 positive) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4289118912.9155006, tolerance: 20809417.990786813 positive) . import matplotlib.pyplot as plt plt.style.use(&#39;ggplot&#39;) plt.figure(figsize = (20, 10)) plt.plot(lasso.predict(x_train), label = &#39;prediction&#39;) plt.plot(y_train, label = &#39;real&#39;) plt.legend(fontsize = 20) plt.show() . &#51228;&#52636; . test_predict = lr.predict(x_test) submission[&#39;number_of_rentals&#39;] = test_predict submission.to_csv(&#39;sample_submission.csv&#39;, index = False) . test_predict = kn.predict(x_test_scaled) submission[&#39;number_of_rentals&#39;] = test_predict submission.to_csv(&#39;sample_submission.csv&#39;, index = False) . test_predict = dt.predict(x_test) submission[&#39;number_of_rentals&#39;] = test_predict submission.to_csv(&#39;sample_submission.csv&#39;, index = False) . test_predict = rf.predict(x_test) submission[&#39;number_of_rentals&#39;] = test_predict submission.to_csv(&#39;sample_submission.csv&#39;, index = False) . test_predict = ridge.predict(x_test_poly_scaled) submission[&#39;number_of_rentals&#39;] = test_predict submission.to_csv(&#39;sample_submission.csv&#39;, index = False) . test_predict = lasso.predict(x_test_scaled) submission[&#39;number_of_rentals&#39;] = test_predict submission.to_csv(&#39;sample_submission.csv&#39;, index = False) . &#45936;&#51060;&#53552; &#48520;&#47084;&#50724;&#44592; . import pandas as pd import numpy as np train = pd.read_csv(&#39;/content/drive/MyDrive/mydata/train.csv&#39;) test = pd.read_csv(&#39;/content/drive/MyDrive/mydata/test.csv&#39;) train.head().T . 0 1 2 3 4 . index 0 | 1 | 2 | 3 | 4 | . gender F | F | M | F | F | . car N | N | Y | N | Y | . reality N | Y | Y | Y | Y | . child_num 0 | 1 | 0 | 0 | 0 | . income_total 202500 | 247500 | 450000 | 202500 | 157500 | . income_type Commercial associate | Commercial associate | Working | Commercial associate | State servant | . edu_type Higher education | Secondary / secondary special | Higher education | Secondary / secondary special | Higher education | . family_type Married | Civil marriage | Married | Married | Married | . house_type Municipal apartment | House / apartment | House / apartment | House / apartment | House / apartment | . DAYS_BIRTH -13899 | -11380 | -19087 | -15088 | -15037 | . DAYS_EMPLOYED -4709 | -1540 | -4434 | -2092 | -2105 | . FLAG_MOBIL 1 | 1 | 1 | 1 | 1 | . work_phone 0 | 0 | 0 | 0 | 0 | . phone 0 | 0 | 1 | 1 | 0 | . email 0 | 1 | 0 | 0 | 0 | . occyp_type NaN | Laborers | Managers | Sales staff | Managers | . family_size 2 | 3 | 2 | 2 | 2 | . begin_month -6 | -5 | -22 | -37 | -26 | . credit 1 | 1 | 2 | 0 | 2 | . # 카드 등록 개월 보기편하게 절대값 train[&#39;DAYS_BIRTH&#39;] = (abs(train[&#39;DAYS_BIRTH&#39;]) / 365) train[&#39;DAYS_EMPLOYED&#39;] = (abs(train[&#39;DAYS_EMPLOYED&#39;]) / 365) train[&#39;begin_month&#39;] = abs(train[&#39;begin_month&#39;]) train.head().T . 0 1 2 3 4 . index 0 | 1 | 2 | 3 | 4 | . gender F | F | M | F | F | . car N | N | Y | N | Y | . reality N | Y | Y | Y | Y | . child_num 0 | 1 | 0 | 0 | 0 | . income_total 202500 | 247500 | 450000 | 202500 | 157500 | . income_type Commercial associate | Commercial associate | Working | Commercial associate | State servant | . edu_type Higher education | Secondary / secondary special | Higher education | Secondary / secondary special | Higher education | . family_type Married | Civil marriage | Married | Married | Married | . house_type Municipal apartment | House / apartment | House / apartment | House / apartment | House / apartment | . DAYS_BIRTH 38.0795 | 31.1781 | 52.2932 | 41.337 | 41.1973 | . DAYS_EMPLOYED 12.9014 | 4.21918 | 12.1479 | 5.73151 | 5.76712 | . FLAG_MOBIL 1 | 1 | 1 | 1 | 1 | . work_phone 0 | 0 | 0 | 0 | 0 | . phone 0 | 0 | 1 | 1 | 0 | . email 0 | 1 | 0 | 0 | 0 | . occyp_type NaN | Laborers | Managers | Sales staff | Managers | . family_size 2 | 3 | 2 | 2 | 2 | . begin_month 6 | 5 | 22 | 37 | 26 | . credit 1 | 1 | 2 | 0 | 2 | .",
            "url": "https://parkjeongung.github.io/Ung.github.io/2022/01/12/%EB%94%B0%EB%A6%89%EC%9D%B4_%EB%8C%80%EC%97%AC%EB%9F%89_%EC%98%88%EC%B8%A1_%EB%AA%A8%EB%8D%B8%EB%A7%81_%EC%97%B0%EC%8A%B5.html",
            "relUrl": "/2022/01/12/%EB%94%B0%EB%A6%89%EC%9D%B4_%EB%8C%80%EC%97%AC%EB%9F%89_%EC%98%88%EC%B8%A1_%EB%AA%A8%EB%8D%B8%EB%A7%81_%EC%97%B0%EC%8A%B5.html",
            "date": " • Jan 12, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "PUBG // 전처리 연습 // query",
            "content": ". &#45936;&#51060;&#53552; &#48520;&#47084;&#50724;&#44592; . pip install kaggle --upgrade . Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12) Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.10.8) Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.3) Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0) Requirement already satisfied: six&gt;=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0) Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2) Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2) Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3) Requirement already satisfied: text-unidecode&gt;=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify-&gt;kaggle) (1.3) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (2.10) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (3.0.4) . !pip install kaggle from google.colab import files files.upload() . Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12) Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2) Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0) Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.3) Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2) Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.10.8) Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3) Requirement already satisfied: six&gt;=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0) Requirement already satisfied: text-unidecode&gt;=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify-&gt;kaggle) (1.3) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (2.10) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (3.0.4) . Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable. Saving kaggle.json to kaggle (2).json . {&#39;kaggle.json&#39;: b&#39;{&#34;username&#34;:&#34;chobocoder&#34;,&#34;key&#34;:&#34;e79bebe8d4ae3d272cdb108fc8f25832&#34;}&#39;} . !mkdir -p ~/.kaggle !cp kaggle.json ~/.kaggle/ !chmod 600 ~/.kaggle/kaggle.json . !kaggle competitions download -c pubg-finish-placement-prediction . Warning: Looks like you&#39;re using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4) train_V2.csv.zip: Skipping, found more recently modified local copy (use --force to force download) test_V2.csv.zip: Skipping, found more recently modified local copy (use --force to force download) sample_submission_V2.csv.zip: Skipping, found more recently modified local copy (use --force to force download) . import pandas as pd train = pd.read_csv(&#39;train_V2.csv.zip&#39;) test = pd.read_csv(&#39;test_V2.csv.zip&#39;) . &#45936;&#51060;&#53552; &#54869;&#51064; . DBNOs - 적을 기절시킨 횟수 | assists - 적을 죽이는데 도움을 준 횟수 | boosts - 드링크, 진통제 사용 횟수 | damageDealt - 딜량(적에게 준 총 피해량)(자해 데미지는 감산됨) | headshotKills - 헤드샷으로 적을 죽인 횟수 | heals - 회복 아이템 사용 횟수 | Id - 플레이어 ID | killPlace - Ranking in match of number of enemy players killed. | killPoints - 킬을 기반으로 한 레이팅(점수) (랭크포인트에 -1 이외의 값이 있는 경우 killPoints의 0은 &quot;없음&quot;으로 간주해야 합니다.) | killStreaks - 단 시간내에 사살한 적플레이어의 수 | kills - 사살한 적 플레이어의 수 | longestKill - 적을 사살했을 때 적 플레이어와 자신 사이의 거리(적을 사살하고 차를 몰고 떠나면 거리가 늘어나기 때문에 오차가 생길 수 있음) | matchDuration - 진행된 게임의 시간 (단위 - 초) | matchId - 매치를 구별하기 위한 매치ID | matchType - 게임의 유형(모드)“solo(1명)”, “duo(2명)”, “squad(4명)”, “solo-fpp(1명 1인칭)”, “duo-fpp(2명 2인칭)”, and “squad-fpp(4명 1인칭)”; 나머지는 이벤트 모드 | rankPoints - 레이팅(점수) 랭킹을 나타내는 지표 -1값은 &quot;없음&quot;을 나타내는 값 | revives - 이 플레이어가 팀을 부활시킨 횟수 | rideDistance - 차량으로 이동한 총 거리 (단위 - 미터) | roadKills - 차량에 탑승한 동안 적을 죽인 횟수 | swimDistance - 수영으로 이동한 총 거리 (단위 - 미터) | teamKills - 팀을 죽인 횟수 | vehicleDestroys - 차량을 파괴한 횟수 | walkDistance - 걸어서 다닌 거리 (단위 - 미터) | weaponsAcquired - 무기를 습득한 횟수 | winPoints - 우승 기반 레이팅(점수) 적 사살횟수와 상관없는 승리 중심의 레이팅(rankPoints에 -1 이외의 값이 있는 경우 winPoints의 0은 &quot;없음&quot;으로 간주해야 합니다.) | groupId - 그룹을 식별하는 ID. 같은 그룹 이라도 다른 게임이라면 서로 다른 그룹아이디를 가짐. ex) 같은 듀오라도 첫판은 4d4b580de459be 두번째판은 684d5656442f9e 이런식으로 다른 그룹ID를 가짐 | numGroups - 한 매치에 속한 그룹의 수(그룹ID의 수) | maxPlace - 꼴찌 팀의 순위(중간에 나가는 그룹이 있어서 numGroups 와 다를 수 있음) | winPlacePerc - 예측 타겟 데이터 백분위수를 기준으로 우승 순위를 배치 1은 1위에 해당하고 0은 꼴찌에 해당 maxPlace 를 기준으로 계산하므로 누락된 chunks 가 있을수 있다. | . train.head().T . 0 1 2 3 4 . Id 7f96b2f878858a | eef90569b9d03c | 1eaf90ac73de72 | 4616d365dd2853 | 315c96c26c9aac | . groupId 4d4b580de459be | 684d5656442f9e | 6a4a42c3245a74 | a930a9c79cd721 | de04010b3458dd | . matchId a10357fd1a4a91 | aeb375fc57110c | 110163d8bb94ae | f1f1f4ef412d7e | 6dc8ff871e21e6 | . assists 0 | 0 | 1 | 0 | 0 | . boosts 0 | 0 | 0 | 0 | 0 | . damageDealt 0 | 91.47 | 68 | 32.9 | 100 | . DBNOs 0 | 0 | 0 | 0 | 0 | . headshotKills 0 | 0 | 0 | 0 | 0 | . heals 0 | 0 | 0 | 0 | 0 | . killPlace 60 | 57 | 47 | 75 | 45 | . killPoints 1241 | 0 | 0 | 0 | 0 | . kills 0 | 0 | 0 | 0 | 1 | . killStreaks 0 | 0 | 0 | 0 | 1 | . longestKill 0 | 0 | 0 | 0 | 58.53 | . matchDuration 1306 | 1777 | 1318 | 1436 | 1424 | . matchType squad-fpp | squad-fpp | duo | squad-fpp | solo-fpp | . maxPlace 28 | 26 | 50 | 31 | 97 | . numGroups 26 | 25 | 47 | 30 | 95 | . rankPoints -1 | 1484 | 1491 | 1408 | 1560 | . revives 0 | 0 | 0 | 0 | 0 | . rideDistance 0 | 0.0045 | 0 | 0 | 0 | . roadKills 0 | 0 | 0 | 0 | 0 | . swimDistance 0 | 11.04 | 0 | 0 | 0 | . teamKills 0 | 0 | 0 | 0 | 0 | . vehicleDestroys 0 | 0 | 0 | 0 | 0 | . walkDistance 244.8 | 1434 | 161.8 | 202.7 | 49.75 | . weaponsAcquired 1 | 5 | 2 | 3 | 2 | . winPoints 1466 | 0 | 0 | 0 | 0 | . winPlacePerc 0.4444 | 0.64 | 0.7755 | 0.1667 | 0.1875 | . &#44208;&#52769;&#52824; &#51228;&#44144; . train.isnull().sum() . Id 0 groupId 0 matchId 0 assists 0 boosts 0 damageDealt 0 DBNOs 0 headshotKills 0 heals 0 killPlace 0 killPoints 0 kills 0 killStreaks 0 longestKill 0 matchDuration 0 matchType 0 maxPlace 0 numGroups 0 rankPoints 0 revives 0 rideDistance 0 roadKills 0 swimDistance 0 teamKills 0 vehicleDestroys 0 walkDistance 0 weaponsAcquired 0 winPoints 0 winPlacePerc 1 dtype: int64 . train = train.dropna(how = &#39;any&#39;) . train.isnull().sum() . Id 0 groupId 0 matchId 0 assists 0 boosts 0 damageDealt 0 DBNOs 0 headshotKills 0 heals 0 killPlace 0 killPoints 0 kills 0 killStreaks 0 longestKill 0 matchDuration 0 matchType 0 maxPlace 0 numGroups 0 rankPoints 0 revives 0 rideDistance 0 roadKills 0 swimDistance 0 teamKills 0 vehicleDestroys 0 walkDistance 0 weaponsAcquired 0 winPoints 0 winPlacePerc 0 dtype: int64 . test.isnull().sum() # 테스트 세트는 결측치 없음 . Id 0 groupId 0 matchId 0 assists 0 boosts 0 damageDealt 0 DBNOs 0 headshotKills 0 heals 0 killPlace 0 killPoints 0 kills 0 killStreaks 0 longestKill 0 matchDuration 0 matchType 0 maxPlace 0 numGroups 0 rankPoints 0 revives 0 rideDistance 0 roadKills 0 swimDistance 0 teamKills 0 vehicleDestroys 0 walkDistance 0 weaponsAcquired 0 winPoints 0 dtype: int64 . &#54616;&#44256; &#49910;&#51008;&#45824;&#47196; &#45796; &#54644;&#48372;&#44592; . 1. &#47924;&#44592;&#46020; &#47803;&#51469;&#44256; &#51060;&#46041;&#44144;&#47532; &#50630;&#45716; &#51104;&#49688;&#52649; &#48320;&#49688; &#49373;&#49457; . train[&#39;total_distance&#39;] = train[&#39;swimDistance&#39;] + train[&#39;rideDistance&#39;] + train[&#39;walkDistance&#39;] test[&#39;total_distance&#39;] = test[&#39;swimDistance&#39;] + test[&#39;rideDistance&#39;] + test[&#39;walkDistance&#39;] . sleep_user_index = train.query(&#39;total_distance == 0 and weaponsAcquired == 0 and damageDealt == 0&#39;).index sleep_user = train.index.isin(sleep_user_index) train[&#39;sleep_user&#39;] = sleep_user train.head().T sleep_user_index = test.query(&#39;total_distance == 0 and weaponsAcquired == 0 and damageDealt == 0&#39;).index sleep_user = test.index.isin(sleep_user_index) test[&#39;sleep_user&#39;] = sleep_user . 2. &#49556;&#47196;&#50640;&#45716; &#44592;&#51208;&#51060; &#50630;&#45796; (3&#45380; &#51204; &#44592;&#51456;) . print(train.matchType.unique()) # 이벤트모드 많이 없을줄 알았지만 많다 # 결론적으로 솔로는 기절 없으니까 다 처리해주자 . [&#39;squad-fpp&#39; &#39;duo&#39; &#39;solo-fpp&#39; &#39;squad&#39; &#39;duo-fpp&#39; &#39;solo&#39; &#39;normal-squad-fpp&#39; &#39;crashfpp&#39; &#39;flaretpp&#39; &#39;normal-solo-fpp&#39; &#39;flarefpp&#39; &#39;normal-duo-fpp&#39; &#39;normal-duo&#39; &#39;normal-squad&#39; &#39;crashtpp&#39; &#39;normal-solo&#39;] . Int64Index([ 29, 116, 151, 237, 283, 404, 520, 606, 619, 638, ... 4446585, 4446592, 4446618, 4446650, 4446743, 4446779, 4446843, 4446849, 4446926, 4446958], dtype=&#39;int64&#39;, length=76645) . solo_index = train.query(&#39;matchType.str.contains(&quot;solo&quot;) and DBNOs == 0&#39;, engine = &#39;python&#39;).index train.at[solo_index, &#39;DBNOs&#39;] = -1 solo_index = test.query(&#39;matchType.str.contains(&quot;solo&quot;) and DBNOs == 0&#39;, engine = &#39;python&#39;).index test.at[solo_index, &#39;DBNOs&#39;] = -1 . train.head().T . 0 1 2 3 4 . Id 7f96b2f878858a | eef90569b9d03c | 1eaf90ac73de72 | 4616d365dd2853 | 315c96c26c9aac | . groupId 4d4b580de459be | 684d5656442f9e | 6a4a42c3245a74 | a930a9c79cd721 | de04010b3458dd | . matchId a10357fd1a4a91 | aeb375fc57110c | 110163d8bb94ae | f1f1f4ef412d7e | 6dc8ff871e21e6 | . assists 0 | 0 | 1 | 0 | 0 | . boosts 0 | 0 | 0 | 0 | 0 | . damageDealt 0 | 91.47 | 68 | 32.9 | 100 | . DBNOs 0 | 0 | 0 | 0 | 0 | . headshotKills 0 | 0 | 0 | 0 | 0 | . heals 0 | 0 | 0 | 0 | 0 | . killPlace 60 | 57 | 47 | 75 | 45 | . killPoints 1241 | 0 | 0 | 0 | 0 | . kills 0 | 0 | 0 | 0 | 1 | . killStreaks 0 | 0 | 0 | 0 | 1 | . longestKill 0 | 0 | 0 | 0 | 58.53 | . matchDuration 1306 | 1777 | 1318 | 1436 | 1424 | . matchType squad-fpp | squad-fpp | duo | squad-fpp | solo-fpp | . maxPlace 28 | 26 | 50 | 31 | 97 | . numGroups 26 | 25 | 47 | 30 | 95 | . rankPoints -1 | 1484 | 1491 | 1408 | 1560 | . revives 0 | 0 | 0 | 0 | 0 | . rideDistance 0 | 0.0045 | 0 | 0 | 0 | . roadKills 0 | 0 | 0 | 0 | 0 | . swimDistance 0 | 11.04 | 0 | 0 | 0 | . teamKills 0 | 0 | 0 | 0 | 0 | . vehicleDestroys 0 | 0 | 0 | 0 | 0 | . walkDistance 244.8 | 1434 | 161.8 | 202.7 | 49.75 | . weaponsAcquired 1 | 5 | 2 | 3 | 2 | . winPoints 1466 | 0 | 0 | 0 | 0 | . winPlacePerc 0.4444 | 0.64 | 0.7755 | 0.1667 | 0.1875 | . headshot_rate 0 | 0 | 0 | 0 | 0 | . &#54645;&#51137;&#51060;&#47484; &#51105;&#44592; &#50948;&#54620; &#45432;&#47141; . 에임핵 --&gt; killPoints, headshotKills, Kills, damageDealt | 스피드핵 --&gt; ridedistance, swimdistance, walkdistance | 자동회복 무적핵 --&gt; 무기를 안먹었는데 부스트랑 힐이 없다 --&gt; | 소환술 --&gt; 이동거리가 적은데 킬수가 많다 --&gt; distance, kill | 에임봇 --&gt; 자동으로 적에게 조준 거리 상관 없음 --&gt; longestkill | 자기장 조종 --&gt; 바다쪽으로 잡아서 혼자 가만히 수영하고 있는 경우가 있다 --&gt; 수영 거리랑 딜량을 보자 --&gt; swimdistance, damageDealt | 아이템esp --&gt; 아이템 위치를 나타내줌 --&gt; weaponsAcquired | import seaborn as sns import matplotlib.pyplot as plt def show_countplot(column): plt.figure(figsize=(12,4)) sns.countplot(data=train, x=column).set_title(column) plt.show() def show_distplot(column): plt.figure(figsize=(12, 4)) sns.distplot(train[column], bins=50) plt.show() . &#49548;&#54872;&#49696; --&gt; &#51060;&#46041;&#44144;&#47532;&#44032; &#51201;&#51008;&#45936; &#53420;&#49688;&#44032; &#47566;&#45796; --&gt; distance, kill . show_countplot(&#39;kills&#39;) . show_distplot(&#39;total_distance&#39;) # 이동거리가 400 km 가 넘어가는 데이터가 존재한다 . /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . ex = train.query(&#39;kills &gt; 30 and total_distance &lt; 5000&#39;) ex . Id groupId matchId assists boosts damageDealt DBNOs headshotKills heals killPlace killPoints kills killStreaks longestKill matchDuration matchType maxPlace numGroups rankPoints revives rideDistance roadKills swimDistance teamKills vehicleDestroys walkDistance weaponsAcquired winPoints winPlacePerc total_distance . 57978 9d8253e21ccbbd | ef7135ed856cd8 | 37f05e2a01015f | 9 | 0 | 3725.0 | 0 | 7 | 0 | 2 | 1000 | 35 | 3 | 105.80 | 1798 | normal-duo-fpp | 8 | 7 | -1 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | 48.82 | 48 | 1500 | 0.8571 | 48.82 | . 87793 45f76442384931 | b3627758941d34 | 37f05e2a01015f | 8 | 0 | 3087.0 | 0 | 8 | 27 | 3 | 1000 | 31 | 3 | 95.34 | 1798 | normal-duo-fpp | 8 | 7 | -1 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | 780.70 | 45 | 1500 | 1.0000 | 780.70 | . 156599 746aa7eabf7c86 | 5723e7d8250da3 | f900de1ec39fa5 | 21 | 0 | 5479.0 | 0 | 12 | 7 | 4 | 0 | 48 | 6 | 81.95 | 1798 | normal-solo-fpp | 11 | 11 | 1500 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | 23.71 | 61 | 0 | 0.7000 | 23.71 | . 160254 15622257cb44e2 | 1a513eeecfe724 | db413c7c48292c | 1 | 0 | 4033.0 | 0 | 40 | 0 | 1 | 1000 | 42 | 5 | 266.20 | 844 | normal-squad-fpp | 8 | 8 | -1 | 0 | 0.0 | 0 | 0.0 | 1 | 0 | 718.30 | 16 | 1500 | 1.0000 | 718.30 | . 180189 1355613d43e2d0 | f863cd38c61dbf | 39c442628f5df5 | 5 | 0 | 3171.0 | 0 | 6 | 15 | 1 | 0 | 35 | 3 | 102.50 | 1796 | normal-solo-fpp | 9 | 6 | 1500 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | 71.51 | 41 | 0 | 1.0000 | 71.51 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 4021018 1f1c3dda0296df | 7c95f475fd2cdb | a9e84c456cc859 | 6 | 0 | 3406.0 | 0 | 8 | 10 | 2 | 0 | 31 | 3 | 103.50 | 1795 | normal-solo-fpp | 13 | 10 | 1500 | 0 | 1553.0 | 0 | 0.0 | 0 | 0 | 2264.00 | 40 | 0 | 0.9167 | 3817.00 | . 4127904 f699c842c5dfab | 9d69e4e697d296 | 1ac375e4121651 | 5 | 0 | 3420.0 | 39 | 20 | 1 | 1 | 1000 | 33 | 7 | 248.40 | 1264 | normal-squad-fpp | 5 | 5 | -1 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | 319.20 | 29 | 1500 | 0.7500 | 319.20 | . 4148675 5283367a7f8d06 | 35b9b765110fd2 | f900de1ec39fa5 | 12 | 0 | 3050.0 | 0 | 5 | 4 | 7 | 0 | 31 | 7 | 48.52 | 1798 | normal-solo-fpp | 11 | 11 | 1500 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | 123.80 | 89 | 0 | 0.4000 | 123.80 | . 4235682 6874be9215646b | af1d17223258d0 | 0f09bd72c4ba97 | 1 | 0 | 3006.0 | 0 | 9 | 18 | 2 | 1000 | 33 | 2 | 150.30 | 1794 | normal-squad-fpp | 8 | 8 | -1 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | 557.50 | 20 | 1500 | 0.4286 | 557.50 | . 4273754 2a4c8d7cdb0361 | 8ee08f3d84dc3a | 17dea22cefe62a | 6 | 0 | 5297.0 | 0 | 5 | 11 | 2 | 0 | 57 | 3 | 63.50 | 1798 | normal-duo-fpp | 15 | 12 | 1500 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | 242.90 | 54 | 0 | 0.8571 | 242.90 | . 101 rows × 30 columns . ex = train.query(&#39;kills &gt; 30 and total_distance &lt; 5000 and (matchType.str.startswith(&quot;solo&quot;) or matchType.str.startswith(&quot;duo&quot;) or matchType.str.startswith(&quot;squad&quot;))&#39;,engine = &#39;python&#39;) ex . Id groupId matchId assists boosts damageDealt DBNOs headshotKills heals killPlace killPoints kills killStreaks longestKill matchDuration matchType maxPlace numGroups rankPoints revives rideDistance roadKills swimDistance teamKills vehicleDestroys walkDistance weaponsAcquired winPoints winPlacePerc total_distance . exindex = train.query(&#39;kills &gt; 30 and (matchType.str.startswith(&quot;solo&quot;) or matchType.str.startswith(&quot;duo&quot;) or matchType.str.startswith(&quot;squad&quot;))&#39;,engine = &#39;python&#39;).index train = train.drop(exindex) exindex . Int64Index([3840888], dtype=&#39;int64&#39;) . &#50640;&#51076;&#54645; --&gt; &#53420;&#49688;&#44032; &#47566;&#51008;&#45936; &#54756;&#46300;&#49399; &#48708;&#50984;&#51060; &#48708;&#51221;&#49345;&#51201;&#51004;&#47196; &#45458;&#51020; . train[&#39;headshot_rate&#39;] = train.headshotKills / train.kills train[&#39;headshot_rate&#39;] = train.headshot_rate.fillna(0) test[&#39;headshot_rate&#39;] = test.headshotKills / test.kills test[&#39;headshot_rate&#39;] = test.headshot_rate.fillna(0) . show_countplot(&#39;headshotKills&#39;) . show_distplot(&#39;headshot_rate&#39;) . /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . ex = train.query(&#39;headshot_rate &gt; 0.90 and kills &gt; 15 and (matchType.str.startswith(&quot;solo&quot;) or matchType.str.startswith(&quot;duo&quot;) or matchType.str.startswith(&quot;squad&quot;))&#39;,engine = &#39;python&#39;) ex . Id groupId matchId assists boosts damageDealt DBNOs headshotKills heals killPlace killPoints kills killStreaks longestKill matchDuration matchType maxPlace numGroups rankPoints revives rideDistance roadKills swimDistance teamKills vehicleDestroys walkDistance weaponsAcquired winPoints winPlacePerc total_distance swimmer hiker heals_boosts headshot_rate . 539354 777a59f7dceb9c | 1be852f783b980 | d89c28b6a06ec5 | 0 | 0 | 1800.0 | 0 | 17 | 7 | 1 | 1023 | 18 | 11 | 36.76 | 1389 | solo | 85 | 66 | -1 | 0 | 0.0138 | 0 | 0.00 | 0 | 0 | 1117.0 | 3 | 1503 | 0.7024 | 1117.0138 | 4583.7 | 8334 | 7 | 0.944444 | . 593688 62aebfb56de63f | fffdc477adcfec | da061fe2055f02 | 0 | 2 | 1565.0 | 7 | 15 | 0 | 1 | 1000 | 16 | 3 | 606.80 | 1218 | duo | 48 | 48 | -1 | 0 | 0.0000 | 0 | 83.38 | 0 | 0 | 2532.0 | 6 | 1500 | 1.0000 | 2615.3800 | 4019.4 | 7308 | 2 | 0.937500 | . 669149 bac98bbc0532fa | 39dc8efdad2529 | 6f8a90ef233ba0 | 0 | 0 | 2270.0 | 19 | 15 | 0 | 1 | 1000 | 16 | 4 | 325.20 | 1294 | squad | 26 | 23 | -1 | 0 | 464.2000 | 0 | 0.00 | 0 | 0 | 2865.0 | 5 | 1500 | 1.0000 | 3329.2000 | 4270.2 | 7764 | 0 | 0.937500 | . 979479 8e9eb1ce0e0135 | d6c25ac6cbe0d4 | 8889fcc108e51e | 0 | 7 | 2067.0 | 0 | 19 | 2 | 1 | 0 | 21 | 2 | 178.30 | 1375 | solo | 97 | 94 | 1507 | 0 | 808.1000 | 0 | 0.00 | 0 | 0 | 3507.0 | 4 | 0 | 1.0000 | 4315.1000 | 4537.5 | 8250 | 9 | 0.904762 | . 1699036 25ec950ff8e179 | f6170ab22a7b34 | 95ebe0c3506179 | 2 | 7 | 2583.0 | 22 | 17 | 4 | 1 | 1255 | 18 | 4 | 316.10 | 1771 | squad | 26 | 24 | -1 | 2 | 2382.0000 | 0 | 0.00 | 0 | 1 | 3726.0 | 6 | 1508 | 1.0000 | 6108.0000 | 5844.3 | 10626 | 11 | 0.944444 | . 1744909 daaf14f76f0398 | 067027699c7d7d | aa4ee3afac1f23 | 0 | 3 | 1883.0 | 10 | 17 | 4 | 1 | 1157 | 18 | 3 | 438.50 | 1264 | duo | 48 | 47 | -1 | 0 | 0.0000 | 0 | 0.00 | 0 | 0 | 4298.0 | 9 | 1512 | 1.0000 | 4298.0000 | 4171.2 | 7584 | 7 | 0.944444 | . 2887119 f80a1249536f1c | 9552f589e4c7e1 | 0dbbcc40eb9f54 | 1 | 11 | 1553.0 | 12 | 15 | 7 | 1 | 1689 | 16 | 3 | 233.30 | 1507 | squad | 27 | 25 | -1 | 2 | 0.0000 | 0 | 0.00 | 0 | 0 | 3299.0 | 3 | 1583 | 0.8846 | 3299.0000 | 4973.1 | 9042 | 18 | 0.937500 | . &#51088;&#46041;&#54924;&#48373; &#47924;&#51201;&#54645; --&gt; &#47924;&#44592;&#47484; &#50504;&#47673;&#50632;&#45716;&#45936; &#48512;&#49828;&#53944;&#46993; &#55184;&#51060; &#50630;&#45796; , &#51104;&#49688;&#52649;&#51064;&#45936; 1&#46321;&#51060;&#45796; . exindex = train.query(&#39;sleep_user == True and winPlacePerc == 1&#39;).index train = train.drop(exindex) exindex . Int64Index([ 88035, 461500, 494808, 519969, 909861, 1169042, 1190582, 1282483, 1570573, 1670584, 1865098, 2327852, 2963215, 3312750, 3531233, 3621244, 3724540, 4252774, 4384705, 4430451], dtype=&#39;int64&#39;) . ex = train.query(&#39;winPlacePerc == 1 and heals == 0&#39;, engine = &#39;python&#39;) ex . Id groupId matchId assists boosts damageDealt DBNOs headshotKills heals killPlace killPoints kills killStreaks longestKill matchDuration matchType maxPlace numGroups rankPoints revives rideDistance roadKills swimDistance teamKills vehicleDestroys walkDistance weaponsAcquired winPoints winPlacePerc total_distance swimmer hiker . 341 ab4532e6427a7e | 91b422b51e687f | 12f3738af22f97 | 0 | 6 | 91.0 | 0 | 1 | 0 | 29 | 0 | 1 | 1 | 20.11 | 1801 | duo-fpp | 46 | 45 | 1552 | 0 | 4427.0 | 0 | 0.0 | 0 | 0 | 2348.0 | 4 | 0 | 1.0 | 6775.0 | 5943.3 | 10806 | . 598 ce39351ac31340 | 0edc46edf214ea | f001fa89bd6db3 | 1 | 0 | 447.3 | 3 | 1 | 0 | 3 | 0 | 4 | 2 | 13.15 | 1327 | squad-fpp | 25 | 23 | 1502 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | 252.8 | 3 | 0 | 1.0 | 252.8 | 4379.1 | 7962 | . 891 f64a4d0045a38a | e3f19fa12f167b | ad4bfbf3044575 | 0 | 0 | 128.3 | 1 | 0 | 0 | 22 | 0 | 1 | 1 | 88.33 | 1269 | squad-fpp | 29 | 28 | 1500 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | 1312.0 | 4 | 0 | 1.0 | 1312.0 | 4187.7 | 7614 | . 1223 fc4cf725214bfc | 2c506858d8dcae | 3b3b448c32bf48 | 2 | 3 | 122.0 | 1 | 1 | 0 | 22 | 0 | 1 | 1 | 86.42 | 1350 | duo-fpp | 50 | 49 | 1448 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | 2540.0 | 5 | 0 | 1.0 | 2540.0 | 4455.0 | 8100 | . 1682 9323384d7c227b | 8f0d0d03501dfb | 3efe5f39b099e8 | 1 | 1 | 509.5 | 3 | 1 | 0 | 1 | 0 | 5 | 2 | 186.90 | 1685 | squad-fpp | 28 | 24 | 2113 | 1 | 1810.0 | 0 | 0.0 | 0 | 0 | 2801.0 | 6 | 0 | 1.0 | 4611.0 | 5560.5 | 10110 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 4445751 32d65105ac0585 | 15ce3114dbd9a3 | bf63c9506132d0 | 1 | 3 | 124.7 | 0 | 0 | 0 | 40 | 0 | 0 | 0 | 0.00 | 1464 | duo | 46 | 46 | 1433 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | 2253.0 | 6 | 0 | 1.0 | 2253.0 | 4831.2 | 8784 | . 4446093 2f57d1e882cc06 | 1c59e2133a2fbd | 0027b1ffb2e346 | 0 | 0 | 116.8 | 0 | 0 | 0 | 3 | 0 | 0 | 0 | 0.00 | 1808 | solo | 17 | 2 | 1500 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | 0.0 | 11 | 0 | 1.0 | 0.0 | 5966.4 | 10848 | . 4446364 18771e24f08f6d | c4c77cd55835dc | a04d6089f9ef99 | 2 | 2 | 723.4 | 5 | 0 | 0 | 1 | 0 | 5 | 3 | 60.98 | 1320 | squad-fpp | 25 | 25 | 1561 | 1 | 0.0 | 0 | 0.0 | 0 | 0 | 3186.0 | 6 | 0 | 1.0 | 3186.0 | 4356.0 | 7920 | . 4446823 714a054acf8227 | 5c2f2875e3f74b | b68a152e9ed994 | 0 | 3 | 49.5 | 0 | 0 | 0 | 45 | 1639 | 0 | 0 | 0.00 | 1352 | squad-fpp | 27 | 26 | -1 | 0 | 0.0 | 0 | 0.0 | 0 | 0 | 4307.0 | 5 | 1623 | 1.0 | 4307.0 | 4461.6 | 8112 | . 4446896 150f2e129ede6b | 4417126d5b0d71 | 8dddf0ace87e54 | 0 | 5 | 129.4 | 1 | 0 | 0 | 13 | 0 | 2 | 1 | 270.20 | 1351 | squad-fpp | 30 | 29 | 1538 | 1 | 0.0 | 0 | 0.0 | 0 | 0 | 3320.0 | 3 | 0 | 1.0 | 3320.0 | 4458.3 | 8106 | . 20889 rows × 32 columns . train[&#39;heals_boosts&#39;] = train.heals + train.boosts exindex = train.query(&#39;winPlacePerc == 1 and heals_boosts == 0&#39;, engine = &#39;python&#39;).index train = train.drop(exindex) exindex . Int64Index([ 598, 891, 3114, 3702, 3911, 3948, 4406, 4665, 6281, 8130, ... 4438851, 4439832, 4440136, 4440338, 4440540, 4442804, 4442954, 4443248, 4445056, 4446093], dtype=&#39;int64&#39;, length=7056) . &#50500;&#51060;&#53596; ESP --&gt; &#47924;&#44592;&#49845;&#46301;&#47049;&#51060; &#44537;&#45800;&#51201;&#51004;&#47196; &#47566;&#45796; . show_countplot(&#39;weaponsAcquired&#39;) . ex = train.query(&#39;weaponsAcquired &gt; 25&#39;) ex . Id groupId matchId assists boosts damageDealt DBNOs headshotKills heals killPlace killPoints kills killStreaks longestKill matchDuration matchType maxPlace numGroups rankPoints revives rideDistance roadKills swimDistance teamKills vehicleDestroys walkDistance weaponsAcquired winPoints winPlacePerc total_distance . 1292 919f57a28a5e02 | 79a34f910375bc | b181e5bc4f0c1d | 6 | 0 | 1750.0 | 0 | 6 | 2 | 7 | 0 | 15 | 3 | 69.88 | 1034 | normal-squad-fpp | 8 | 8 | 1500 | 0 | 1657.00 | 0 | 0.00 | 0 | 0 | 1137.00 | 43 | 0 | 0.7143 | 2794.00 | . 2769 89120d2f4e44e0 | fa54efeb11b8a6 | 662c9c7f9e87d0 | 4 | 0 | 970.5 | 6 | 4 | 17 | 4 | 1000 | 11 | 2 | 21.61 | 1263 | normal-duo | 7 | 7 | -1 | 2 | 0.00 | 0 | 0.00 | 0 | 0 | 90.79 | 28 | 1500 | 1.0000 | 90.79 | . 13418 e17aabeb0af04d | 5a8a87a3c05a02 | 6bc71df7f5fcba | 3 | 0 | 1169.0 | 0 | 4 | 0 | 8 | 1000 | 14 | 2 | 121.80 | 769 | normal-squad-fpp | 8 | 8 | -1 | 0 | 0.00 | 0 | 0.00 | 0 | 0 | 122.90 | 26 | 1500 | 0.8571 | 122.90 | . 14404 504befe1cc25e2 | 6dcd8c5ad01853 | 313c9787226bbe | 1 | 0 | 829.1 | 9 | 4 | 5 | 35 | 0 | 7 | 1 | 21.77 | 1198 | normal-squad-fpp | 7 | 7 | 1500 | 0 | 0.00 | 0 | 0.00 | 0 | 0 | 1742.00 | 28 | 0 | 0.6667 | 1742.00 | . 18041 5eee942f2b6d79 | 325464bcf4c846 | f3a64f99badeca | 0 | 0 | 198.0 | 0 | 0 | 0 | 4 | 0 | 0 | 0 | 0.00 | 1659 | solo | 19 | 1 | 1500 | 0 | 81.13 | 0 | 0.00 | 0 | 0 | 776.50 | 28 | 0 | 0.0000 | 857.63 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 4426908 b44116139a0746 | eddfc937ecaeb1 | 7d2fd7f5d4f8b2 | 3 | 1 | 1000.0 | 10 | 1 | 0 | 13 | 0 | 12 | 2 | 39.00 | 1195 | normal-squad-fpp | 6 | 6 | 1500 | 4 | 0.00 | 0 | 0.00 | 0 | 0 | 191.10 | 28 | 0 | 1.0000 | 191.10 | . 4427592 25cfb10fe2ab37 | 9c82bece9f9188 | f0d57afd891ebf | 1 | 2 | 215.2 | 0 | 0 | 1 | 13 | 1165 | 2 | 1 | 187.40 | 1712 | duo-fpp | 47 | 44 | -1 | 0 | 3869.00 | 0 | 20.43 | 0 | 0 | 2952.00 | 26 | 1432 | 1.0000 | 6841.43 | . 4427700 0a30aa920382e7 | 6746d691ea319c | c4abb3540ed3c2 | 3 | 0 | 332.0 | 0 | 0 | 0 | 51 | 0 | 2 | 1 | 18.17 | 597 | normal-squad-fpp | 15 | 15 | 1500 | 0 | 1939.00 | 0 | 0.00 | 0 | 0 | 1206.00 | 26 | 0 | 0.5000 | 3145.00 | . 4429697 6cc9d7b1643cbd | 11fce60d5c8ea3 | 60c6e90b964266 | 3 | 0 | 645.6 | 6 | 1 | 0 | 28 | 0 | 5 | 2 | 57.58 | 1216 | normal-squad-fpp | 6 | 6 | 1500 | 2 | 0.00 | 0 | 0.00 | 0 | 0 | 0.00 | 31 | 0 | 1.0000 | 0.00 | . 4431589 2a63c809ee549e | b2e3e24c25c744 | bc10cc08f1f56a | 4 | 0 | 2639.0 | 0 | 9 | 11 | 2 | 1000 | 28 | 2 | 134.20 | 1792 | normal-duo-fpp | 7 | 6 | -1 | 0 | 0.00 | 0 | 0.00 | 0 | 0 | 1032.00 | 45 | 1500 | 0.8333 | 1032.00 | . 1799 rows × 30 columns . ex.winPlacePerc.mean() . 0.5552904391328511 . # 낙하산 떨어진 지점에 정확히 무기가 있어서 먹었는데 바로 죽은 경우를 생각해서 무기습득을 1보다 크게 잡았다 exindex = train.query(&#39;weaponsAcquired &gt; 1 and total_distance &lt; 1&#39;).index train = train.drop(exindex) exindex . Int64Index([ 846, 1357, 1824, 2640, 3826, 4737, 6673, 7327, 7776, 7812, ... 4441511, 4441545, 4441570, 4441644, 4442729, 4443136, 4444608, 4445068, 4446318, 4446682], dtype=&#39;int64&#39;, length=7255) . &#49828;&#54588;&#46300;&#54645; --&gt; &#49828;&#54588;&#46300;&#54645;&#51060; &#53448;&#44163;&#51012; &#53448;&#44144;&#46972;&#44256; &#49373;&#44033;&#46104;&#51648;&#45716; &#50506;&#45716;&#45796; &#44151;&#44592;&#45208; &#49688;&#50689;&#44144;&#47532;&#47196; &#54032;&#45800;&#54644;&#48372;&#51088; . 수영 기본속도 --&gt; 3.3 m/s | 달리기 기본속도 --&gt; 6.0 m/s | . show_distplot(&#39;swimDistance&#39;) . /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . train[&#39;swimmer&#39;] = 3.3 * train.matchDuration exindex = train.query(&#39;swimmer &lt; swimDistance&#39;).index train = train.drop(exindex) exindex . Int64Index([1227362], dtype=&#39;int64&#39;) . show_distplot(&#39;walkDistance&#39;) . /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . train[&#39;hiker&#39;] = 6 * train.matchDuration exindex = train.query(&#39;hiker &lt; walkDistance&#39;).index train = train.drop(exindex) exindex . Int64Index([ 23026, 42183, 68590, 125103, 179341, 250629, 297474, 326613, 326773, 412967, ... 4106226, 4259976, 4287887, 4288445, 4306598, 4318693, 4380785, 4382774, 4405009, 4415088], dtype=&#39;int64&#39;, length=139) . &#51228;&#52636; . train.drop([&#39;Id&#39;,&#39;groupId&#39;,&#39;matchId&#39;,&#39;hiker&#39;,&#39;swimmer&#39;,&#39;total_distance&#39;,&#39;heals_boosts&#39;], axis = 1, inplace = True) test.drop([&#39;Id&#39;,&#39;groupId&#39;,&#39;matchId&#39;,&#39;total_distance&#39;], axis = 1, inplace = True) . train = pd.get_dummies(train, columns=[&#39;matchType&#39;]) test = pd.get_dummies(test, columns = [&#39;matchType&#39;]) . x_train = train.drop([&#39;winPlacePerc&#39;], axis = 1) y_train = train.winPlacePerc . from sklearn.linear_model import LinearRegression lr = LinearRegression() lr.fit(x_train, y_train) . LinearRegression() . test[&#39;winPlacePerc&#39;] = lr.predict(test) . test2 = pd.read_csv(&#39;test_V2.csv.zip&#39;) test[&#39;Id&#39;] = test2[&#39;Id&#39;] . submission = pd.read_csv(&#39;sample_submission_V2.csv.zip&#39;) . submission . Id winPlacePerc . 0 9329eb41e215eb | 0.191694 | . 1 639bd0dcd7bda8 | 0.925974 | . 2 63d5c8ef8dfe91 | 0.466032 | . 3 cf5b81422591d1 | 0.517204 | . 4 ee6a295187ba21 | 0.962933 | . ... ... | ... | . 1934169 a316c3a13887d5 | 0.623755 | . 1934170 5312146b27d875 | 0.413684 | . 1934171 fc8818b5b32ad3 | 0.814674 | . 1934172 a0f91e35f8458f | 0.778642 | . 1934173 3696fc9f3a42b2 | 0.017932 | . 1934174 rows × 2 columns . submission.to_csv(&#39;submission.csv&#39;, index = False) . !kaggle competitions submit -c pubg-finish-placement-prediction -f submission.csv -m &quot;Message&quot; . Warning: Looks like you&#39;re using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4) 100% 63.1M/63.1M [00:00&lt;00:00, 86.4MB/s] 400 - Bad Request .",
            "url": "https://parkjeongung.github.io/Ung.github.io/2022/01/12/PUBG_%EC%A0%84%EC%B2%98%EB%A6%AC_%EC%97%B0%EC%8A%B5_query.html",
            "relUrl": "/2022/01/12/PUBG_%EC%A0%84%EC%B2%98%EB%A6%AC_%EC%97%B0%EC%8A%B5_query.html",
            "date": " • Jan 12, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Group Product Classification // 인코딩 // Confusion Matrix",
            "content": ". &#45936;&#51060;&#53552; &#48520;&#47084;&#50724;&#44592; . pip install kaggle --upgrade . Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12) Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0) Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.3) Requirement already satisfied: six&gt;=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0) Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2) Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3) Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.10.8) Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2) Requirement already satisfied: text-unidecode&gt;=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify-&gt;kaggle) (1.3) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (3.0.4) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (2.10) . !pip install kaggle from google.colab import files files.upload() . Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12) Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.10.8) Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0) Requirement already satisfied: six&gt;=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0) Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3) Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2) Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2) Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.3) Requirement already satisfied: text-unidecode&gt;=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify-&gt;kaggle) (1.3) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (2.10) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (3.0.4) . Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable. Saving kaggle (1).json to kaggle (1).json . {&#39;kaggle (1).json&#39;: b&#39;{&#34;username&#34;:&#34;chobocoder&#34;,&#34;key&#34;:&#34;89866a08becb23b3c536081d68f0c29b&#34;}&#39;} . !mkdir -p ~/.kaggle !cp kaggle.json ~/.kaggle/ !chmod 600 ~/.kaggle/kaggle.json . !kaggle competitions download -c otto-group-product-classification-challenge . 401 - Unauthorized . import pandas as pd train = pd.read_csv(&#39;/content/drive/MyDrive/mydata/train.csv&#39;) test = pd.read_csv(&#39;/content/drive/MyDrive/mydata/test.csv&#39;) . train.head().T . 0 1 2 3 4 . id 1 | 2 | 3 | 4 | 5 | . feat_1 1 | 0 | 0 | 1 | 0 | . feat_2 0 | 0 | 0 | 0 | 0 | . feat_3 0 | 0 | 0 | 0 | 0 | . feat_4 0 | 0 | 0 | 1 | 0 | . ... ... | ... | ... | ... | ... | . feat_90 0 | 0 | 0 | 0 | 1 | . feat_91 0 | 0 | 0 | 0 | 0 | . feat_92 0 | 0 | 0 | 0 | 0 | . feat_93 0 | 0 | 0 | 0 | 0 | . target Class_1 | Class_1 | Class_1 | Class_1 | Class_1 | . 95 rows × 5 columns . &#44208;&#52769;&#52824; &#54869;&#51064; . sum(train.isnull().sum()) . id 0 feat_1 0 feat_2 0 feat_3 0 feat_4 0 .. feat_90 0 feat_91 0 feat_92 0 feat_93 0 target 0 Length: 95, dtype: int64 . &#53440;&#44191; &#48516;&#47532; . x_train = train.drop(&#39;target&#39;, axis = 1) y_train = train[&#39;target&#39;] . &#51064;&#53076;&#46377; . 사이킷런 알고리즘은 문자열 값을 입력값으로 허용하지 않는다 | 모든 문자열 값은 인코딩 되어 숫자 형으로 변환해야 한다. | . &#46972;&#48296; &#51064;&#53076;&#46377; (Label Encoding) . 카테코리 피쳐를 숫자 값으로 변환한다. | 숫자 값으로 변환 되었기 때문에 특정 알고리즘에서는 이를 가중치로 인식할 수 있다. | 트리 계열의 알고리즘을 사용할 때 주로 사용된다. | . from sklearn.preprocessing import LabelEncoder items=[&#39;TV&#39;, &#39;냉장고&#39;, &#39;전자레인지&#39;, &#39;컴퓨터&#39;, &#39;선풍기&#39;, &#39;믹서&#39;, &#39;믹서&#39;] encoder = LabelEncoder() encoder.fit(items) label = encoder.transform(items) label . array([0, 1, 4, 5, 3, 2, 2]) . # 데이터가 많은 경우에는 classes_ 를 이용해서 확인한다 encoder.classes_ # 처음부터 0, 1 , 2 ... 순서대로 . array([&#39;TV&#39;, &#39;냉장고&#39;, &#39;믹서&#39;, &#39;선풍기&#39;, &#39;전자레인지&#39;, &#39;컴퓨터&#39;], dtype=&#39;&lt;U5&#39;) . print(encoder.inverse_transform([0, 1, 4, 5, 3, 2, 2])) print(items) . [&#39;TV&#39; &#39;냉장고&#39; &#39;전자레인지&#39; &#39;컴퓨터&#39; &#39;선풍기&#39; &#39;믹서&#39; &#39;믹서&#39;] [&#39;TV&#39;, &#39;냉장고&#39;, &#39;전자레인지&#39;, &#39;컴퓨터&#39;, &#39;선풍기&#39;, &#39;믹서&#39;, &#39;믹서&#39;] . &#50896;&#54635; &#51064;&#53076;&#46377; (One-Hot Encoding) . # 원핫 인코딩은 입력값으로 2차원 데이터가 필요하다 from sklearn.preprocessing import OneHotEncoder import numpy as np items=[&#39;TV&#39;, &#39;냉장고&#39;, &#39;전자레인지&#39;, &#39;컴퓨터&#39;, &#39;선풍기&#39;, &#39;믹서&#39;, &#39;믹서&#39;] # 숫자형 값으로 변환 (feat.라벨인코딩) encoder = LabelEncoder() encoder.fit(items) label = encoder.transform(items) # 2차원 데이터로 변환 label = label.reshape(-1, 1) # 원핫인코딩 encoder = OneHotEncoder() encoder.fit(label) label = encoder.transform(label) print(label.toarray()) . [[1. 0. 0. 0. 0. 0.] [0. 1. 0. 0. 0. 0.] [0. 0. 0. 0. 1. 0.] [0. 0. 0. 0. 0. 1.] [0. 0. 0. 1. 0. 0.] [0. 0. 1. 0. 0. 0.] [0. 0. 1. 0. 0. 0.]] . import pandas as pd items = [&#39;TV&#39;, &#39;냉장고&#39;, &#39;전자레인지&#39;, &#39;컴퓨터&#39;, &#39;선풍기&#39;, &#39;믹서&#39;, &#39;믹서&#39;] df = pd.DataFrame({&#39;item&#39;:items}) df . item . 0 TV | . 1 냉장고 | . 2 전자레인지 | . 3 컴퓨터 | . 4 선풍기 | . 5 믹서 | . 6 믹서 | . pd.get_dummies(df) . item_TV item_냉장고 item_믹서 item_선풍기 item_전자레인지 item_컴퓨터 . 0 1 | 0 | 0 | 0 | 0 | 0 | . 1 0 | 1 | 0 | 0 | 0 | 0 | . 2 0 | 0 | 0 | 0 | 1 | 0 | . 3 0 | 0 | 0 | 0 | 0 | 1 | . 4 0 | 0 | 0 | 1 | 0 | 0 | . 5 0 | 0 | 1 | 0 | 0 | 0 | . 6 0 | 0 | 1 | 0 | 0 | 0 | . KNN . from sklearn.neighbors import KNeighborsClassifier kn = KNeighborsClassifier( algorithm = &#39;auto&#39;, # 가장 가까운 이웃을 계산하는데 사용할 알고리즘을 선택 (ball_tree, kd_tree 등이 있음) metric = &#39;minkowski&#39;, # 거리 측정방식 선택 // minkowski 는 맨하튼 과 유클리드 거리공식을 하나로 나타낸 것 p = 2, # p값이 1일때는 맨하튼 거리공식, 2일때는 유클리드 거리공식을 사용 (defalt = 2) weights = &#39;distance&#39; # 예측에 사용하는 가중치 --&gt; &quot;distance&quot; 는 가까운 이웃에 가중치를 더 주고 &quot;uniform&quot; 은 이웃에 모두 동일한 가중치를 준다 ) . . Confusion Matrix // &#48516;&#47448;&#47784;&#45944; &#54217;&#44032; . from sklearn.model_selection import train_test_split x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size = 0.2, random_state = 42, stratify = y_train) x = [x_train, y_train, x_test, y_test] for i in x: print(i.shape) . (49502, 94) (49502,) (12376, 94) (12376,) . from sklearn.preprocessing import StandardScaler ss = StandardScaler() ss.fit(x_train) x_train_scaled = ss.transform(x_train) x_test_scaled = ss.transform(x_test) # 모델링 from sklearn.neighbors import KNeighborsClassifier kn = KNeighborsClassifier( algorithm = &#39;auto&#39;, # 가장 가까운 이웃을 계산하는데 사용할 알고리즘을 선택 (ball_tree, kd_tree 등이 있음) metric = &#39;minkowski&#39;, # 거리 측정방식 선택 // minkowski 는 맨하튼 과 유클리드 거리공식을 하나로 나타낸 것 p = 2, # p값이 1일때는 맨하튼 거리공식, 2일때는 유클리드 거리공식을 사용 (defalt = 2) weights = &#39;distance&#39; # 예측에 사용하는 가중치 --&gt; &quot;distance&quot; 는 가까운 이웃에 가중치를 더 주고 &quot;uniform&quot; 은 이웃에 모두 동일한 가중치를 준다 ) kn.fit(x_train_scaled, y_train) kn_predict = kn.predict(x_test_scaled) # confusion matrix from sklearn.metrics import confusion_matrix, accuracy_score, classification_report cm = confusion_matrix(y_test, kn_predict) print(cm) print(accuracy_score(y_test, kn_predict)) . [[ 308 30 6 0 1 14 4 10 13] [ 7 2840 333 25 5 2 11 1 0] [ 0 456 1058 70 2 1 9 3 2] [ 0 80 182 253 6 15 2 0 0] [ 0 5 8 0 532 3 0 0 0] [ 2 7 12 10 8 2654 56 52 26] [ 0 5 25 15 2 45 423 42 11] [ 3 2 2 2 0 61 29 1552 42] [ 1 0 1 1 0 30 7 38 913]] 0.8510827407886231 . Confusion Matrix &#49884;&#44033;&#54868; . import seaborn as sns import matplotlib.pyplot as plt plt.figure(figsize = (10, 7)) sns.heatmap(cm, annot = True) # 정규화를 하는데 귀찮음이 있다 . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f2fb0ad95d0&gt; . from sklearn.metrics import plot_confusion_matrix figure, ax = plt.subplots(figsize = (15, 10)) plot = plot_confusion_matrix(kn, x_test_scaled, y_test, ax = ax) plt.show() . /usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator. warnings.warn(msg, category=FutureWarning) . from sklearn.metrics import plot_confusion_matrix figure , ax = plt.subplots(figsize = (15, 10)) plot_confusion_matrix(kn, x_test_scaled, y_test, normalize = &#39;true&#39;, ax= ax) plt.show() . /usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator. warnings.warn(msg, category=FutureWarning) . RandomForest . from sklearn.ensemble import RandomForestClassifier rf = RandomForestClassifier( n_estimator = 100, # 앙상블을 구성할 트리의 개수를 지정 criterion = &#39;gini&#39;, # 어떤 불순도를 사용하여 트리 분할 할것인지 지정 &#39;gini&#39; or &#39;entropy&#39; max_features = &#39;auto&#39;, # 각 트리를 구성할 피처의 갯수 선택 bootstrap = &#39;True&#39;, # 부트스트랩 샘플을 사용할 것인지 여부 결정 // False 면 전체 데이터 샘플이 각 트리의 훈련데이터가 됨 warm_start = &#39;False&#39;, # 이전에 훈련된 모델을 불러와 추가훈련을 시킬 것인지를 결정 class_weight = &#39;None&#39;) # &quot;balanced&quot; --&gt; n_samples / (n_classes * np.bincount(y)) 각 클래스의 빈도수에 반비례하게 가중치를 조절한다 # &quot;balanced_subsample&quot; --&gt; 부트스트랩 샘플로 각 트리마다 빈도수를 결정해서 가중치를 조절한다 .",
            "url": "https://parkjeongung.github.io/Ung.github.io/2022/01/12/Group_Product_Classification_%EC%9D%B8%EC%BD%94%EB%94%A9_Confusion_Matrix.html",
            "relUrl": "/2022/01/12/Group_Product_Classification_%EC%9D%B8%EC%BD%94%EB%94%A9_Confusion_Matrix.html",
            "date": " • Jan 12, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "기본적인 EDA 와 전처리 (1주차)",
            "content": ". EDA . &#54028;&#51060;&#50028; &#46972;&#51060;&#48652;&#47084;&#47532; &#48520;&#47084;&#50724;&#44592; (import) . import [라이브러리] as [사용할이름] | from [라이브러리] import [라이브러리] | . import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split import warnings warnings.filterwarnings(&quot;ignore&quot;) . &#54028;&#51060;&#50028; &#54028;&#51068; &#48520;&#47084;&#50724;&#44592; (read_csv) . import pandas as pd | data = pd.read_csv(&#39;파일경로/파일이름.csv&#39;) | . import pandas as pd # 방법 1 // 경로복사 노가다 train_data = pd.read_csv(&#39;/content/drive/MyDrive/mydata/movies_train.csv&#39;) test_data = pd.read_csv(&#39;/content/drive/MyDrive/mydata/movies_test.csv&#39;) sub_data = pd.read_csv(&#39;/content/drive/MyDrive/mydata/submission.csv&#39;) train_data.head() . title distributor genre release_time time screening_rat director dir_prev_bfnum dir_prev_num num_staff num_actor box_off_num . 0 개들의 전쟁 | 롯데엔터테인먼트 | 액션 | 2012-11-22 | 96 | 청소년 관람불가 | 조병옥 | NaN | 0 | 91 | 2 | 23398 | . 1 내부자들 | (주)쇼박스 | 느와르 | 2015-11-19 | 130 | 청소년 관람불가 | 우민호 | 1161602.50 | 2 | 387 | 3 | 7072501 | . 2 은밀하게 위대하게 | (주)쇼박스 | 액션 | 2013-06-05 | 123 | 15세 관람가 | 장철수 | 220775.25 | 4 | 343 | 4 | 6959083 | . 3 나는 공무원이다 | (주)NEW | 코미디 | 2012-07-12 | 101 | 전체 관람가 | 구자홍 | 23894.00 | 2 | 20 | 6 | 217866 | . 4 불량남녀 | 쇼박스(주)미디어플렉스 | 코미디 | 2010-11-04 | 108 | 15세 관람가 | 신근호 | 1.00 | 1 | 251 | 2 | 483387 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; path = &#39;/content/drive/MyDrive/mydata/&#39; # 변수지정 train_data = pd.read_csv(path+&#39;movies_train.csv&#39;) test_data = pd.read_csv(path+&#39;movies_test.csv&#39;) sub_data = pd.read_csv(path+&#39;submission.csv&#39;) train_data.head() . title distributor genre release_time time screening_rat director dir_prev_bfnum dir_prev_num num_staff num_actor box_off_num . 0 개들의 전쟁 | 롯데엔터테인먼트 | 액션 | 2012-11-22 | 96 | 청소년 관람불가 | 조병옥 | NaN | 0 | 91 | 2 | 23398 | . 1 내부자들 | (주)쇼박스 | 느와르 | 2015-11-19 | 130 | 청소년 관람불가 | 우민호 | 1161602.50 | 2 | 387 | 3 | 7072501 | . 2 은밀하게 위대하게 | (주)쇼박스 | 액션 | 2013-06-05 | 123 | 15세 관람가 | 장철수 | 220775.25 | 4 | 343 | 4 | 6959083 | . 3 나는 공무원이다 | (주)NEW | 코미디 | 2012-07-12 | 101 | 전체 관람가 | 구자홍 | 23894.00 | 2 | 20 | 6 | 217866 | . 4 불량남녀 | 쇼박스(주)미디어플렉스 | 코미디 | 2010-11-04 | 108 | 15세 관람가 | 신근호 | 1.00 | 1 | 251 | 2 | 483387 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; &#52880;&#44544;&#50672;&#46041; . 캐글 자전거 수요 예측 데이터 | . pip install kaggle --upgrade . Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12) Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.10.8) Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3) Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2) Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0) Requirement already satisfied: six&gt;=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0) Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2) Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.3) Requirement already satisfied: text-unidecode&gt;=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify-&gt;kaggle) (1.3) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (2.10) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (3.0.4) . !pip install kaggle from google.colab import files files.upload() . Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12) Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2) Requirement already satisfied: six&gt;=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0) Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0) Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3) Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.3) Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.10.8) Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2) Requirement already satisfied: text-unidecode&gt;=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify-&gt;kaggle) (1.3) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (3.0.4) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (2.10) . Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable. Saving kaggle.json to kaggle.json . {&#39;kaggle.json&#39;: b&#39;{&#34;username&#34;:&#34;chobocoder&#34;,&#34;key&#34;:&#34;514a8f418d2b9a0ffce95b37a62e1215&#34;}&#39;} . !mkdir -p ~/.kaggle !cp kaggle.json ~/.kaggle/ !chmod 600 ~/.kaggle/kaggle.json . !kaggle competitions download -c bike-sharing-demand . Warning: Looks like you&#39;re using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4) Downloading train.csv to /content 0% 0.00/633k [00:00&lt;?, ?B/s] 100% 633k/633k [00:00&lt;00:00, 31.6MB/s] Downloading test.csv to /content 0% 0.00/316k [00:00&lt;?, ?B/s] 100% 316k/316k [00:00&lt;00:00, 117MB/s] Downloading sampleSubmission.csv to /content 0% 0.00/140k [00:00&lt;?, ?B/s] 100% 140k/140k [00:00&lt;00:00, 44.3MB/s] . import pandas as pd train_data = pd.read_csv(&#39;train.csv&#39;) test_data = pd.read_csv(&#39;test.csv&#39;) sub_data = pd.read_csv(&#39;sampleSubmission.csv&#39;) train_data.head() . datetime season holiday workingday weather temp atemp humidity windspeed casual registered count . 0 2011-01-01 00:00:00 | 1 | 0 | 0 | 1 | 9.84 | 14.395 | 81 | 0.0 | 3 | 13 | 16 | . 1 2011-01-01 01:00:00 | 1 | 0 | 0 | 1 | 9.02 | 13.635 | 80 | 0.0 | 8 | 32 | 40 | . 2 2011-01-01 02:00:00 | 1 | 0 | 0 | 1 | 9.02 | 13.635 | 80 | 0.0 | 5 | 27 | 32 | . 3 2011-01-01 03:00:00 | 1 | 0 | 0 | 1 | 9.84 | 14.395 | 75 | 0.0 | 3 | 10 | 13 | . 4 2011-01-01 04:00:00 | 1 | 0 | 0 | 1 | 9.84 | 14.395 | 75 | 0.0 | 0 | 1 | 1 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; &#54028;&#51060;&#50028; &#54665; &#50676; &#44079;&#49688; &#44288;&#52272;&#54616;&#44592; (shape) . train_data.shape # 행 10886 개 # 열 12 개 . (10886, 12) . &#54028;&#51060;&#50028; &#45936;&#51060;&#53552; &#54869;&#51064;&#54616;&#44592; (head(), tail()) . head() // 앞의 5개 행까지 출력 | tail() // 뒤의 5개 행까지 출력 | . train_data.head() . datetime season holiday workingday weather temp atemp humidity windspeed casual registered count . 0 2011-01-01 00:00:00 | 1 | 0 | 0 | 1 | 9.84 | 14.395 | 81 | 0.0 | 3 | 13 | 16 | . 1 2011-01-01 01:00:00 | 1 | 0 | 0 | 1 | 9.02 | 13.635 | 80 | 0.0 | 8 | 32 | 40 | . 2 2011-01-01 02:00:00 | 1 | 0 | 0 | 1 | 9.02 | 13.635 | 80 | 0.0 | 5 | 27 | 32 | . 3 2011-01-01 03:00:00 | 1 | 0 | 0 | 1 | 9.84 | 14.395 | 75 | 0.0 | 3 | 10 | 13 | . 4 2011-01-01 04:00:00 | 1 | 0 | 0 | 1 | 9.84 | 14.395 | 75 | 0.0 | 0 | 1 | 1 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; train_data.tail() . datetime season holiday workingday weather temp atemp humidity windspeed casual registered count . 10881 2012-12-19 19:00:00 | 4 | 0 | 1 | 1 | 15.58 | 19.695 | 50 | 26.0027 | 7 | 329 | 336 | . 10882 2012-12-19 20:00:00 | 4 | 0 | 1 | 1 | 14.76 | 17.425 | 57 | 15.0013 | 10 | 231 | 241 | . 10883 2012-12-19 21:00:00 | 4 | 0 | 1 | 1 | 13.94 | 15.910 | 61 | 15.0013 | 4 | 164 | 168 | . 10884 2012-12-19 22:00:00 | 4 | 0 | 1 | 1 | 13.94 | 17.425 | 61 | 6.0032 | 12 | 117 | 129 | . 10885 2012-12-19 23:00:00 | 4 | 0 | 1 | 1 | 13.12 | 16.665 | 66 | 8.9981 | 4 | 84 | 88 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; train_data.head().T . 0 1 2 3 4 . datetime 2011-01-01 00:00:00 | 2011-01-01 01:00:00 | 2011-01-01 02:00:00 | 2011-01-01 03:00:00 | 2011-01-01 04:00:00 | . season 1 | 1 | 1 | 1 | 1 | . holiday 0 | 0 | 0 | 0 | 0 | . workingday 0 | 0 | 0 | 0 | 0 | . weather 1 | 1 | 1 | 1 | 1 | . temp 9.84 | 9.02 | 9.02 | 9.84 | 9.84 | . atemp 14.395 | 13.635 | 13.635 | 14.395 | 14.395 | . humidity 81 | 80 | 80 | 75 | 75 | . windspeed 0 | 0 | 0 | 0 | 0 | . casual 3 | 8 | 5 | 3 | 0 | . registered 13 | 32 | 27 | 10 | 1 | . count 16 | 40 | 32 | 13 | 1 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; &#54028;&#51060;&#50028; &#44208;&#52769;&#52824; &#54869;&#51064;&#54616;&#44592; (is_null() ) . 결측치 (Missing Value) - 데이터에 값이 없음 | . import pandas as pd import numpy as np train_data = pd.read_csv(path+&#39;movies_train.csv&#39;) # 결측치 확인 train_data.isnull() . title distributor genre release_time time screening_rat director dir_prev_bfnum dir_prev_num num_staff num_actor box_off_num . 0 False | False | False | False | False | False | False | True | False | False | False | False | . 1 False | False | False | False | False | False | False | False | False | False | False | False | . 2 False | False | False | False | False | False | False | False | False | False | False | False | . 3 False | False | False | False | False | False | False | False | False | False | False | False | . 4 False | False | False | False | False | False | False | False | False | False | False | False | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 595 False | False | False | False | False | False | False | False | False | False | False | False | . 596 False | False | False | False | False | False | False | False | False | False | False | False | . 597 False | False | False | False | False | False | False | True | False | False | False | False | . 598 False | False | False | False | False | False | False | True | False | False | False | False | . 599 False | False | False | False | False | False | False | True | False | False | False | False | . 600 rows × 12 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; train_data.isnull().sum() . title 0 distributor 0 genre 0 release_time 0 time 0 screening_rat 0 director 0 dir_prev_bfnum 330 dir_prev_num 0 num_staff 0 num_actor 0 box_off_num 0 dtype: int64 . &#54028;&#51060;&#50028; &#45936;&#51060;&#53552; &#44592;&#48376; &#51221;&#48372; &#54869;&#51064;&#54616;&#44592; (info()) . info() 로도 데이터 결측치를 확인할 수 있다 | . train_data.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 600 entries, 0 to 599 Data columns (total 12 columns): # Column Non-Null Count Dtype -- -- 0 title 600 non-null object 1 distributor 600 non-null object 2 genre 600 non-null object 3 release_time 600 non-null object 4 time 600 non-null int64 5 screening_rat 600 non-null object 6 director 600 non-null object 7 dir_prev_bfnum 270 non-null float64 8 dir_prev_num 600 non-null int64 9 num_staff 600 non-null int64 10 num_actor 600 non-null int64 11 box_off_num 600 non-null int64 dtypes: float64(1), int64(5), object(6) memory usage: 56.4+ KB . &#54028;&#51060;&#50028; &#44208;&#52769;&#52824;&#49325;&#51228;, &#45824;&#52404; (dropna, fillna) . 결측치는 비율에 따라서 처리 방법을 달리한다 10% 미만이라면 : row를 삭제하거나 치환한다. | 10~50% 사이라면 : 모델을 만들어서 예측한다. | 50% 이상이라면 : column을 삭제한다. | | . # aixs = 0 ex = train_data.copy() # 데이터 복사 ex = ex.dropna(axis = 0) # 결측치가 있는 행 제거 ex.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 270 entries, 1 to 596 Data columns (total 12 columns): # Column Non-Null Count Dtype -- -- 0 title 270 non-null object 1 distributor 270 non-null object 2 genre 270 non-null object 3 release_time 270 non-null object 4 time 270 non-null int64 5 screening_rat 270 non-null object 6 director 270 non-null object 7 dir_prev_bfnum 270 non-null float64 8 dir_prev_num 270 non-null int64 9 num_staff 270 non-null int64 10 num_actor 270 non-null int64 11 box_off_num 270 non-null int64 dtypes: float64(1), int64(5), object(6) memory usage: 27.4+ KB . # 훈련세트 생성 train = ex.drop([&#39;title&#39;, &#39;distributor&#39;, &#39;genre&#39;, &#39;screening_rat&#39;, &#39;director&#39;, &#39;release_time&#39;, &#39;dir_prev_bfnum&#39;], axis = 1) train_target = ex.dir_prev_bfnum # 테스트세트 생성 test = train_data.copy() test = train_data.drop([&#39;title&#39;, &#39;distributor&#39;, &#39;genre&#39;, &#39;screening_rat&#39;, &#39;director&#39;, &#39;release_time&#39;, &#39;dir_prev_bfnum&#39;], axis = 1) test_target = train_data.dir_prev_bfnum # 예측해야할 타겟 값 # 모델링 from sklearn.linear_model import LinearRegression lr = LinearRegression() lr.fit(train, target) # 타겟값 예측 train_data.dir_prev_bfnum = lr.predict(test) train_data.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 600 entries, 0 to 599 Data columns (total 12 columns): # Column Non-Null Count Dtype -- -- 0 title 600 non-null object 1 distributor 600 non-null object 2 genre 600 non-null object 3 release_time 600 non-null object 4 time 600 non-null int64 5 screening_rat 600 non-null object 6 director 600 non-null object 7 dir_prev_bfnum 600 non-null float64 8 dir_prev_num 600 non-null int64 9 num_staff 600 non-null int64 10 num_actor 600 non-null int64 11 box_off_num 600 non-null int64 dtypes: float64(1), int64(5), object(6) memory usage: 56.4+ KB . # axis = 1 ex = train_data.copy() # 데이터 복사 ex = ex.dropna(axis = 1) # 결측치가 있는 열 제거 ex.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 600 entries, 0 to 599 Data columns (total 11 columns): # Column Non-Null Count Dtype -- -- 0 title 600 non-null object 1 distributor 600 non-null object 2 genre 600 non-null object 3 release_time 600 non-null object 4 time 600 non-null int64 5 screening_rat 600 non-null object 6 director 600 non-null object 7 dir_prev_num 600 non-null int64 8 num_staff 600 non-null int64 9 num_actor 600 non-null int64 10 box_off_num 600 non-null int64 dtypes: int64(5), object(6) memory usage: 51.7+ KB .",
            "url": "https://parkjeongung.github.io/Ung.github.io/2022/01/12/1%EC%A3%BC%EC%B0%A8_%EA%B8%B0%EB%B3%B8%EC%A0%81%EC%9D%B8_EDA_%EC%99%80_%EC%A0%84%EC%B2%98%EB%A6%AC.html",
            "relUrl": "/2022/01/12/1%EC%A3%BC%EC%B0%A8_%EA%B8%B0%EB%B3%B8%EC%A0%81%EC%9D%B8_EDA_%EC%99%80_%EC%A0%84%EC%B2%98%EB%A6%AC.html",
            "date": " • Jan 12, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "지도학습 비지도학습 // 훈련세트 테스트세트",
            "content": ". &#54984;&#47144;&#49464;&#53944;&#50752; &#53580;&#49828;&#53944;&#49464;&#53944; . 샘플링편향 발생 . fish_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0, 9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0] fish_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0, 6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9] . fish_data = [[l, w] for l, w in zip(fish_length, fish_weight)] fish_target = [1] * 35 + [0] * 14 . from sklearn.neighbors import KNeighborsClassifier kn = KNeighborsClassifier() . train_input = fish_data[: 35] train_target = fish_target[: 35] test_input = fish_data[35: ] test_target = fish_target[35: ] . # 테스트세트 --&gt; score()함수로 모델 평가 kn = kn.fit(train_input, train_target) kn.score(test_input, test_target) #샘플링 편향 발생 --&gt; 마지막 14개(빙어 특성 14개)를 test_input 으로 넣어놔서 훈련에 사용된 train_input 에는 빙어가 하나도 없음 --&gt; 데이터를 섞든지 골고루 샘플을 뽑아야 함 --&gt; numpy 사용 . 0.0 . NUMPY&#47484; &#51060;&#50857;&#54620; &#49368;&#54540;&#47553; &#54200;&#54693; &#54644;&#44208; . 파이썬의 대표적인 배열 라이브러리 | 고차원의 배열을 쉽게 만들고 조작가능 | . import numpy as np . input_arr = np.array(fish_data) target_arr = np.array(fish_target) . print(input_arr.shape) . (49, 2) . # input 과 target 에서 같은 인덱스는 함께 선택되어야 한다. input의 2번은 train으로 target의 2번은 test로 가면 안된다. # 넘파이의 random 함수들은 실행할 때마다 다른 결과를 만든다. --&gt; random.seed()를 지정하면 항상 일정한 결과를 얻을 수 있다. np.random.seed(42) index = np.arange(49) np.random.shuffle(index) index . array([13, 45, 47, 44, 17, 27, 26, 25, 31, 19, 12, 4, 34, 8, 3, 6, 40, 41, 46, 15, 9, 16, 24, 33, 30, 0, 43, 32, 5, 29, 11, 36, 1, 21, 2, 37, 35, 23, 39, 10, 22, 18, 48, 20, 7, 42, 14, 28, 38]) . print(input_arr[[1,3]]) # input_arr의 2번째와 4번째 원소 출력 # numpy 배열을 인덱스로 전하기 --&gt; 훈련세트 생성 train_input = input_arr[index [ : 35]] train_target = input_arr[index [ : 35]] #랜덤으로 만들어진 index의 첫번째 원소는 13 --&gt; input_arr의 14번째 원소(index = 13)가 train_input의 1번째 원소(index = 0)에 들어감 print(input_arr[13], train_input[0]) # numpy 배열을 인덱스로 전하기 --&gt; 테스트세트 생성 test_input = input_arr[index [35 : ]] test_target = input_arr[index [35 : ]] #랜덤으로 만들어진 index의 35번째 원소는 37 --&gt; input_arr의 38번째 원소(indexe = 37)가 test_input의 1번째 원소(index = 0)에 들어감 print(input_arr[37], test_input[0]) . [[ 26.3 290. ] [ 29. 363. ]] [ 32. 340.] [ 32. 340.] [10.6 7. ] [10.6 7. ] . import matplotlib.pyplot as plt plt.scatter(train_input[ :, 0], train_input[ :, 1]) plt.scatter(test_input[ :, 0], test_input[ :, 1]) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() . &#50672;&#49845; . fish_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0, 9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0] fish_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0, 6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9] fish_data = [[l, w] for l, w in zip(fish_length, fish_weight)] fish_target = [1] * 35 + [0] * 14 import numpy as np input_arr = np.array(fish_data) target_arr = np.array(fish_target) np.random.seed(42) index = np.arange(49) np.random.shuffle(index) train_input = input_arr[index[ : 35]] train_target = target_arr[index [ : 35]] test_input = input_arr[index[35 :]] test_target = target_arr[index[35 :]] from sklearn.neighbors import KNeighborsClassifier kn = KNeighborsClassifier() kn.fit(train_input, train_target) kn.score(test_input, test_target) . 1.0 .",
            "url": "https://parkjeongung.github.io/Ung.github.io/2022/01/11/%EC%A7%80%EB%8F%84%ED%95%99%EC%8A%B5_%EB%B9%84%EC%A7%80%EB%8F%84%ED%95%99%EC%8A%B5_%ED%9B%88%EB%A0%A8%EC%84%B8%ED%8A%B8_%ED%85%8C%EC%8A%A4%ED%8A%B8%EC%84%B8%ED%8A%B8.html",
            "relUrl": "/2022/01/11/%EC%A7%80%EB%8F%84%ED%95%99%EC%8A%B5_%EB%B9%84%EC%A7%80%EB%8F%84%ED%95%99%EC%8A%B5_%ED%9B%88%EB%A0%A8%EC%84%B8%ED%8A%B8_%ED%85%8C%EC%8A%A4%ED%8A%B8%EC%84%B8%ED%8A%B8.html",
            "date": " • Jan 11, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "주성분 분석 // PCA",
            "content": ". &#52264;&#50896; &#52629;&#49548; . 3차원을 2차원으로 줄이는 개념이 아니라 특성의 갯수를 줄인다는 의미 | . &#51452;&#49457;&#48516; . 데이터를 가장 잘 표현하는 벡터를 찾는다 --&gt; 분산이 큰방향을 찾는다 | 찾은 벡터(주성분)를 원점에 맞춘다 | 샘플을 주성분에 투영한다 --&gt; S(4, 2) x1 = 4, x2 = 2 인 샘플을 P(4.5) 하나로 나타내어 차원을 축소시킴 | 찾은 벡터에 수직이고 분산이 가장 큰 다음 벡터(주성분)를 찾는다 --&gt; 수직인 벡터를 찾는 이유는 처음 찾았던 주성분이 표현하지 못하는 주성분을 찾기위해서임 | . !wget https://bit.ly/fruits_300_data -O fruits_300.npy import numpy as np fruits = np.load(&#39;fruits_300.npy&#39;) fruits_2d = fruits.reshape(-1, 100 * 100) . --2021-12-16 06:48:09-- https://bit.ly/fruits_300_data Resolving bit.ly (bit.ly)... 67.199.248.10, 67.199.248.11 Connecting to bit.ly (bit.ly)|67.199.248.10|:443... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://github.com/rickiepark/hg-mldl/raw/master/fruits_300.npy [following] --2021-12-16 06:48:09-- https://github.com/rickiepark/hg-mldl/raw/master/fruits_300.npy Resolving github.com (github.com)... 140.82.114.4 Connecting to github.com (github.com)|140.82.114.4|:443... connected. HTTP request sent, awaiting response... 302 Found Location: https://raw.githubusercontent.com/rickiepark/hg-mldl/master/fruits_300.npy [following] --2021-12-16 06:48:09-- https://raw.githubusercontent.com/rickiepark/hg-mldl/master/fruits_300.npy Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 3000128 (2.9M) [application/octet-stream] Saving to: ‘fruits_300.npy’ fruits_300.npy 100%[===================&gt;] 2.86M --.-KB/s in 0.07s 2021-12-16 06:48:09 (41.8 MB/s) - ‘fruits_300.npy’ saved [3000128/3000128] . from sklearn.decomposition import PCA pca = PCA(n_components = 50) # n_components 로 주성분의 개수를 지정 pca.fit(fruits_2d) . PCA(n_components=50) . print(pca.components_.shape) . (50, 10000) . import matplotlib.pyplot as plt def draw_fruits(arr, ratio = 1): n = len(arr) rows = int(np.ceil(n / 10)) cols = n if rows &lt; 2 else 10 fig, axs = plt.subplots(rows, cols, figsize = (cols * ratio, rows * ratio), squeeze = False) for i in range(rows): for j in range(cols): if i * 10 + j &lt; n: axs[i, j].imshow(arr[i * 10 + j], cmap = &#39;gray_r&#39;) axs[i, j].axis(&#39;off&#39;) plt.show() . draw_fruits(pca.components_.reshape(-1, 100, 100)) . # 픽셀이 총 10000개 가 있었으니 이 데이터의 특성은 총 10000개 print(fruits_2d.shape) # 투영 --&gt; transform() fruits_pca = pca.transform(fruits_2d) print(fruits_pca.shape) # 10000개였던 특성을 50개로 줄임 --&gt; 차원 축소 . (300, 10000) (300, 50) . # 하지만 상당 부분 원본데이터를 재구성 할 수 있다 # 축소시킨 차원을 다시 복원한다 --&gt; inverse_transform fruits_inverse = pca.inverse_transform(fruits_pca) print(fruits_inverse.shape) # 50개였던 특성을 다시 10000개로 늘림 . (300, 10000) . fruits_reconstruct = fruits_inverse.reshape(-1, 100, 100) for start in [0, 100, 200]: draw_fruits(fruits_reconstruct[start : 100 + start]) print(&quot; n&quot;) # 조금 번지긴 했지만 그래도 잘 복원됨 . . &#49444;&#47749;&#46108; &#48516;&#49328; . 주성분이 원본 데이터를 얼마나 잘 나타내는지를 기록한 값 | . # 이 분산비율을 모두더하면 주성분으로 표현하는 총 분산비율 print(np.sum(pca.explained_variance_ratio_)) # 92퍼센트가 넘는 분산을 유지하고 있기에 데이터를 복원했을 때 이미지가 잘 나온거임 . 0.9215667058771523 . import matplotlib.pyplot as plt plt.plot(pca.explained_variance_ratio_) plt.show() # 이 그래프는 적절한 주성분의 개수를 찾는 데 도움이 됨 # 이 데이터 같은 경우는 처음 10개의 주성분이 대부분의 분산을 표현함 . &#45796;&#47480; &#50508;&#44256;&#47532;&#51608;&#44284; &#54632;&#44760; &#49324;&#50857;&#54616;&#44592; . PCA 로 축소시킨 데이터를 지도 학습에 적용하기 | . from sklearn.linear_model import LogisticRegression lr = LogisticRegression() . # 사과(0), 파인애플(1), 바나나(2) 각각 100개씩 import numpy as np target = np.array([0] * 100 + [1] * 100 + [2] * 100) target . array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) . from sklearn.model_selection import cross_validate scores = cross_validate(lr, fruits_2d, target) print(np.mean(scores[&#39;test_score&#39;])) print(np.mean(scores[&#39;fit_time&#39;])) # 원본 데이터는 특성이 10000개이기 때문에 과대적합되기 쉬움 # fit_time 에는 교차 검증 훈련 시간이 기록됨 . 0.9966666666666667 1.886031198501587 . scores = cross_validate(lr, fruits_pca, target) print(np.mean(scores[&#39;test_score&#39;])) print(np.mean(scores[&#39;fit_time&#39;])) # 50개의 특성으로 정확도 100% 만들어냄 # 또한 훈련시간도 줄고 데이터를 줄였기에 저장 공간까지 확보함 . 1.0 0.030258846282958985 . pca = PCA(n_components = 0.5) # 설명된 분산이 50%에 달하는 주성분을 찾는다 pca.fit(fruits_2d) print(pca.n_components_) # 2개의 주성분으로 원본 데이터에 있는 분산의 50%를 표현할 수 있다 . 2 . fruits_pca = pca.transform(fruits_2d) print(fruits_pca.shape) # 주성분이 2개이므로 특성이 2개 . (300, 2) . scores = cross_validate(lr, fruits_pca, target) print(np.mean(scores[&#39;test_score&#39;])) print(np.mean(scores[&#39;fit_time&#39;])) # 2개의 특성으로 좋은 정확도 . /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG, /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG, . 0.9933333333333334 0.17764811515808104 . /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG, . from sklearn.cluster import KMeans km = KMeans(n_clusters = 3, random_state = 42) km.fit(fruits_pca) print(np.unique(km.labels_, return_counts = True)) # 원본 데이터를 K-Means 에 적용 했을때 --&gt; [111, 98, 91] . (array([0, 1, 2], dtype=int32), array([110, 99, 91])) . for label in range(0, 3): draw_fruits(fruits[km.labels_ == label]) . # PCA 한 데이터는 특성이 2개이므로 2차원으로 표현이 가능하다 # 산점도 그리기 for label in range(0, 3): data = fruits_pca[km.labels_ == label] plt.scatter(data[:, 0], data[:, 1]) plt.legend([&#39;pineapple&#39;,&#39;banana&#39;,&#39;apple&#39;]) plt.show() # 바나나는 확실히 분리되어있지만 # 파인애플과 사과의 경계는 가까움 몇 개의 샘플이 혼동을 일으킬 가능성이 큼 .",
            "url": "https://parkjeongung.github.io/Ung.github.io/2022/01/11/%EC%A3%BC%EC%84%B1%EB%B6%84_%EB%B6%84%EC%84%9D_PCA.html",
            "relUrl": "/2022/01/11/%EC%A3%BC%EC%84%B1%EB%B6%84_%EB%B6%84%EC%84%9D_PCA.html",
            "date": " • Jan 11, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "앙상블 // 랜덤 포레스트 RandomForest // 엑스트라 트리 ExtraTreesClassifier",
            "content": ". &#47004;&#45924; &#54252;&#47112;&#49828;&#53944; // RandomForest . 랜덤하게 만들어진 여러개의 결정트리가 모인 숲 | 랜덤 포레스트는 각 트리마다 훈련 세트가 다름 --&gt; 각 트리의 훈련 세트를 만들때 전체 훈련세트에서 복원추출을 시행하여 만듬 --&gt; 부트스트랩 샘플 | 랜덤하게 샘플링된 훈련 세트 덕분에 과대적합을 방지해줌 | . import numpy as np import pandas as pd from sklearn.model_selection import train_test_split wine = pd.read_csv(&#39;https://bit.ly/wine_csv_data&#39;) wine.head() . alcohol sugar pH class . 0 9.4 | 1.9 | 3.51 | 0.0 | . 1 9.8 | 2.6 | 3.20 | 0.0 | . 2 9.8 | 2.3 | 3.26 | 0.0 | . 3 9.8 | 1.9 | 3.16 | 0.0 | . 4 9.4 | 1.9 | 3.51 | 0.0 | . wine_input = wine[[&#39;alcohol&#39;, &#39;sugar&#39;, &#39;pH&#39;]].to_numpy() wine_target = wine[&#39;class&#39;].to_numpy() train_input, test_input, train_target, test_target = train_test_split(wine_input, wine_target, test_size = 0.2, random_state = 42) . from sklearn.model_selection import cross_validate from sklearn.ensemble import RandomForestClassifier rf = RandomForestClassifier(n_jobs = -1, random_state = 42) score = cross_validate(rf, train_input, train_target ,n_jobs = -1, return_train_score = True) # return_train_score = True --&gt; 훈련 세트에 대한 점수도 반환 --&gt; 과대적합을 확인할때 편함 score . {&#39;fit_time&#39;: array([0.68797898, 0.68438673, 0.69149828, 0.68736911, 0.44117284]), &#39;score_time&#39;: array([0.10288453, 0.1029191 , 0.10267925, 0.10335732, 0.10253263]), &#39;test_score&#39;: array([0.88461538, 0.88942308, 0.90279115, 0.88931665, 0.88642926]), &#39;train_score&#39;: array([0.9971133 , 0.99663219, 0.9978355 , 0.9973545 , 0.9978355 ])} . print(np.mean(score[&#39;test_score&#39;]), np.mean(score[&#39;train_score&#39;])) # 과대적합이 나타남 --&gt; 랜덤포레스트도 결정트리를 사용하는 거기 때문에 가지치기가 필요할 것으로 보임 . 0.8905151032797809 0.9973541965122431 . rf.fit(train_input, train_target) print(rf.feature_importances_) # 결정트리만 사용했을 때의 feature_importances_ 결과 값 --&gt; [0.12345626 0.86862934 0.0079144] # 랜덤포레스트는 각 트리마다 전체 특성 개수의 제곱근만큼의 특성을 선택함 ex) 1번 트리 alcohol, sugar / 2번 트리 sugar,pH # 각 트리마다 특성의 일부만 훈련에 사용하기 때문에 하나의 특성에 과도하게 집중하지 않고 많은 특성을 사용할 수 있음 --&gt; 과대적합을 줄임 . [0.23167441 0.50039841 0.26792718] . # 복원추출을 했기 때문에 부트스트랩 샘플로 뽑히지 않은 샘플들이 존재 --&gt; OOB 샘플 # 이런 OOB 샘플을 이용해 부트스트랩 샘플로 훈련한 결정트리를 평가 할 수 있음 --&gt; oob_score = True --&gt; OOB 샘플이 검증세트가 되어 교차검증을 하는 느낌 rf = RandomForestClassifier(oob_score = True, random_state = 42, n_jobs = -1) rf.fit(train_input, train_target) print(rf.oob_score_) # cross_validate 로 확인한 점수와 비슷하게 나옴 . 0.8934000384837406 . &#50672;&#49845; . import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import cross_validate wine = pd.read_csv(&#39;https://bit.ly/wine_csv_data&#39;) wine.head() wine_input = wine[[&#39;alcohol&#39;, &#39;sugar&#39;, &#39;pH&#39;]].to_numpy() wine_target = wine[&#39;class&#39;].to_numpy() train_input, test_input, train_target, test_target = train_test_split(wine_input, wine_target, test_size = 0.2, random_state = 42) score = cross_validate(RandomForestClassifier(random_state = 42), train_input, train_target, n_jobs = -1, return_train_score = True) print(np.mean(score[&#39;test_score&#39;]), np.mean(score[&#39;train_score&#39;])) rf = RandomForestClassifier(random_state = 42, n_jobs = -1, oob_score = True) rf.fit(train_input, train_target) print(rf.oob_score_) . 0.8905151032797809 0.9973541965122431 0.8934000384837406 . &#50641;&#49828;&#53944;&#46972; &#53944;&#47532; . 부트스트랩 샘플을 사용하지 않는다 --&gt; 알고리즘 훈련 때 전체훈련세트 이용 | 노드를 분할할 때 무작위로 분할한다 | 무작위성 때문에 속도가 빠르다 | . from sklearn.ensemble import ExtraTreesClassifier et = ExtraTreesClassifier(n_jobs = -1, random_state = 42) scores = cross_validate(et, train_input, train_target, return_train_score = True, n_jobs = -1) print(np.mean(score[&#39;train_score&#39;]), np.mean(score[&#39;test_score&#39;])) . 0.9973541965122431 0.8905151032797809 . et.fit(train_input, train_target) print(et.feature_importances_) . [0.20183568 0.52242907 0.27573525] .",
            "url": "https://parkjeongung.github.io/Ung.github.io/2022/01/11/%EC%95%99%EC%83%81%EB%B8%94_%EB%9E%9C%EB%8D%A4_%ED%8F%AC%EB%A0%88%EC%8A%A4%ED%8A%B8_RandomForest_%EC%97%91%EC%8A%A4%ED%8A%B8%EB%9D%BC_%ED%8A%B8%EB%A6%AC_ExtraTreesClassifier.html",
            "relUrl": "/2022/01/11/%EC%95%99%EC%83%81%EB%B8%94_%EB%9E%9C%EB%8D%A4_%ED%8F%AC%EB%A0%88%EC%8A%A4%ED%8A%B8_RandomForest_%EC%97%91%EC%8A%A4%ED%8A%B8%EB%9D%BC_%ED%8A%B8%EB%A6%AC_ExtraTreesClassifier.html",
            "date": " • Jan 11, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "선형 회귀 // LinearRegression",
            "content": ". import numpy as np perch_length = np.array([8.4, 13.7, 15.0, 16.2, 17.4, 18.0, 18.7, 19.0, 19.6, 20.0, 21.0, 21.0, 21.0, 21.3, 22.0, 22.0, 22.0, 22.0, 22.0, 22.5, 22.5, 22.7, 23.0, 23.5, 24.0, 24.0, 24.6, 25.0, 25.6, 26.5, 27.3, 27.5, 27.5, 27.5, 28.0, 28.7, 30.0, 32.8, 34.5, 35.0, 36.5, 36.0, 37.0, 37.0, 39.0, 39.0, 39.0, 40.0, 40.0, 40.0, 40.0, 42.0, 43.0, 43.0, 43.5, 44.0]) # 농어의 타겟 = 무게 perch_weight = np.array([5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0, 115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0, 150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0, 218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0, 556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0, 850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0, 1000.0]) # train test split from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(perch_length, perch_weight, random_state = 42) # 2차원 변형 train_input = train_input.reshape(-1, 1) test_input = test_input.reshape(-1, 1) # 모델링 --&gt; 이웃개수 3개인 최근접회귀 from sklearn.neighbors import KNeighborsRegressor knr = KNeighborsRegressor(n_neighbors = 3) knr.fit(train_input, train_target) . KNeighborsRegressor(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;, metric_params=None, n_jobs=None, n_neighbors=3, p=2, weights=&#39;uniform&#39;) . 50cm &#51064; &#45453;&#50612;&#51032; &#47924;&#44172; &#50696;&#52769; . print(knr.predict([[50]])) # 그러나 실제 50cm인 농어의 무게와 오차가 심하다는 예시 . [1033.33333333] . distancecs, indexes = knr.kneighbors([[50]]) predict = knr.predict([[50]]) import matplotlib.pyplot as plt plt.scatter(train_input, train_target) plt.scatter(50, predict, marker = &#39;^&#39;) plt.scatter(train_input[indexes], train_target[indexes], marker = &#39;D&#39;) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() # 훈련세트의 범위를 벗어났기 때문에 엉뚱한 값을 출력함 --&gt; 100cm 농어의 경우도 똑같은 무게를 출력 print(knr.predict([[100]])) . [1033.33333333] . &#49440;&#54805; &#54924;&#44480; &#47784;&#45944;&#47553; . KNeighborsRegressor로 위 문제를 해결하려면 train 세트를 길이가 긴 농어가 포함되도록 다시 만들어야 한다. | 선형 회귀를 이용하면 train 세트의 범위를 벗어나도 길이가 긴 농어의 무게를 예측할 수 있다. | 선형 회귀에서 알고리즘이 찾아낸 직선의 기울기나 y절편 값을 &quot;모델 파라미터&quot; 라고 한다. | . from sklearn.linear_model import LinearRegression lr = LinearRegression() lr.fit(train_input, train_target) # 50cm 농어의 무게 예측값과 100cm 농어의 무게 예측값 print(lr.predict([[50], [100]])) . [1241.83860323 3192.69585141] . # 이러한 파라미터들은 coef_(기울기) 와 intercept(y절편) 에 저장되어 있다. # 머신러닝에서 &quot;기울기&quot;는 &quot;계수&quot; 또는 &quot;가중치&quot; 라고도 한다. print(lr.coef_, lr.intercept_) # 훈련세트와 농어의 길이 15 ~ 50를 기준으로 알고리즘이 찾아낸 직선 그리기 import matplotlib.pyplot as plt plt.scatter(train_input, train_target) # 15 ~ 50까지 직선 그리기 plt.plot([15, 50], [15 * lr.coef_ + lr.intercept_, 50 * lr.coef_ + lr.intercept_]) # 50cm 농어 데이터 predict = lr.predict([[50]]) plt.scatter(50, predict, marker = &#39;^&#39;) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() . [39.01714496] -709.0186449535477 . print(lr.score(train_input, train_target)) print(lr.score(test_input, test_target)) # 테스트세트의 점수가 현저히 낮은 과대적합 # 훈련세트의 점수 또한 높은 편이 아니기에 전체적으로 과소적합 되었다. # 직선을 확인했을 때 X = 15 인 구간에서 Y 값이 0 이하가 나온다. --&gt; 현실에서는 불가능 . 0.9398463339976039 0.8247503123313558 . &#45796;&#54637; &#54924;&#44480; . import matplotlib.pyplot as plt plt.scatter(train_input, train_target) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() # train세트의 산점도는 일직선 이라기 보다는 곡선의 형태에 가깝다 --&gt; 2차방정식의 그래프를 그려보자 --&gt; y = ax^2 + bx + c . # np.column_stack() 을 이용해서 train세트의 제곱과 train세트 2배열을 나란히 붙인다. --&gt; test세트도 마찬가지 train_poly = np.column_stack((train_input ** 2, train_input)) test_poly = np.column_stack((test_input ** 2, test_input)) print(train_poly.shape, test_poly.shape) # 모든 원소 제곱이 하나의 열을 이루었기 때문에 열이 2개로 늘어났다. . (42, 2) (14, 2) . from sklearn.linear_model import LinearRegression lr = LinearRegression() lr.fit(train_poly, train_target) print(lr.predict([[50 ** 2, 50]])) . [1573.98423528] . print(lr.coef_, lr.intercept_) # trian 세트 산점도를 그리고 그 위에 계수와 절편을 이용해 모델링에 사용된 곡선 그리기 x = np.arange(15, 51) # 15cm ~ 50cm길이의 농어 배열 생성 import matplotlib.pyplot as plt plt.scatter(train_input, train_target) plt.plot(x, lr.coef_[0] * x ** 2 + lr.coef_[1] * x + lr.intercept_) plt.scatter(50, lr.predict([[50 ** 2, 50]]), marker = &#39;^&#39;) plt.show() . [ 1.01433211 -21.55792498] 116.05021078278276 . print(lr.score(train_poly, train_target)) print(lr.score(test_poly, test_target)) # 과소적합이 아직 조금은 남아있음 --&gt; 조금 더 복잡한 모델 생성 . 0.9706807451768623 0.9775935108325122 . &#50672;&#49845; . import numpy as np perch_length = np.array([8.4, 13.7, 15.0, 16.2, 17.4, 18.0, 18.7, 19.0, 19.6, 20.0, 21.0, 21.0, 21.0, 21.3, 22.0, 22.0, 22.0, 22.0, 22.0, 22.5, 22.5, 22.7, 23.0, 23.5, 24.0, 24.0, 24.6, 25.0, 25.6, 26.5, 27.3, 27.5, 27.5, 27.5, 28.0, 28.7, 30.0, 32.8, 34.5, 35.0, 36.5, 36.0, 37.0, 37.0, 39.0, 39.0, 39.0, 40.0, 40.0, 40.0, 40.0, 42.0, 43.0, 43.0, 43.5, 44.0]) # 농어의 타겟 = 무게 perch_weight = np.array([5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0, 115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0, 150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0, 218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0, 556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0, 850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0, 1000.0]) # train test split from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(perch_length, perch_weight, random_state = 42) # 2차원 변형 train_input = train_input.reshape(-1,1) test_input = test_input.reshape(-1,1) # 선형회귀 모델링 from sklearn.linear_model import LinearRegression lr = LinearRegression() lr.fit(train_input, train_target) # 점수 확인 --&gt; 과대적합 이면서 전체적인 과소적합 print(lr.score(train_input, train_target)) print(lr.score(test_input, test_target)) # 산점도와 직선 확인 import matplotlib.pyplot as plt plt.scatter(train_input, train_target) plt.plot([15, 50], [15 * lr.coef_ + lr.intercept_, 50 * lr.coef_ + lr.intercept_]) plt.scatter(50, lr.predict([[50]]), marker = &#39;^&#39;) print(plt.show()) # 2차방정식 선형회귀 모델링 # 제곱형태의 train, test세트 준비 train_poly = np.column_stack((train_input ** 2, train_input)) test_poly = np.column_stack((test_input ** 2, test_input)) # 모델링 lr.fit(train_poly, train_target) # 점수확인 --&gt; 점수 많이 좋아짐 print(lr.score(train_poly, train_target)) print(lr.score(test_poly, test_target)) # 산점도와 곡선확인 plt.scatter(train_input, train_target) x = np.arange(15, 51) plt.plot(x, x ** 2 * lr.coef_[0] + x * lr.coef_[1] + lr.intercept_) plt.scatter(50, lr.predict([[50 ** 2, 50]]), marker = &#39;^&#39;) plt.show() . 0.9398463339976039 0.8247503123313558 . None 0.9706807451768623 0.9775935108325122 .",
            "url": "https://parkjeongung.github.io/Ung.github.io/2022/01/11/%EC%84%A0%ED%98%95_%ED%9A%8C%EA%B7%80_LinearRegression.html",
            "relUrl": "/2022/01/11/%EC%84%A0%ED%98%95_%ED%9A%8C%EA%B7%80_LinearRegression.html",
            "date": " • Jan 11, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "로지스틱회귀 LogisticeRegression // 시그모이드 // 소프트맥스",
            "content": ". import pandas as pd fish = pd.read_csv(&#39;https://bit.ly/fish_csv_data&#39;) fish.head() . Species Weight Length Diagonal Height Width . 0 Bream | 242.0 | 25.4 | 30.0 | 11.5200 | 4.0200 | . 1 Bream | 290.0 | 26.3 | 31.2 | 12.4800 | 4.3056 | . 2 Bream | 340.0 | 26.5 | 31.1 | 12.3778 | 4.6961 | . 3 Bream | 363.0 | 29.0 | 33.5 | 12.7300 | 4.4555 | . 4 Bream | 430.0 | 29.0 | 34.0 | 12.4440 | 5.1340 | . print(pd.unique(fish[&#39;Species&#39;])) . [&#39;Bream&#39; &#39;Roach&#39; &#39;Whitefish&#39; &#39;Parkki&#39; &#39;Perch&#39; &#39;Pike&#39; &#39;Smelt&#39;] . # Species 를 제외한 나머지 열이 특성이 된다. fish_input = fish[[&#39;Weight&#39;, &#39;Length&#39;, &#39;Diagonal&#39;, &#39;Height&#39;, &#39;Width&#39;]].to_numpy() fish_target = fish[&#39;Species&#39;].to_numpy() # 넘파이 배열로 잘 바뀌었는지 확인 print(fish_input[: 5]) . [[242. 25.4 30. 11.52 4.02 ] [290. 26.3 31.2 12.48 4.3056] [340. 26.5 31.1 12.3778 4.6961] [363. 29. 33.5 12.73 4.4555] [430. 29. 34. 12.444 5.134 ]] . from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(fish_input, fish_target, random_state = 42) . from sklearn.preprocessing import StandardScaler ss = StandardScaler() ss.fit(train_input) train_scaled = ss.transform(train_input) test_scaled = ss.transform(test_input) . K - &#52572;&#44540;&#51217; &#51060;&#50883; &#48516;&#47448;&#44592; &#51060;&#50857; . from sklearn.neighbors import KNeighborsClassifier kn = KNeighborsClassifier(n_neighbors = 3) kn.fit(train_scaled, train_target) print(kn.score(train_scaled, train_target)) print(kn.score(test_scaled, test_target)) . 0.8907563025210085 0.85 . print(kn.classes_) # 도미 빙어 분류 --&gt; 이진분류 # 럭키백 --&gt; 다중분류 . [&#39;Bream&#39; &#39;Parkki&#39; &#39;Perch&#39; &#39;Pike&#39; &#39;Roach&#39; &#39;Smelt&#39; &#39;Whitefish&#39;] . print(kn.predict(test_scaled[:5])) . [&#39;Perch&#39; &#39;Smelt&#39; &#39;Pike&#39; &#39;Perch&#39; &#39;Perch&#39;] . # np.round --&gt; 반올림 --&gt; decimals 로 소수점 어디서 반올림 할건지 지정 import numpy as np proba = kn.predict_proba(test_scaled[:5]) print(np.round(proba, decimals = 4)) . [[0. 0. 1. 0. 0. 0. 0. ] [0. 0. 0. 0. 0. 1. 0. ] [0. 0. 0. 1. 0. 0. 0. ] [0. 0. 0.6667 0. 0.3333 0. 0. ] [0. 0. 0.6667 0. 0.3333 0. 0. ]] . distances, indexes = kn.kneighbors(test_scaled[3:4]) # kneighbors 메서드의 입력은 항상 2차원 배열이어야 하므로 슬라이싱으로 해야지 오류가 안남 print(train_target[indexes]) . [[&#39;Roach&#39; &#39;Perch&#39; &#39;Perch&#39;]] . &#50672;&#49845; . import pandas as pd fish = pd.read_csv(&#39;https://bit.ly/fish_csv_data&#39;) fish.head() # 특성 확인 print(pd.unique(fish[&#39;Species&#39;])) # 입력 타깃 나누기 fish_input = fish[[&#39;Weight&#39;, &#39;Length&#39;, &#39;Diagonal&#39;, &#39;Height&#39;, &#39;Width&#39;]].to_numpy() fish_target = fish[&#39;Species&#39;].to_numpy() # train test split from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(fish_input, fish_target, random_state = 42) # standard scaler from sklearn.preprocessing import StandardScaler ss = StandardScaler() ss.fit(train_input) train_scaled = ss.transform(train_input) test_scaled = ss.transform(test_input) # 최근접 이웃 from sklearn.neighbors import KNeighborsClassifier kn = KNeighborsClassifier(n_neighbors = 3) kn.fit(train_scaled, train_target) print(kn.score(train_scaled, train_target)) print(kn.score(test_scaled, test_target)) # 사용가능 타깃값 확인 kn.classes_ print(kn.classes_) # 샘플 5개 예측 print(kn.predict(test_scaled[:5])) # 타깃별 확률값 출력 import numpy as np proba = kn.predict_proba(test_scaled[:5]) print(np.round(proba, decimals = 4)) # 4번째 샘플의 사용 이웃확인 distances, indexes = kn.kneighbors(test_scaled[3: 4]) print(train_target[indexes]) . [&#39;Bream&#39; &#39;Roach&#39; &#39;Whitefish&#39; &#39;Parkki&#39; &#39;Perch&#39; &#39;Pike&#39; &#39;Smelt&#39;] 0.8907563025210085 0.85 [&#39;Bream&#39; &#39;Parkki&#39; &#39;Perch&#39; &#39;Pike&#39; &#39;Roach&#39; &#39;Smelt&#39; &#39;Whitefish&#39;] [&#39;Perch&#39; &#39;Smelt&#39; &#39;Pike&#39; &#39;Perch&#39; &#39;Perch&#39;] [[0. 0. 1. 0. 0. 0. 0. ] [0. 0. 0. 0. 0. 1. 0. ] [0. 0. 0. 1. 0. 0. 0. ] [0. 0. 0.6667 0. 0.3333 0. 0. ] [0. 0. 0.6667 0. 0.3333 0. 0. ]] [[&#39;Roach&#39; &#39;Perch&#39; &#39;Perch&#39;]] . &#47196;&#51648;&#49828;&#54001; &#54924;&#44480; . 이름은 회귀이지만 분류 모델 | 다중회귀를 한 선형 회귀와 동일하게 선형 방정식을 학습 | 시그모이드 0과 1사이의 값만 출력 --&gt; 이진분류에 사용 | 소프트맥스 0과 1사이의 값을 출력 하는데 총합이 1 --&gt; 다중분류에 사용 | . import numpy as np import matplotlib.pyplot as plt z = np.arange(-5, 5, 0.1) phi = 1 / (1 + np.exp(-z)) # 지수 함수의 계산 --&gt; np.exp() plt.plot(z, phi) plt.xlabel(&#39;z&#39;) plt.ylabel(&#39;phi&#39;) plt.show() # 시그모이드 함수 출력이 0.5 보다 크면 양성, 0.5 이하이면 음성 # 도미와 빙어로 이진분류 수행해보기 . # ex --&gt; A, C 만 선택하기 char_arr = np.array([&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;, &#39;E&#39;]) print(char_arr[[True, False, True, False, False]]) # 적용 bream_smelt_indexes = (train_target == &#39;Bream&#39;) | (train_target == &#39;Smelt&#39;) train_bream_smelt = train_scaled[bream_smelt_indexes] target_bream_smelt = train_target[bream_smelt_indexes] . [&#39;A&#39; &#39;C&#39;] . from sklearn.linear_model import LogisticRegression lr = LogisticRegression() lr.fit(train_bream_smelt, target_bream_smelt) . LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=100, multi_class=&#39;auto&#39;, n_jobs=None, penalty=&#39;l2&#39;, random_state=None, solver=&#39;lbfgs&#39;, tol=0.0001, verbose=0, warm_start=False) . lr.predict(train_bream_smelt[:5]) . array([&#39;Bream&#39;, &#39;Smelt&#39;, &#39;Bream&#39;, &#39;Bream&#39;, &#39;Bream&#39;], dtype=object) . print(lr.predict_proba(train_bream_smelt[:5])) # 첫번째 열이 음성클래스 두번째 열이 양성클래스 # 어떤 것이 음성인지 확인 print(lr.classes_) # bream 이 음성 smelt 가 양성 . [[0.99759855 0.00240145] [0.02735183 0.97264817] [0.99486072 0.00513928] [0.98584202 0.01415798] [0.99767269 0.00232731]] [&#39;Bream&#39; &#39;Smelt&#39;] . decisions = lr.decision_function(train_bream_smelt[: 5]) decisions . array([-6.02927744, 3.57123907, -5.26568906, -4.24321775, -6.0607117 ]) . # 시그모이드 함수 --&gt; expit() from scipy.special import expit print(expit(decisions)) # predict_proba 의 두번째 열과 동일하다 --&gt; decision_function 은 양성 클래스에 대한 Z값을 반환한다. # predict_proba 는 음성과 양성 클래스에 대한 확률을 둘다 반환한다. . [0.00240145 0.97264817 0.00513928 0.01415798 0.00232731] . from sklearn.linear_model import LogisticRegression lr = LogisticRegression(C = 20, max_iter = 1000) # C 는 라쏘 릿지의 alpha 값 같은 느낌 --&gt; 규제의 정도를 조절 --&gt; alpha 와는 달리 값이 작을수록 규제의 강도가 큼 lr.fit(train_scaled, train_target) print(lr.score(train_scaled, train_target)) print(lr.score(test_scaled, test_target)) . 0.9327731092436975 0.925 . print(lr.predict(test_scaled[: 5])) . [&#39;Perch&#39; &#39;Smelt&#39; &#39;Pike&#39; &#39;Roach&#39; &#39;Perch&#39;] . import numpy as np proba = lr.predict_proba(test_scaled[: 5]) print(np.round(proba, decimals = 3)) . [[0. 0.014 0.841 0. 0.136 0.007 0.003] [0. 0.003 0.044 0. 0.007 0.946 0. ] [0. 0. 0.034 0.935 0.015 0.016 0. ] [0.011 0.034 0.306 0.007 0.567 0. 0.076] [0. 0. 0.904 0.002 0.089 0.002 0.001]] . print(lr.classes_) . [&#39;Bream&#39; &#39;Parkki&#39; &#39;Perch&#39; &#39;Pike&#39; &#39;Roach&#39; &#39;Smelt&#39; &#39;Whitefish&#39;] . decision = lr.decision_function(test_scaled[: 5]) print(np.round(decision, decimals = 2)) from scipy.special import softmax proba = softmax(decision, axis = 1) print(np.round(proba, decimals = 3)) . [[ -6.5 1.03 5.16 -2.73 3.34 0.33 -0.63] [-10.86 1.93 4.77 -2.4 2.98 7.84 -4.26] [ -4.34 -6.23 3.17 6.49 2.36 2.42 -3.87] [ -0.68 0.45 2.65 -1.19 3.26 -5.75 1.26] [ -6.4 -1.99 5.82 -0.11 3.5 -0.11 -0.71]] [[0. 0.014 0.841 0. 0.136 0.007 0.003] [0. 0.003 0.044 0. 0.007 0.946 0. ] [0. 0. 0.034 0.935 0.015 0.016 0. ] [0.011 0.034 0.306 0.007 0.567 0. 0.076] [0. 0. 0.904 0.002 0.089 0.002 0.001]] . &#50672;&#49845; . import pandas as pd fish = pd.read_csv(&#39;https://bit.ly/fish_csv_data&#39;) fish.head() # 넘파이 전환 fish_input = fish[[&#39;Weight&#39;, &#39;Length&#39;, &#39;Diagonal&#39;, &#39;Height&#39;, &#39;Width&#39;]].to_numpy() fish_target = fish[&#39;Species&#39;].to_numpy() # train test split from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(fish_input, fish_target, random_state = 42) # 스케일링 from sklearn.preprocessing import StandardScaler ss = StandardScaler() ss.fit(train_input) train_scaled = ss.transform(train_input) test_scaled = ss.transform(test_input) # 이진분류 도미 빙어 bream_smelt_indexes = (train_target == &#39;Bream&#39;) | (train_target == &#39;Smelt&#39;) bream_smelt_train = train_scaled[bream_smelt_indexes] bream_smelt_target = train_target[bream_smelt_indexes] # 모델 훈련 from sklearn.linear_model import LogisticRegression lr = LogisticRegression() lr.fit(bream_smelt_train, bream_smelt_target) # 샘플 5개 예측 lr.predict(bream_smelt_train[: 5]) # 샘플 5개 확률 print(lr.predict_proba(bream_smelt_train[: 5])) # Z값 구하기 decision = lr.decision_function(bream_smelt_train[: 5]) print(decision) # 시그모이드 대입 from scipy.special import expit print(expit(decision)) # 다중분류 lr.fit(train_scaled, train_target) # 샘플 5개 예측 print(lr.predict(test_scaled[: 5])) # 샘플 5개 확률 proba = lr.predict_proba(test_scaled[: 5]) import numpy as np print(np.round(proba, decimals=3)) # Z값 구하기 decision = lr.decision_function(test_scaled[: 5]) # 소프트맥스 대입 from scipy.special import softmax softmax = softmax(decision, axis = 1) print(np.round(softmax, decimals=3)) .",
            "url": "https://parkjeongung.github.io/Ung.github.io/2022/01/11/%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1%ED%9A%8C%EA%B7%80_LogisticeRegression_%EC%8B%9C%EA%B7%B8%EB%AA%A8%EC%9D%B4%EB%93%9C_%EC%86%8C%ED%94%84%ED%8A%B8%EB%A7%A5%EC%8A%A4.html",
            "relUrl": "/2022/01/11/%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1%ED%9A%8C%EA%B7%80_LogisticeRegression_%EC%8B%9C%EA%B7%B8%EB%AA%A8%EC%9D%B4%EB%93%9C_%EC%86%8C%ED%94%84%ED%8A%B8%EB%A7%A5%EC%8A%A4.html",
            "date": " • Jan 11, 2022"
        }
        
    
  
    
        ,"post10": {
            "title": "데이터 전처리",
            "content": ". fish_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0, 9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0] fish_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0, 6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9] . &#45336;&#54028;&#51060;&#47196; input &#45936;&#51060;&#53552;&#50752; target &#45936;&#51060;&#53552; &#47564;&#46308;&#44592; . import numpy as np fish_data = np.column_stack((fish_length, fish_weight)) # target 데이터 생성 --&gt; np.ones(), np.zeros() # 두개의 배열을 연결한다 --&gt; np.concatenate(()) fish_target = np.concatenate((np.ones(35), np.zeros(14))) . &#49324;&#51060;&#53431;&#47088;&#51004;&#47196; &#54984;&#47144;&#49464;&#53944;&#50752; &#53580;&#49828;&#53944;&#49464;&#53944; &#45208;&#45572;&#44592; . from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(fish_data, fish_target, random_state = 42) # 잘 나누어졌는지 shpae로 데이터크기 출력 print(train_input.shape, test_input.shape) print(train_target.shape, test_target.shape) # 전체 데이터의 도미와 빙어의 비율 = 35 (도미) : 14 (빙어) / 2.5 : 1 # 테스트 세트의 도미와 빙어의 비율 = 10 (도미) : 3 (빙어) / 3.3 : 1 # 샘플링 편향 발생 print(test_target) . (36, 2) (13, 2) (36,) (13,) [1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.] . # stratify 매개변수에 target데이터를 전달하면 클래스 비율에 맞게 데이터를 나눈다. from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(fish_data, fish_target, stratify = fish_target, random_state = 42) # 전체 데이터의 도미와 빙어의 비율 = 35 (도미) : 14 (빙어) / 2.5 : 1 # 테스트 세트의 도미와 빙어의 비율 = 9 (도미) : 4 (빙어) / 2.25 : 1 # 데이터가 작아 비율을 동일하게 맞출 수 없지만 꽤 올바른 샘플링 print(test_target) . [0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1.] . &#49688;&#49345;&#54620; &#46020;&#48120; &#54620; &#47560;&#47532; . [[25, 150]] 이라는 도미 데이터를 예측값으로 넣었더니 빙어로 출력이 된다. | 산점도에 있어서 직관적으로는 [[25, 150]] 데이터가 도미에 가까워 보이지만 x축의 범위와 y축의 범위를 생각하면 [[25, 150]] 데이터가 빙어데이터에 가까운것이 합리적이다. | 두 특성의 스케일이 다르면 알고리즘이 올바른 예측을 하지 못한다. --&gt; 데이터 전처리를 통해 특성값을 일정한 기준으로 맞춰준다. | . from sklearn.neighbors import KNeighborsClassifier kn = KNeighborsClassifier() kn.fit(train_input, train_target) kn.score(test_input, test_target) # [[25, 150]] 이라는 도미 데이터를 예측값으로 넣었더니 빙어[0]로 출력이 된다. print(kn.predict([[25, 150]])) . [0.] . import matplotlib.pyplot as plt plt.scatter(train_input[:, 0], train_input[:, 1]) plt.scatter(25, 150, marker = &#39;^&#39;) # 새로운 데이터를 marker = &#39;^&#39;로 지정하여 삼각형으로 표현함. --&gt; 구분하기 쉬움 plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() . distances, indexes = kn.kneighbors([[25, 150]]) # 주어진 샘플의 이웃 샘플을 따로 구분해서 산점도를 그린다. plt.scatter(train_input[ : , 0], train_input[ : , 1]) plt.scatter(25, 150 , marker = &#39;^&#39;) plt.scatter(train_input[indexes, 0], train_input[indexes, 1], marker = &#39;D&#39;) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() print(train_input[indexes]) print(train_target[indexes]) print(distances) print(indexes) # [[25, 150]]의 샘플은 도미 데이터를 1개밖에 포함하지 않는다. . [[[ 37. 1000. ] [ 41. 975. ] [ 38.5 955. ] [ 39.5 925. ] [ 36. 850. ]]] [[1. 1. 1. 1. 1.]] [[150.24524979 150.25805338 150.35871414 150.43446443 150.71926768]] [[10 35 17 4 7]] . # x축의 길이와 y축의 길이를 같게 해서 산점도를 그려보자. --&gt; xlim() = x축 범위 지정 , ylim() = y축 범위 지정 plt.scatter(train_input[ : , 0], train_input[ : , 1]) plt.scatter(25, 150 , marker = &#39;^&#39;) plt.scatter(train_input[indexes, 0], train_input[indexes, 1], marker = &#39;D&#39;) plt.xlim((0, 1000)) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() # weight 와 length 두 특성의 값이 놓인 범위가 매우 다르다. --&gt; 두 특성의 스케일이 다르다. # 두 특성의 스케일이 다르면 알고리즘이 올바르게 예측할 수 없다. --&gt; 특성값을 일정한 기준으로 맞춰주어야 한다. --&gt; 데이터 전처리를 한다. . &#54364;&#51456;&#51216;&#49688;(standard score) --&gt; &#45936;&#51060;&#53552; &#51204;&#52376;&#47532; . 각 특성값이 평균에서 표준편차의 몇 배만큼 떨어져 있는지를 나타낸다. | 평균을 빼고 표준편차로 나눈다. | 특성마다 값의 스케일이 다르므로 평균과 표준편차는 각 특성별로 계산해야 한다. | 테스트 세트를 스케일할때에도 훈련 세트의 평균과 표준편차를 이용하여 스케일 해야 한다. | . # 평균을 구하는 넘파이 함수 --&gt; np.mean() mean = np.mean(train_input, axis = 0) # 표준편차를 구하는 넘파이 함수 --&gt; np.std() std = np.std(train_input, axis = 0) mean, std . (array([ 27.29722222, 454.09722222]), array([ 9.98244253, 323.29893931])) . train_scaled = (train_input - mean) / std . plt.scatter(train_scaled[ :, 0], train_scaled[ :, 1]) plt.scatter(25, 150, marker = &#39;^&#39;) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() # 새로운 데이터는 전처리가 안돼있어서 따로 멀리 떨어져나옴 --&gt; 새로운 데이터도 전처리 . new = ([25, 150] - mean) / std plt.scatter(train_scaled[ :, 0], train_scaled[ :, 1]) plt.scatter(new[0], new[1], marker = &#39;^&#39;) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() #새로운 데이터도 올바르게 표현됨 . kn.fit(train_scaled, train_target) . KNeighborsClassifier(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;, metric_params=None, n_jobs=None, n_neighbors=5, p=2, weights=&#39;uniform&#39;) . test_scaled = (test_input - mean) / std . kn.score(test_scaled, test_target) . 1.0 . print(kn.predict([new])) # 수상한 도미 한 마리와 가장 근접한 이웃 데이터 확인하기 distances, indexes = kn.kneighbors([new]) plt.scatter(train_scaled[ :, 0], train_scaled[ :, 1]) plt.scatter(new[0], new[1], marker = &#39;^&#39;) plt.scatter(train_scaled[indexes, 0], train_scaled[indexes, 1], marker = &#39;D&#39;) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() . [1.] . &#50672;&#49845; . fish_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0, 9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0] fish_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0, 6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9] # train 세트 test 세트 분류 import numpy as np fish_data = np.column_stack((fish_length, fish_weight)) fish_target = np.concatenate((np.ones(35), np.zeros(14))) from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(fish_data, fish_target, stratify = fish_target, random_state = 42) # 데이터 전처리 mean = np.mean(train_input, axis = 0) std = np.std(train_input, axis = 0) train_scaled = (train_input - mean) / std test_scaled = (test_input - mean) / std # 모델링 from sklearn.neighbors import KNeighborsClassifier kn = KNeighborsClassifier() kn.fit(train_scaled, train_target) kn.score(test_scaled, test_target) # 수상한 도미 한 마리 --&gt; new = [25,150] new = [25, 150] scaled_new = ([25, 150] - mean) / std distances, indexes = kn.kneighbors([scaled_new]) import matplotlib.pyplot as plt plt.scatter(train_scaled[ :, 0], train_scaled[ :, 1]) plt.scatter(scaled_new[0], scaled_new[1], marker = &#39;^&#39;) plt.scatter(train_scaled[indexes, 0], train_scaled[indexes, 1]) plt.show() kn.predict([scaled_new]) . array([1.]) .",
            "url": "https://parkjeongung.github.io/Ung.github.io/2022/01/11/%EB%8D%B0%EC%9D%B4%ED%84%B0_%EC%A0%84%EC%B2%98%EB%A6%AC.html",
            "relUrl": "/2022/01/11/%EB%8D%B0%EC%9D%B4%ED%84%B0_%EC%A0%84%EC%B2%98%EB%A6%AC.html",
            "date": " • Jan 11, 2022"
        }
        
    
  
    
        ,"post11": {
            "title": "그레디언트 부스팅 GradientBoostingClassifier // 히스토그램 기반 그레디언트 부스팅 HistGradientBoostingClassifier // XGBoost // LightGBM",
            "content": ". &#44536;&#47112;&#46356;&#50616;&#53944; &#48512;&#49828;&#54021; GradientBoostingClassifier . 이전 트리의 오차를 보완 | 깊이가 얕은 결정트리로 만든 확률적 경사 하강법 | . . from sklearn.ensemble import GradientBoostingClassifier gb = GradientBoostingClassifier(random_state = 42) scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs = -1) print(np.mean(scores[&#39;train_score&#39;]), np.mean(scores[&#39;test_score&#39;])) # 결정트리를 이용하는 알고리즘인데 가지치기를 하지 않아도 과대적합이 발생하지 않음 . 0.8881086892152563 0.8720430147331015 . # 트리 갯수를 5배나 늘렸는데 이정도면 양호 gb = GradientBoostingClassifier(n_estimators=500, learning_rate=0.2, random_state = 42) # learning_rate 는 학습률 --&gt; 이전 트리의 오차를 얼마나 보완할 것인지 --&gt; defalt = 0.1 scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs = -1) print(np.mean(scores[&#39;train_score&#39;]), np.mean(scores[&#39;test_score&#39;])) # 랜덤포레스트 보다 좋은 성능을 보여주지만 확률적 경사 하강법처럼 하나하나 내려가기 때문에 속도는 느림 . 0.9464595437171814 0.8780082549788999 . . &#55176;&#49828;&#53664;&#44536;&#47016; &#44592;&#48152; &#44536;&#47112;&#46356;&#50616;&#53944; &#48512;&#49828;&#54021; . 기존 그레디언트 부스팅의 속도와 성능을 개선한 모델 | 훈련 데이터를 256 (255 + 1) 개의 구간으로 나눈다 1개의 구간에는 데이터에 존재하는 누락값이 들어간다 --&gt; 누락된 값에 대해 따로 전처리 해줄 필요가 없다. | . from sklearn.experimental import enable_hist_gradient_boosting # 아직 테스트중인 모델 from sklearn.ensemble import HistGradientBoostingClassifier hgb = HistGradientBoostingClassifier(random_state = 42) scores = cross_validate(hgb, train_input, train_target, return_train_score=True, n_jobs = -1) print(np.mean(scores[&#39;train_score&#39;]), np.mean(scores[&#39;test_score&#39;])) . 0.9321723946453317 0.8801241948619236 . # 특성을 하나씩 랜덤하게 섞어가면서 모델의 성능이 변화하는지 관찰한다 from sklearn.inspection import permutation_importance hgb.fit(train_input, train_target) result = permutation_importance(hgb, train_input, train_target, n_repeats = 10, random_state = 42, n_jobs = -1) print(result.importances_mean) # 2번째 특성에 과도하게 집중됨 . [0.08876275 0.23438522 0.08027708] . result = permutation_importance(hgb, test_input, test_target, n_repeats = 10, random_state = 42, n_jobs = -1) print(result.importances_mean) . [0.05969231 0.20238462 0.049 ] . XGBoost . 사이킷런 라이브러리 말고 다른 라이브러리를 이용해보자 | 균형잡힌 트리를 만든다 | . . from xgboost import XGBClassifier xgb = XGBClassifier(tree_method = &#39;hist&#39;, random_state = 42) # tree_method = &#39;hist&#39; 로 하면 히스토그램 기반 그레디언트 부스팅을 사용하는 것 scores = cross_validate(xgb, train_input, train_target, return_train_score=True, n_jobs = -1) print(np.mean(scores[&#39;train_score&#39;]), np.mean(scores[&#39;test_score&#39;])) . 0.8824322471423747 0.8726214185237284 . LightGBM . 리프노드를 향해서 달려간다 | 균형을 맞춰야하는 XGBoost 보다 속도가 빠르다 | . . from lightgbm import LGBMClassifier lgb = LGBMClassifier(random_state = 42) scores = cross_validate(lgb, train_input, train_target, return_train_score=True, n_jobs = -1) print(np.mean(scores[&#39;train_score&#39;]), np.mean(scores[&#39;test_score&#39;])) . 0.9338079582727165 0.8789710890649293 .",
            "url": "https://parkjeongung.github.io/Ung.github.io/2022/01/11/%EA%B7%B8%EB%A0%88%EB%94%94%EC%96%B8%ED%8A%B8_%EB%B6%80%EC%8A%A4%ED%8C%85_GradientBoostingClassifier_%ED%9E%88%EC%8A%A4%ED%86%A0%EA%B7%B8%EB%9E%A8_%EA%B8%B0%EB%B0%98_%EA%B7%B8%EB%A0%88%EB%94%94%EC%96%B8%ED%8A%B8_%EB%B6%80%EC%8A%A4%ED%8C%85_HistGradientBoostingClassifier_XGBoost_LightGBM.html",
            "relUrl": "/2022/01/11/%EA%B7%B8%EB%A0%88%EB%94%94%EC%96%B8%ED%8A%B8_%EB%B6%80%EC%8A%A4%ED%8C%85_GradientBoostingClassifier_%ED%9E%88%EC%8A%A4%ED%86%A0%EA%B7%B8%EB%9E%A8_%EA%B8%B0%EB%B0%98_%EA%B7%B8%EB%A0%88%EB%94%94%EC%96%B8%ED%8A%B8_%EB%B6%80%EC%8A%A4%ED%8C%85_HistGradientBoostingClassifier_XGBoost_LightGBM.html",
            "date": " • Jan 11, 2022"
        }
        
    
  
    
        ,"post12": {
            "title": "교차 검증 cross validation //  그리드 서치 GridSearchCV // 랜덤서치 RandomizedSearchCV",
            "content": ". &#44160;&#51613; &#49464;&#53944; . 훈련 세트를 한번 더 나눠서 테스트 세트 이외의 새로운 데이터 세트를 만든다. | 만들어진 검증세트는 모델의 가장 좋은 매개변수를 찾는데 사용된다. | . import pandas as pd wine = pd.read_csv(&#39;https://bit.ly/wine_csv_data&#39;) . wine_input = wine[[&#39;alcohol&#39;, &#39;sugar&#39;, &#39;pH&#39;]].to_numpy() wine_target = wine[&#39;class&#39;].to_numpy() . from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(wine_input, wine_target, test_size = 0.2, random_state = 42) . sub_input, val_input, sub_target, val_target = train_test_split(train_input, train_target, test_size = 0.2, random_state = 42) . print(train_input.shape) print(sub_input.shape, val_input.shape) . (5197, 3) (4157, 3) (1040, 3) . from sklearn.tree import DecisionTreeClassifier dt = DecisionTreeClassifier(random_state = 42) dt.fit(sub_input, sub_target) print(dt.score(sub_input, sub_target)) print(dt.score(val_input, val_target)) # 훈련 세트에 과대적합된 모델 --&gt; 좋은 하이퍼 파라미터를 찾아야함 . 0.9971133028626413 0.864423076923077 . &#44368;&#52264; &#44160;&#51613; // cross validation --&gt; &#44160;&#51613; &#49464;&#53944;&#47484; &#46524;&#50612; &#45236;&#50612; &#54217;&#44032;&#54616;&#45716; &#44284;&#51221;&#51012; &#50668;&#47084; &#48264; &#48152;&#48373;&#54620;&#45796;. . 검증 세트를 만들었기에 훈련 세트의 데이터 수가 줄어듦 | 교차검증을 이용하면 안정적인 검증점수와 더 많은 데이터를 훈련에 사용할 수 있다. | . from sklearn.model_selection import cross_validate scores = cross_validate(dt, train_input, train_target) print(scores) # fit_time = 모델 훈련하는데 걸린 시간 # score_time = 모델 검증하는데 걸린 시간 # test_score = 검증 점수 . {&#39;fit_time&#39;: array([0.00829268, 0.00734234, 0.00762033, 0.00749898, 0.00734591]), &#39;score_time&#39;: array([0.00101829, 0.00070906, 0.00075388, 0.00072241, 0.00079203]), &#39;test_score&#39;: array([0.86923077, 0.84615385, 0.87680462, 0.84889317, 0.83541867])} . import numpy as np print(np.mean(scores[&#39;test_score&#39;])) . 0.855300214703487 . # 분류 모델일 경우 StratifiedKFold() # 분류 모델인 DecisionTreeClassifier 는 StratifiedKFold() 사용 from sklearn.model_selection import StratifiedKFold scores = cross_validate(dt, train_input, train_target, cv = StratifiedKFold()) print(scores) . {&#39;fit_time&#39;: array([0.01132774, 0.00710869, 0.00750995, 0.00733685, 0.00716186]), &#39;score_time&#39;: array([0.00084209, 0.00075936, 0.00071144, 0.00073504, 0.00069094]), &#39;test_score&#39;: array([0.86923077, 0.84615385, 0.87680462, 0.84889317, 0.83541867])} . splitter = StratifiedKFold(n_splits = 10, shuffle = True, random_state = 42) # n_splits --&gt; 몇폴드 교차검증을 할지 결정 --&gt; 기본은 5폴드 scores = cross_validate(dt, train_input, train_target, cv = splitter) print(np.mean(scores[&#39;test_score&#39;])) . 0.8574181117533719 . &#50672;&#49845; . import pandas as pd wine = pd.read_csv(&#39;https://bit.ly/wine_csv_data&#39;) wine.head() wine_input = wine[[&#39;alcohol&#39;, &#39;sugar&#39;, &#39;pH&#39;]].to_numpy() wine_target = wine[&#39;class&#39;].to_numpy() from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(wine_input, wine_target, random_state = 42, test_size = 0.2) sub_input, val_input, sub_target, val_target = train_test_split(train_input, train_target, random_state = 42, test_size = 0.2) from sklearn.tree import DecisionTreeClassifier dt = DecisionTreeClassifier(random_state = 42) dt.fit(sub_input, sub_target) print(dt.score(sub_input, sub_target)) print(dt.score(val_input, val_target)) from sklearn.model_selection import cross_validate score = cross_validate(dt, train_input, train_target) print(score) import numpy as np print(np.mean(score[&#39;test_score&#39;])) from sklearn.model_selection import StratifiedKFold score = cross_validate(dt, train_input, train_target , cv = StratifiedKFold()) splitter = StratifiedKFold(n_splits=10, shuffle = True, random_state = 42) score = cross_validate(dt, train_input, train_target, cv = splitter) print(score) print(np.mean(score[&#39;test_score&#39;])) . 0.9971133028626413 0.864423076923077 {&#39;fit_time&#39;: array([0.00733209, 0.00694227, 0.00748754, 0.00725961, 0.00699782]), &#39;score_time&#39;: array([0.0006938 , 0.00061035, 0.00068307, 0.00061774, 0.00060177]), &#39;test_score&#39;: array([0.86923077, 0.84615385, 0.87680462, 0.84889317, 0.83541867])} 0.855300214703487 {&#39;fit_time&#39;: array([0.00808716, 0.00804591, 0.00808191, 0.00787067, 0.00802708, 0.00803208, 0.00794387, 0.00791359, 0.00801635, 0.00786281]), &#39;score_time&#39;: array([0.00054526, 0.00052047, 0.00048876, 0.00051832, 0.00057459, 0.00050998, 0.00052834, 0.00052619, 0.00053453, 0.00055051]), &#39;test_score&#39;: array([0.83461538, 0.87884615, 0.85384615, 0.85384615, 0.84615385, 0.87307692, 0.85961538, 0.85549133, 0.85163776, 0.86705202])} 0.8574181117533719 . &#44536;&#47532;&#46300; &#49436;&#52824; . 파라미터를 바꿔가면서 교차 검증을 수행한다 | 최적의 하이퍼 파라미터를 찾는다 | . # 0.0001 ~ 0.0005 까지 0.0001 씩 증가하는 5개의 값을 시도 from sklearn.model_selection import GridSearchCV # 파라미터를 딕셔너리 형태로 저장 params = {&#39;min_impurity_decrease&#39; : [0.0001, 0.0002, 0.0003, 0.0004, 0.0005]} . gs = GridSearchCV(DecisionTreeClassifier(random_state=42), params, n_jobs=-1) # n_jobs 는 cpu 코어 사용갯수 지정 # 최적의 하이퍼파라미터를 찾았으면 전체 훈련세트로 모델을 다시 만든다 gs.fit(train_input, train_target) . # 가장 높은 점수의 모델은 best_estimator_ 에 저장됨 dt = gs.best_estimator_ print(dt.score(train_input, train_target)) print(dt.score(test_input, test_target)) . 0.9615162593804117 0.8653846153846154 . print(gs.best_params_) . {&#39;min_impurity_decrease&#39;: 0.0001} . print(gs.cv_results_[&#39;mean_test_score&#39;]) . [0.86819297 0.86453617 0.86492226 0.86780891 0.86761605] . params = {&#39;min_impurity_decrease&#39; : np.arange(0.0001, 0.001, 0.0001), &#39;max_depth&#39; : range(5, 20, 1), &#39;min_samples_split&#39; : range(2, 100, 10) } . gs = GridSearchCV(DecisionTreeClassifier(random_state = 42), params, n_jobs = -1) gs.fit(train_input, train_target) . print(gs.best_params_) . {&#39;max_depth&#39;: 14, &#39;min_impurity_decrease&#39;: 0.0004, &#39;min_samples_split&#39;: 12} . print(gs.cv_results_[&#39;mean_test_score&#39;]) print(np.max(gs.cv_results_[&#39;mean_test_score&#39;])) . [0.85780355 0.85799604 0.85799604 ... 0.86126601 0.86165063 0.86357629] 0.8683865773302731 . &#47004;&#45924; &#49436;&#52824; . 그리드 서치로 매개변수를 일일이 바꿔가며 모델을 훈련시켜 최적의 파라미터를 찾아내지 않을 수 있게 됨 | 그리드 서치는 매개변수의 범위를 내가 지정하는 거기 때문에 범위에 대한 근거가 부족함 | 랜덤 서치로 그 문제를 해결 | . from scipy.stats import uniform, randint # uniform 은 실수를 randint 는 정수를 뽑음 # 0 ~ 9 까지의 정수 중 10개를 랜덤 추출 rgen = randint(0, 10) rgen.rvs(10) . array([3, 4, 8, 5, 2, 7, 3, 8, 6, 4]) . np.unique(rgen.rvs(1000), return_counts = True) . (array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([103, 101, 99, 99, 106, 108, 97, 94, 103, 90])) . ugen = uniform(0, 1) ugen.rvs(10) . array([0.50854909, 0.14208736, 0.22524214, 0.2296259 , 0.54808845, 0.49409384, 0.7933543 , 0.07221253, 0.50430007, 0.18970072]) . params = {&#39;min_impurity_decrease&#39; : uniform(0.0001, 0.001), &#39;max_depth&#39; : randint(20, 50), &#39;min_samples_split&#39; : randint(2, 25), &#39;min_samples_leaf&#39; : randint(1, 25)} . from sklearn.model_selection import RandomizedSearchCV rs = RandomizedSearchCV(DecisionTreeClassifier(random_state = 42), params, n_iter = 100, n_jobs = -1, random_state = 42) # n_iter --&gt; 정의된 매개변수 범위에서 몇 번 샘플링 할 것인지 결정 rs.fit(train_input, train_target) . print(rs.best_params_) . {&#39;max_depth&#39;: 39, &#39;min_impurity_decrease&#39;: 0.00034102546602601173, &#39;min_samples_leaf&#39;: 7, &#39;min_samples_split&#39;: 13} . print(rs.cv_results_[&#39;mean_test_score&#39;]) print(rs.cv_results_[&#39;mean_test_score&#39;].shape) print(np.max(rs.cv_results_[&#39;mean_test_score&#39;])) . [0.86511513 0.86261235 0.86838528 0.86588547 0.86376731 0.86434497 0.86280503 0.86280484 0.86357592 0.86357555 0.86280503 0.8626142 0.86472977 0.86954283 0.86203543 0.86761827 0.86222884 0.86473033 0.86877082 0.86184423 0.86126657 0.86511494 0.8626142 0.86203543 0.86511476 0.86607722 0.86222773 0.86684682 0.86261309 0.86338436 0.8629977 0.86242171 0.86184478 0.86165211 0.86049808 0.86530706 0.86280521 0.86684775 0.86203524 0.86318983 0.86780947 0.86761624 0.86126694 0.86934867 0.86857889 0.86530743 0.86434497 0.86415303 0.86838602 0.86530688 0.86145813 0.86684626 0.8618446 0.86145961 0.86338454 0.86569131 0.86242152 0.86376805 0.86203543 0.86376916 0.86511457 0.86184275 0.86338454 0.86242004 0.86107481 0.86203654 0.86184478 0.86434552 0.86184478 0.86338473 0.86299993 0.8641534 0.86338269 0.85972662 0.86415303 0.86665433 0.86261253 0.86222884 0.86858111 0.86472903 0.86242097 0.86261457 0.86742448 0.86434497 0.86684682 0.86184423 0.86107481 0.86877193 0.86338362 0.86242171 0.8674243 0.86222773 0.86242004 0.86415414 0.86376805 0.86261272 0.86184478 0.86357518 0.86203635 0.86203691] (100,) 0.8695428296438884 . dt = rs.best_estimator_ dt.fit(train_input, train_target) print(dt.score(test_input, test_target)) . 0.86 .",
            "url": "https://parkjeongung.github.io/Ung.github.io/2022/01/11/%EA%B5%90%EC%B0%A8_%EA%B2%80%EC%A6%9D_cross_validation_%EA%B7%B8%EB%A6%AC%EB%93%9C_%EC%84%9C%EC%B9%98_GridSearchCV_%EB%9E%9C%EB%8D%A4%EC%84%9C%EC%B9%98_RandomizedSearchCV.html",
            "relUrl": "/2022/01/11/%EA%B5%90%EC%B0%A8_%EA%B2%80%EC%A6%9D_cross_validation_%EA%B7%B8%EB%A6%AC%EB%93%9C_%EC%84%9C%EC%B9%98_GridSearchCV_%EB%9E%9C%EB%8D%A4%EC%84%9C%EC%B9%98_RandomizedSearchCV.html",
            "date": " • Jan 11, 2022"
        }
        
    
  
    
        ,"post13": {
            "title": "결정 트리 // DecisionTreeClassifier",
            "content": ". import pandas as pd wine = pd.read_csv(&#39;https://bit.ly/wine_csv_data&#39;) wine.head() . alcohol sugar pH class . 0 9.4 | 1.9 | 3.51 | 0.0 | . 1 9.8 | 2.6 | 3.20 | 0.0 | . 2 9.8 | 2.3 | 3.26 | 0.0 | . 3 9.8 | 1.9 | 3.16 | 0.0 | . 4 9.4 | 1.9 | 3.51 | 0.0 | . wine.info() # 6497개의 샘플, 4개의 열은 모두 float형, Non-Null Count가 모두 6497 이므로 누락된 값 없음 # 누락된 값이 확인되면 그 데이터 행을 버리거나 평균값으로 채운 후 사용 --&gt; 어떤 방식이 최선인지는 알기 어려우므로 둘다 시도해보기 . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 6497 entries, 0 to 6496 Data columns (total 4 columns): # Column Non-Null Count Dtype -- -- 0 alcohol 6497 non-null float64 1 sugar 6497 non-null float64 2 pH 6497 non-null float64 3 class 6497 non-null float64 dtypes: float64(4) memory usage: 203.2 KB . wine.describe() # 각 열의 스케일이 다름 --&gt; 스케일링 . alcohol sugar pH class . count 6497.000000 | 6497.000000 | 6497.000000 | 6497.000000 | . mean 10.491801 | 5.443235 | 3.218501 | 0.753886 | . std 1.192712 | 4.757804 | 0.160787 | 0.430779 | . min 8.000000 | 0.600000 | 2.720000 | 0.000000 | . 25% 9.500000 | 1.800000 | 3.110000 | 1.000000 | . 50% 10.300000 | 3.000000 | 3.210000 | 1.000000 | . 75% 11.300000 | 8.100000 | 3.320000 | 1.000000 | . max 14.900000 | 65.800000 | 4.010000 | 1.000000 | . wine_input = wine[[&#39;alcohol&#39;, &#39;sugar&#39;, &#39;pH&#39;]].to_numpy() wine_target = wine[&#39;class&#39;].to_numpy() . from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(wine_input, wine_target, test_size = 0.2, random_state = 42) # test_size 를 따로 지정하지 않으면 전체 데이터의 25% 를 테스트 세트로 지정. 샘플 갯수가 충분하므로 &quot;test_size = 0.2&quot; 로 20% 만 테스트 세트로 지정하겠다는 뜻 print(train_input.shape, test_input.shape) . (5197, 3) (1300, 3) . from sklearn.preprocessing import StandardScaler ss = StandardScaler() ss.fit(train_input) train_scaled = ss.transform(train_input) test_scaled = ss.transform(test_input) . from sklearn.linear_model import LogisticRegression lr = LogisticRegression() lr.fit(train_scaled, train_target) print(lr.score(train_scaled, train_target)) print(lr.score(test_scaled, test_target)) . 0.7808350971714451 0.7776923076923077 . from sklearn.tree import DecisionTreeClassifier dt = DecisionTreeClassifier(random_state = 42) dt.fit(train_scaled, train_target) print(dt.score(train_scaled, train_target)) print(dt.score(test_scaled, test_target)) . 0.996921300750433 0.8592307692307692 . import matplotlib.pyplot as plt from sklearn.tree import plot_tree plt.figure(figsize = (16,7)) plot_tree(dt) plt.show() # 가장 위에있는 걸 루트노드 # 가장 밑에있는 걸 리프노드 # 그림만 봐도 현기증 날라함 . # 클래스 별로 색칠하기 --&gt; filled # 특성이름 전달 --&gt; feature_names plt.figure(figsize=(10, 7)) # 사이즈 조절 plot_tree(dt, max_depth = 1, filled = True, feature_names = [&#39;alcohol&#39;, &#39;sugar&#39;, &#39;pH&#39;]) plt.show() # 1. 테스트 조건 # 2. 불순도 # 3. 총 샘플 수 # 4. 클래스별 샘플 수 # 1. 테스트 조건을 만족하면 왼쪽, 아니면 오른쪽 # 2. 마지막 리프 노드에 도착했을 때 음성 클래스 샘플 수가 많으면 음성 클래스로 예측, 양성 클래스 샘플 수가 많으면 양성 클래스로 예측 # 정보이득이 최대가 되도록 노드를 분할함 --&gt; 불순도를 이용한 공식이 따로 있지만 결론은 한 클래스 쪽으로 샘플 수가 치우치게 노드를 분할함 . # 끝까지 자라나는 tree는 훈련 세트에 과대적합 된다. dt = DecisionTreeClassifier(max_depth = 3, random_state = 42) dt.fit(train_scaled, train_target) print(dt.score(train_scaled, train_target)) print(dt.score(test_scaled, test_target)) . 0.8454877814123533 0.8415384615384616 . plt.figure(figsize = (20, 15)) plot_tree(dt, filled = True, feature_names = [&#39;alcohol&#39;, &#39;sugar&#39;, &#39;pH&#39;]) plt.show() . dt = DecisionTreeClassifier(max_depth = 3, random_state = 42) dt.fit(train_input, train_target) print(dt.score(train_input, train_target)) print(dt.score(test_input, test_target)) . 0.8454877814123533 0.8415384615384616 . print(dt.feature_importances_) . [0.12345626 0.86862934 0.0079144 ] . &#50672;&#49845; . import pandas as pd wine = pd.read_csv(&#39;https://bit.ly/wine_csv_data&#39;) wine.head() wine.info() wine.describe() wine_input = wine[[&#39;alcohol&#39;, &#39;sugar&#39;, &#39;pH&#39;]].to_numpy() wine_target = wine[&#39;class&#39;].to_numpy() from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(wine_input, wine_target, random_state = 42, test_size = 0.2) from sklearn.tree import DecisionTreeClassifier dt = DecisionTreeClassifier(random_state = 42) dt.fit(train_input, train_target) print(dt.score(train_input, train_target)) print(dt.score(test_input, test_target)) from sklearn.tree import plot_tree import matplotlib.pyplot as plt plt.figure(figsize = (10, 7)) plot_tree(dt, max_depth = 1, filled = True, feature_names=[&#39;alcohol&#39;,&#39;sugar&#39;,&#39;pH&#39;]) dt = DecisionTreeClassifier(random_state = 42, max_depth=3) dt.fit(train_input, train_target) print(dt.score(train_input, train_target)) print(dt.score(test_input, test_target)) . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 6497 entries, 0 to 6496 Data columns (total 4 columns): # Column Non-Null Count Dtype -- -- 0 alcohol 6497 non-null float64 1 sugar 6497 non-null float64 2 pH 6497 non-null float64 3 class 6497 non-null float64 dtypes: float64(4) memory usage: 203.2 KB 0.996921300750433 0.8584615384615385 0.8454877814123533 0.8415384615384616 .",
            "url": "https://parkjeongung.github.io/Ung.github.io/2022/01/11/%EA%B2%B0%EC%A0%95_%ED%8A%B8%EB%A6%AC_DecisionTreeClassifier.html",
            "relUrl": "/2022/01/11/%EA%B2%B0%EC%A0%95_%ED%8A%B8%EB%A6%AC_DecisionTreeClassifier.html",
            "date": " • Jan 11, 2022"
        }
        
    
  
    
        ,"post14": {
            "title": "PolynomialFeatures // 릿지 Ridge // 라쏘 Lasso",
            "content": ". import pandas as pd df = pd.read_csv(&#39;https://bit.ly/perch_csv_data&#39;) # 판다스로 준비된 데이터 넘파이 배열로 바꾸기 --&gt; to_numpy() perch_full = df.to_numpy() import numpy as np perch_weight = np.array([5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0, 115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0, 150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0, 218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0, 556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0, 850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0, 1000.0]) . from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(perch_full, perch_weight, random_state = 42) . PolynomialFeatures . 사이킷런은 특성을 만들거나 전처리하기 위한 다양한 클래스를 제공한다. | . from sklearn.preprocessing import PolynomialFeatures poly = PolynomialFeatures() poly.fit([[2, 3]]) print(poly.transform([[2, 3]])) poly = PolynomialFeatures(include_bias = False) # include_bias = False 로 배열에서 1을 제거한다. 사실 굳이 지정하지 않아도 사이킷런 모델은 1을 무시함. 문과라 평면을 안배워서 뭔말인지 잘모르겠으니까 스킵 poly.fit([[2, 3]]) print(poly.transform([[2, 3]])) # PolynomialFeatures 적용 poly = PolynomialFeatures(include_bias = False) poly.fit(train_input) train_poly = poly.transform(train_input) print(train_poly.shape) print(train_input.shape) . [[1. 2. 3. 4. 6. 9.]] [[2. 3. 4. 6. 9.]] (42, 9) (42, 3) . poly.get_feature_names() . [&#39;x0&#39;, &#39;x1&#39;, &#39;x2&#39;, &#39;x0^2&#39;, &#39;x0 x1&#39;, &#39;x0 x2&#39;, &#39;x1^2&#39;, &#39;x1 x2&#39;, &#39;x2^2&#39;] . test_poly = poly.transform(test_input) # 다중 회귀 모델 훈련 from sklearn.linear_model import LinearRegression lr = LinearRegression() lr.fit(train_poly, train_target) print(lr.score(train_poly, train_target)) print(lr.score(test_poly, test_target)) # 여러개의 특성을 사용하니 과소적합도 해결되고 점수도 좋게 나온다. . 0.9903183436982124 0.9714559911594132 . poly = PolynomialFeatures(degree = 5, include_bias = False) poly.fit(train_input) train_poly = poly.transform(train_input) test_poly = poly.transform(test_input) print(train_poly.shape, test_poly.shape) # 특성이 55개 # 추가된 특성으로 모델 훈련 from sklearn.linear_model import LinearRegression lr = LinearRegression() lr.fit(train_poly, train_target) print(lr.score(train_poly, train_target)) # 거의 완벽한 점수 print(lr.score(test_poly, test_target)) # 음수? --&gt; 특성을 너무 많이 늘리면 train 세트에 과대적합 되므로 테스트 세트의 점수가 굉장히 낮게 나온다. . (42, 55) (14, 55) 0.9999999999991096 -144.40579242335605 . &#44508;&#51228; . 모델이 훈련 세트를 너무 과도하게 학습하지 못하도록 하는 것 --&gt; 훈련세트에 과대적합되지 않도록 하는 것 | 선형 회귀 모델의 경우 특성에 곱해지는 계수의 크기를 작게 만드는 일 | . from sklearn.preprocessing import StandardScaler ss = StandardScaler() ss.fit(train_poly) train_scaled = ss.transform(train_poly) test_scaled = ss.transform(test_poly) . &#47551;&#51648; &#46972;&#50136; // &#49440;&#54805; &#54924;&#44480; &#47784;&#45944;&#50640; &#44508;&#51228;&#47484; &#52628;&#44032;&#54620; &#47784;&#45944; . 릿지 회귀 --&gt; 계수를 제곱한 값을 기준으로 규제를 적용 | 라쏘 회귀 --&gt; 계수의 절댓값을 기준으로 규제를 적용 | from sklearn.linear_model import Ridge ridge = Ridge() ridge.fit(train_scaled, train_target) print(ridge.score(train_scaled, train_target)) print(ridge.score(test_scaled, test_target)) # 많은 특성을 사용했는데도 train세트에 너무 과대적합되지 않음 . 0.9896101671037343 0.9790693977615398 . # alpha 와 같이 사람이 직접 알려줘야 하는 파라미터를 &quot;하이퍼파라미터&quot; 라고 한다. # 적절한 alpha 값 찾는 방법 --&gt; alpha 값에 대한 R^2 그래프 그리기 --&gt; train세트와 test세트의 점수가 가장 가까운 지점이 최적의 alpha값 import matplotlib.pyplot as plt # 리스트 만들기 train_score = [] test_score = [] alpha_list = [0.001, 0.01, 0.1, 1, 10, 100] # 반복문으로 alpha 값 돌리기 for alpha in alpha_list: ridge = Ridge(alpha = alpha) ridge.fit(train_scaled, train_target) train_score.append(ridge.score(train_scaled, train_target)) test_score.append(ridge.score(test_scaled, test_target)) # 그래프 그리기 plt.plot(np.log10(alpha_list), train_score) plt.plot(np.log10(alpha_list), test_score) plt.xlabel(&#39;alpha&#39;) plt.ylabel(&#39;R^2&#39;) plt.show() # train 스코어가 가장 높고 test 세트의 점수가 가장 높은 -1 = 10^-1 = 0.1 --&gt; alpha = 0.1로 하여 최종 모델을 훈련해야 가장 좋은 모델 . from sklearn.linear_model import Ridge ridge = Ridge(alpha = 0.1) ridge.fit(train_scaled, train_target) print(ridge.score(train_scaled, train_target)) print(ridge.score(test_scaled, test_target)) . 0.9903815817570366 0.9827976465386927 . from sklearn.linear_model import Lasso lasso = Lasso() lasso.fit(train_scaled, train_target) print(lasso.score(train_scaled, train_target)) print(lasso.score(test_scaled, test_target)) . 0.9897898972080961 0.9800593698421883 . import matplotlib.pyplot as plt from sklearn.linear_model import Lasso train_score = [] test_score = [] alpha_list = [0.001, 0.01, 0.1, 1, 10, 100] for alpha in alpha_list: lasso = Lasso(alpha = alpha) lasso.fit(train_scaled, train_target) train_score.append(lasso.score(train_scaled, train_target)) test_score.append(lasso.score(test_scaled, test_target)) . /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 23364.075969939808, tolerance: 518.2793833333334 positive) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 20251.975097475122, tolerance: 518.2793833333334 positive) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 806.2370926333242, tolerance: 518.2793833333334 positive) . import matplotlib.pyplot as plt from sklearn.linear_model import Lasso train_score = [] test_score = [] alpha_list = [0.001, 0.01, 0.1, 1, 10, 100] for alpha in alpha_list: lasso = Lasso(alpha = alpha, max_iter = 10000) # 라쏘 모델은 최적의 계수를 찾기 위해 반복적인 계산을 수행하는데 지정한 횟수가 부족할 때 이런 경고가 발생 --&gt; max_iter 를 늘려줌으로써 해결 lasso.fit(train_scaled, train_target) train_score.append(lasso.score(train_scaled, train_target)) test_score.append(lasso.score(test_scaled, test_target)) plt.plot(np.log10(alpha_list), train_score) plt.plot(np.log10(alpha_list), test_score) plt.xlabel(&#39;alpha&#39;) plt.ylabel(&#39;R^2&#39;) plt.show() # 최적의 알파값은 1 = 10 --&gt; alpha = 10 으로 하여 모델 훈련 . /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 18778.697957792876, tolerance: 518.2793833333334 positive) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 12972.821345404844, tolerance: 518.2793833333334 positive) . from sklearn.linear_model import Lasso lasso = Lasso(alpha = 10) lasso.fit(train_scaled, train_target) print(lasso.score(train_scaled, train_target)) print(lasso.score(test_scaled, test_target)) . 0.9888067471131867 0.9824470598706695 . # 라쏘 모델의 계수는 coef_ 에 저장되어 있다. print(lasso.coef_) # np.sum 은 배열을 모두 더한 값을 반환한다 # 넘파이 배열에 비교연산자를 사용하면 각 원소는 True = 1 아니면 False = 0 가 된다. print(np.sum(lasso.coef_ == 0)) # 총 55개의 특성 중 40개의 특성의 계수를 0으로 만듬 --&gt; 15개의 특성만 사용 . [ 0. 0. 0. 12.14852453 55.44856399 42.23100799 0. 0. 13.70596191 0. 43.2185952 5.7033775 47.46254536 7.42309425 11.85823365 0. 0. 13.53038193 21.22111356 0. 0. 0. 0. 0. 0. 18.66993032 0. 0. 0. 15.81041778 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 18.14672268 0. 0. 0. 0. 15.51272953 0. 0. 0. 0. 0. ] 40 . &#50672;&#49845; . import pandas as pd df = pd.read_csv(&#39;https://bit.ly/perch_csv_data&#39;) perch_full = df.to_numpy() import numpy as np perch_weight = np.array([5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0, 115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0, 150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0, 218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0, 556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0, 850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0, 1000.0]) from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(perch_full, perch_weight, random_state = 42) from sklearn.preprocessing import PolynomialFeatures poly = PolynomialFeatures(degree = 5, include_bias = False) poly.fit(train_input) train_poly = poly.transform(train_input) test_poly = poly.transform(test_input) from sklearn.linear_model import LinearRegression lr = LinearRegression() lr.fit(train_poly, train_target) print(lr.score(train_poly, train_target)) print(lr.score(test_poly, test_target)) from sklearn.preprocessing import StandardScaler ss = StandardScaler() ss.fit(train_poly) train_scaled = ss.transform(train_poly) test_scaled = ss.transform(test_poly) from sklearn.linear_model import Ridge ridge = Ridge() ridge.fit(train_scaled, train_target) print(ridge.score(train_scaled, train_target)) print(ridge.score(test_scaled, test_target)) train_score = [] test_score = [] import matplotlib.pyplot as plt alpha_list = [0.001, 0.01, 0.1, 1, 10, 100] for alpha in alpha_list: ridge = Ridge(alpha = alpha) ridge.fit(train_scaled, train_target) train_score.append(ridge.score(train_scaled, train_target)) test_score.append(ridge.score(test_scaled, test_target)) plt.plot(np.log10(alpha_list), train_score) plt.plot(np.log10(alpha_list), test_score) plt.xlabel(&#39;alpha&#39;) plt.ylabel(&#39;R^2&#39;) plt.show() from sklearn.linear_model import Lasso lasso = Lasso() lasso.fit(train_scaled, train_target) print(lasso.score(train_scaled, train_target)) print(lasso.score(test_scaled, test_target)) train_score = [] test_score = [] alpha_list = [0.001, 0.01, 0.1, 1, 10, 100] for alpha in alpha_list: lasso = Lasso(alpha = alpha) lasso.fit(train_scaled, train_target) train_score.append(lasso.score(train_scaled, train_target)) test_score.append(lasso.score(test_scaled, test_target)) plt.plot(np.log10(alpha_list), train_score) plt.plot(np.log10(alpha_list), test_score) plt.xlabel(&#39;alpha&#39;) plt.ylabel(&#39;R^2&#39;) plt.show() . 0.9999999999991096 -144.40579242335605 0.9896101671037343 0.9790693977615398 . 0.9897898972080961 0.9800593698421883 . /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 23364.075969939808, tolerance: 518.2793833333334 positive) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 20251.975097475122, tolerance: 518.2793833333334 positive) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 806.2370926333242, tolerance: 518.2793833333334 positive) .",
            "url": "https://parkjeongung.github.io/Ung.github.io/2022/01/11/PolynomialFeatures_%EB%A6%BF%EC%A7%80_Ridge_%EB%9D%BC%EC%8F%98_Lasso.html",
            "relUrl": "/2022/01/11/PolynomialFeatures_%EB%A6%BF%EC%A7%80_Ridge_%EB%9D%BC%EC%8F%98_Lasso.html",
            "date": " • Jan 11, 2022"
        }
        
    
  
    
        ,"post15": {
            "title": "K - 평균 // K - Means",
            "content": ". &#50508;&#44256;&#47532;&#51608; &#51089;&#46041;&#50896;&#47532; . !wget https://bit.ly/fruits_300_data -O fruits_300.npy . --2021-12-12 10:02:39-- https://bit.ly/fruits_300_data Resolving bit.ly (bit.ly)... 67.199.248.10, 67.199.248.11 Connecting to bit.ly (bit.ly)|67.199.248.10|:443... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://github.com/rickiepark/hg-mldl/raw/master/fruits_300.npy [following] --2021-12-12 10:02:39-- https://github.com/rickiepark/hg-mldl/raw/master/fruits_300.npy Resolving github.com (github.com)... 140.82.113.3 Connecting to github.com (github.com)|140.82.113.3|:443... connected. HTTP request sent, awaiting response... 302 Found Location: https://raw.githubusercontent.com/rickiepark/hg-mldl/master/fruits_300.npy [following] --2021-12-12 10:02:40-- https://raw.githubusercontent.com/rickiepark/hg-mldl/master/fruits_300.npy Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 3000128 (2.9M) [application/octet-stream] Saving to: ‘fruits_300.npy’ fruits_300.npy 100%[===================&gt;] 2.86M --.-KB/s in 0.07s 2021-12-12 10:02:40 (43.0 MB/s) - ‘fruits_300.npy’ saved [3000128/3000128] . import numpy as np fruits = np.load(&#39;fruits_300.npy&#39;) # 2차원 배열로 변형 fruits_2d = fruits.reshape(-1, 100 * 100) . from sklearn.cluster import KMeans km = KMeans(n_clusters = 3, random_state = 42) # n_clusters 로 만들어질 클러스터의 갯수 설정 km.fit(fruits_2d) . KMeans(n_clusters=3, random_state=42) . print(km.labels_) . [2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 0 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 0 0 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] . print(np.unique(km.labels_, return_counts = True)) . (array([0, 1, 2], dtype=int32), array([111, 98, 91])) . import matplotlib.pyplot as plt def draw_fruits(arr, ratio = 1): n = len(arr) rows = int(np.ceil(n / 10)) cols = n if rows &lt; 2 else 10 fig, axs = plt.subplots(rows, cols, figsize = (cols * ratio, rows * ratio), squeeze = False) for i in range(rows): for j in range(cols): if i * 10 + j &lt; n: axs[i, j].imshow(arr[i * 10 + j], cmap = &#39;gray_r&#39;) axs[i, j].axis(&#39;off&#39;) plt.show() . draw_fruits(fruits[km.labels_ == 0]) . draw_fruits(fruits[km.labels_ == 1]) . draw_fruits(fruits[km.labels_ == 2]) . &#45824;&#54364;&#51060;&#48120;&#51648; &#52636;&#47141;&#54644;&#48372;&#44592; . # 훈련 데이터가 2차원 배열로 변형된 데이터 였기 때문에 2차원 배열로나옴 print(km.cluster_centers_.shape) # 이미지를 그리기 위해 3차원 배열로 다시 바꿔준다 cluster_centers_ = km.cluster_centers_.reshape(-1, 100, 100) draw_fruits(cluster_centers_, ratio = 3) # 이전에 픽셀값 평균으로 찾았던 대표이미지와 유사하다 . (3, 10000) . print(km.transform(fruits_2d[100: 101])) # 0번째 클러스터 까지의 거리가 가장 가까우므로 이 샘플은 0번째 클러스터에 속해있다. . [[3393.8136117 8837.37750892 5267.70439881]] . print(km.predict(fruits_2d[100: 101])) # 0번째 클러스터에 속해있다. 위의 transform 으로 확인한 결과와 일치한다 . [0] . # 그림을 그려 확인해보자 draw_fruits(fruits[100: 101], ratio = 3) . # 최적의 클러스터 중심을 찾기위해 알고리즘이 반복한 횟수 확인 --&gt; n_iter_ print(km.n_iter_) . 4 . &#52572;&#51201;&#51032; K(n_cluster) &#44050; &#52286;&#44592; . 알고리즘을 훈련할 때 타겟값을 사용하지 않긴 했지만 클러스터를 총 3개(사과, 파인애플, 바나나)를 써야한다는 것을 알고 있었음 --&gt; &quot;n_cluster = 3&quot; 으로 지정 | 실전에서는 클러스터 개수를 알 수 없다 --&gt; 엘보우 방법으로 해결 | . 클러스터 중심과 그 클러스터에 속한 샘플 사이의 거리 제곱 합을 이너셔 라고한다. | 이너셔는 클러스터에 속한 샘플이 얼마나 가깝게 모여있는지를 나타내는 값으로 생각할 수 있다. | 일반적으로 클러스터 갯수가 늘어나면 클러스터 각각의 크기가 줄어들기 때문에 이너셔도 줄어든다 | 클러스터 개수를 늘려가며 이너셔의 변화를 관찰한다. 최적의 클러스터 갯수를 찾는다 | . # 클러스터 갯수를 늘려가면서 이너셔를 확인하는 그래프를 그린다 inertia = [] for k in range(2, 7): km = KMeans(n_clusters = k, random_state = 42) km.fit(fruits_2d) inertia.append(km.inertia_) plt.plot(range(2, 7), inertia) plt.xlabel(&#39;cluster&#39;) plt.ylabel(&#39;inertia&#39;) plt.show() # 그래프의 모양이 팔꿈치 처럼 생겼대서 엘보우래요 ㅋㅋ # K = 3 에서 이너셔의 변화가 줄어들기 때문에 최적의 K 값은 3 이다 .",
            "url": "https://parkjeongung.github.io/Ung.github.io/2022/01/11/K_%ED%8F%89%EA%B7%A0_K_Means.html",
            "relUrl": "/2022/01/11/K_%ED%8F%89%EA%B7%A0_K_Means.html",
            "date": " • Jan 11, 2022"
        }
        
    
  
    
        ,"post16": {
            "title": "K - 최근접 이웃 회귀 // KNeighborsRegressor",
            "content": ". &#54924;&#44480; . 임의의 어떤 숫자를 예측 | 두 변수 사이의 상관관계를 분석하는 방법 | . import numpy as np # 농어의 특성 = 길이 perch_length = np.array([8.4, 13.7, 15.0, 16.2, 17.4, 18.0, 18.7, 19.0, 19.6, 20.0, 21.0, 21.0, 21.0, 21.3, 22.0, 22.0, 22.0, 22.0, 22.0, 22.5, 22.5, 22.7, 23.0, 23.5, 24.0, 24.0, 24.6, 25.0, 25.6, 26.5, 27.3, 27.5, 27.5, 27.5, 28.0, 28.7, 30.0, 32.8, 34.5, 35.0, 36.5, 36.0, 37.0, 37.0, 39.0, 39.0, 39.0, 40.0, 40.0, 40.0, 40.0, 42.0, 43.0, 43.0, 43.5, 44.0]) # 농어의 타겟 = 무게 perch_weight = np.array([5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0, 115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0, 150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0, 218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0, 556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0, 850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0, 1000.0]) . import matplotlib.pyplot as plt plt.scatter(perch_length, perch_weight) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() # 농어의 길이가 길어질수록 무게가 늘어난다. . from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(perch_length, perch_weight, random_state = 42) . # reshape 메서드는 크기에 -1 을 지정하면 남은 원소 개수로 모두 채우라는 의미 train_input = train_input.reshape(-1, 1) # == train_input.reshape(42, 1) print(train_input.shape) test_input = test_input.reshape(-1, 1) print(test_input.shape) . (42, 1) (14, 1) . from sklearn.neighbors import KNeighborsRegressor knr = KNeighborsRegressor() knr.fit(train_input, train_target) print(knr.score(test_input, test_target)) # 스코어가 1이 안나왔다 --&gt; 회귀에서는 정확한 숫자를 맞힌다는 것이 거의 불가능하다. --&gt; 예측하는 값이 모두 임의의 수치이기 때문 . 0.9928094061010639 . from sklearn.metrics import mean_absolute_error # 테스트 세트의 예측값을 만든다. test_prediction = knr.predict(test_input) # 테스트 세트에 대한 절댓값 오차의 평균을 구한다. mae = mean_absolute_error(test_target, test_prediction) mae # 19그람 정도의 오차가 발생한다. . 19.157142857142862 . &#54984;&#47144; &#49464;&#53944;&#47484; &#49324;&#50857;&#54644; &#47784;&#45944; &#54217;&#44032;&#54616;&#44592; // &#44284;&#49548;&#51201;&#54633;, &#44284;&#45824;&#51201;&#54633; . score(train) &gt; score(test) --&gt; 모델이 훈련세트에 과대적합 되었다. --&gt; 실전에 투입하면 예측이 잘 안됨 --&gt; 모델을 덜 복잡하게 만들어 해결한다. --&gt; 이웃의 개수를 늘린다 | score(train) &lt; score(test) --&gt; 모델이 훈련세트에 과소적합 되었다. --&gt; 모델이 너무 단순하다. --&gt; 모델을 더 복잡하게 만들어 해결한다. --&gt; 이웃의 개수를 줄인다. | . print(knr.score(train_input, train_target)) # 테스트 세트로 점수 확인하기 print(knr.score(test_input, test_target)) # score(train) &lt; score(test) --&gt; 과소적합 --&gt; 데이터가 작을경우 과소적합이 발생할 수 있다. . 0.9698823289099255 0.9928094061010639 . knr.n_neighbors = 3 knr.fit(train_input, train_target) print(knr.score(train_input, train_target)) print(knr.score(test_input, test_target)) # 이웃의 개수를 줄였더니 train 세트와 test 세트의 score가 비슷하게 나온다. . 0.9804899950518966 0.974645996398761 . &#50672;&#49845; . import numpy as np perch_length = np.array([8.4, 13.7, 15.0, 16.2, 17.4, 18.0, 18.7, 19.0, 19.6, 20.0, 21.0, 21.0, 21.0, 21.3, 22.0, 22.0, 22.0, 22.0, 22.0, 22.5, 22.5, 22.7, 23.0, 23.5, 24.0, 24.0, 24.6, 25.0, 25.6, 26.5, 27.3, 27.5, 27.5, 27.5, 28.0, 28.7, 30.0, 32.8, 34.5, 35.0, 36.5, 36.0, 37.0, 37.0, 39.0, 39.0, 39.0, 40.0, 40.0, 40.0, 40.0, 42.0, 43.0, 43.0, 43.5, 44.0]) # 농어의 타겟 = 무게 perch_weight = np.array([5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0, 115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0, 150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0, 218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0, 556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0, 850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0, 1000.0]) # train test split from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(perch_length, perch_weight, random_state = 42) # 2차원 배열로 변형 train_input = train_input.reshape(-1, 1) test_input = test_input.reshape(-1, 1) # 모델링 from sklearn.neighbors import KNeighborsRegressor knr = KNeighborsRegressor() knr.fit(train_input, train_target) # 스코어 knr.score(test_input, test_target) # 오차확인 mean absoulte error from sklearn.metrics import mean_absolute_error test_prediction = knr.predict(test_input) mae = mean_absolute_error(test_target, test_prediction) # 과소적합 --&gt; 테스트세트 스코어가 트레인세트 스코어보다 높다 print(knr.score(train_input, train_target)) print(knr.score(test_input, test_target)) # 과소적합 해결 --&gt; 모델을 복잡하게 만든다. --&gt; 이웃수 줄이기 knr.n_neighbors = 3 knr.fit(train_input, train_target) print(knr.score(train_input, train_target)) print(knr.score(test_input, test_target)) . 0.9698823289099255 0.9928094061010639 0.9804899950518966 0.974645996398761 . # n_neighbors = [1, 5, 10] # 농어의 길이를 5에서 45까지 바꿔가며 모델링 결과 확인하기 from sklearn.neighbors import KNeighborsRegressor import matplotlib.pyplot as plt knr = KNeighborsRegressor() x = np.arange(5, 45).reshape(-1, 1) for n in [1, 5, 10]: knr.n_neighbors = n knr.fit(train_input, train_target) predict = knr.predict(x) plt.scatter(train_input, train_target) plt.scatter(x, predict, marker = &#39;^&#39;) plt.title(&quot;n_neighbors = {}&quot;.format(n)) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() .",
            "url": "https://parkjeongung.github.io/Ung.github.io/2022/01/11/K_%EC%B5%9C%EA%B7%BC%EC%A0%91_%EC%9D%B4%EC%9B%83_%ED%9A%8C%EA%B7%80_KNeighborsRegressor.html",
            "relUrl": "/2022/01/11/K_%EC%B5%9C%EA%B7%BC%EC%A0%91_%EC%9D%B4%EC%9B%83_%ED%9A%8C%EA%B7%80_KNeighborsRegressor.html",
            "date": " • Jan 11, 2022"
        }
        
    
  
    
        ,"post17": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://parkjeongung.github.io/Ung.github.io/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://parkjeongung.github.io/Ung.github.io/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://parkjeongung.github.io/Ung.github.io/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}