{
  
    
        "post0": {
            "title": "지도학습 비지도학습 // 훈련세트 테스트세트",
            "content": ". &#54984;&#47144;&#49464;&#53944;&#50752; &#53580;&#49828;&#53944;&#49464;&#53944; . 샘플링편향 발생 . fish_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0, 9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0] fish_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0, 6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9] . fish_data = [[l, w] for l, w in zip(fish_length, fish_weight)] fish_target = [1] * 35 + [0] * 14 . from sklearn.neighbors import KNeighborsClassifier kn = KNeighborsClassifier() . train_input = fish_data[: 35] train_target = fish_target[: 35] test_input = fish_data[35: ] test_target = fish_target[35: ] . # 테스트세트 --&gt; score()함수로 모델 평가 kn = kn.fit(train_input, train_target) kn.score(test_input, test_target) #샘플링 편향 발생 --&gt; 마지막 14개(빙어 특성 14개)를 test_input 으로 넣어놔서 훈련에 사용된 train_input 에는 빙어가 하나도 없음 --&gt; 데이터를 섞든지 골고루 샘플을 뽑아야 함 --&gt; numpy 사용 . 0.0 . NUMPY&#47484; &#51060;&#50857;&#54620; &#49368;&#54540;&#47553; &#54200;&#54693; &#54644;&#44208; . 파이썬의 대표적인 배열 라이브러리 | 고차원의 배열을 쉽게 만들고 조작가능 | . import numpy as np . input_arr = np.array(fish_data) target_arr = np.array(fish_target) . print(input_arr.shape) . (49, 2) . # input 과 target 에서 같은 인덱스는 함께 선택되어야 한다. input의 2번은 train으로 target의 2번은 test로 가면 안된다. # 넘파이의 random 함수들은 실행할 때마다 다른 결과를 만든다. --&gt; random.seed()를 지정하면 항상 일정한 결과를 얻을 수 있다. np.random.seed(42) index = np.arange(49) np.random.shuffle(index) index . array([13, 45, 47, 44, 17, 27, 26, 25, 31, 19, 12, 4, 34, 8, 3, 6, 40, 41, 46, 15, 9, 16, 24, 33, 30, 0, 43, 32, 5, 29, 11, 36, 1, 21, 2, 37, 35, 23, 39, 10, 22, 18, 48, 20, 7, 42, 14, 28, 38]) . print(input_arr[[1,3]]) # input_arr의 2번째와 4번째 원소 출력 # numpy 배열을 인덱스로 전하기 --&gt; 훈련세트 생성 train_input = input_arr[index [ : 35]] train_target = input_arr[index [ : 35]] #랜덤으로 만들어진 index의 첫번째 원소는 13 --&gt; input_arr의 14번째 원소(index = 13)가 train_input의 1번째 원소(index = 0)에 들어감 print(input_arr[13], train_input[0]) # numpy 배열을 인덱스로 전하기 --&gt; 테스트세트 생성 test_input = input_arr[index [35 : ]] test_target = input_arr[index [35 : ]] #랜덤으로 만들어진 index의 35번째 원소는 37 --&gt; input_arr의 38번째 원소(indexe = 37)가 test_input의 1번째 원소(index = 0)에 들어감 print(input_arr[37], test_input[0]) . [[ 26.3 290. ] [ 29. 363. ]] [ 32. 340.] [ 32. 340.] [10.6 7. ] [10.6 7. ] . import matplotlib.pyplot as plt plt.scatter(train_input[ :, 0], train_input[ :, 1]) plt.scatter(test_input[ :, 0], test_input[ :, 1]) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() . &#50672;&#49845; . fish_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0, 9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0] fish_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0, 6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9] fish_data = [[l, w] for l, w in zip(fish_length, fish_weight)] fish_target = [1] * 35 + [0] * 14 import numpy as np input_arr = np.array(fish_data) target_arr = np.array(fish_target) np.random.seed(42) index = np.arange(49) np.random.shuffle(index) train_input = input_arr[index[ : 35]] train_target = target_arr[index [ : 35]] test_input = input_arr[index[35 :]] test_target = target_arr[index[35 :]] from sklearn.neighbors import KNeighborsClassifier kn = KNeighborsClassifier() kn.fit(train_input, train_target) kn.score(test_input, test_target) . 1.0 .",
            "url": "https://parkjeongung.github.io/Ung.github.io/2021/12/26/%EC%A7%80%EB%8F%84%ED%95%99%EC%8A%B5_%EB%B9%84%EC%A7%80%EB%8F%84%ED%95%99%EC%8A%B5_%ED%9B%88%EB%A0%A8%EC%84%B8%ED%8A%B8_%ED%85%8C%EC%8A%A4%ED%8A%B8%EC%84%B8%ED%8A%B8.html",
            "relUrl": "/2021/12/26/%EC%A7%80%EB%8F%84%ED%95%99%EC%8A%B5_%EB%B9%84%EC%A7%80%EB%8F%84%ED%95%99%EC%8A%B5_%ED%9B%88%EB%A0%A8%EC%84%B8%ED%8A%B8_%ED%85%8C%EC%8A%A4%ED%8A%B8%EC%84%B8%ED%8A%B8.html",
            "date": " • Dec 26, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "앙상블 // 랜덤 포레스트 RandomForest // 엑스트라 트리 ExtraTreesClassifier",
            "content": ". &#47004;&#45924; &#54252;&#47112;&#49828;&#53944; // RandomForest . 랜덤하게 만들어진 여러개의 결정트리가 모인 숲 | 랜덤 포레스트는 각 트리마다 훈련 세트가 다름 --&gt; 각 트리의 훈련 세트를 만들때 전체 훈련세트에서 복원추출을 시행하여 만듬 --&gt; 부트스트랩 샘플 | 랜덤하게 샘플링된 훈련 세트 덕분에 과대적합을 방지해줌 | . import numpy as np import pandas as pd from sklearn.model_selection import train_test_split wine = pd.read_csv(&#39;https://bit.ly/wine_csv_data&#39;) wine.head() . alcohol sugar pH class . 0 9.4 | 1.9 | 3.51 | 0.0 | . 1 9.8 | 2.6 | 3.20 | 0.0 | . 2 9.8 | 2.3 | 3.26 | 0.0 | . 3 9.8 | 1.9 | 3.16 | 0.0 | . 4 9.4 | 1.9 | 3.51 | 0.0 | . wine_input = wine[[&#39;alcohol&#39;, &#39;sugar&#39;, &#39;pH&#39;]].to_numpy() wine_target = wine[&#39;class&#39;].to_numpy() train_input, test_input, train_target, test_target = train_test_split(wine_input, wine_target, test_size = 0.2, random_state = 42) . from sklearn.model_selection import cross_validate from sklearn.ensemble import RandomForestClassifier rf = RandomForestClassifier(n_jobs = -1, random_state = 42) score = cross_validate(rf, train_input, train_target ,n_jobs = -1, return_train_score = True) # return_train_score = True --&gt; 훈련 세트에 대한 점수도 반환 --&gt; 과대적합을 확인할때 편함 score . {&#39;fit_time&#39;: array([0.68797898, 0.68438673, 0.69149828, 0.68736911, 0.44117284]), &#39;score_time&#39;: array([0.10288453, 0.1029191 , 0.10267925, 0.10335732, 0.10253263]), &#39;test_score&#39;: array([0.88461538, 0.88942308, 0.90279115, 0.88931665, 0.88642926]), &#39;train_score&#39;: array([0.9971133 , 0.99663219, 0.9978355 , 0.9973545 , 0.9978355 ])} . print(np.mean(score[&#39;test_score&#39;]), np.mean(score[&#39;train_score&#39;])) # 과대적합이 나타남 --&gt; 랜덤포레스트도 결정트리를 사용하는 거기 때문에 가지치기가 필요할 것으로 보임 . 0.8905151032797809 0.9973541965122431 . rf.fit(train_input, train_target) print(rf.feature_importances_) # 결정트리만 사용했을 때의 feature_importances_ 결과 값 --&gt; [0.12345626 0.86862934 0.0079144] # 랜덤포레스트는 각 트리마다 전체 특성 개수의 제곱근만큼의 특성을 선택함 ex) 1번 트리 alcohol, sugar / 2번 트리 sugar,pH # 각 트리마다 특성의 일부만 훈련에 사용하기 때문에 하나의 특성에 과도하게 집중하지 않고 많은 특성을 사용할 수 있음 --&gt; 과대적합을 줄임 . [0.23167441 0.50039841 0.26792718] . # 복원추출을 했기 때문에 부트스트랩 샘플로 뽑히지 않은 샘플들이 존재 --&gt; OOB 샘플 # 이런 OOB 샘플을 이용해 부트스트랩 샘플로 훈련한 결정트리를 평가 할 수 있음 --&gt; oob_score = True --&gt; OOB 샘플이 검증세트가 되어 교차검증을 하는 느낌 rf = RandomForestClassifier(oob_score = True, random_state = 42, n_jobs = -1) rf.fit(train_input, train_target) print(rf.oob_score_) # cross_validate 로 확인한 점수와 비슷하게 나옴 . 0.8934000384837406 . &#50672;&#49845; . import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import cross_validate wine = pd.read_csv(&#39;https://bit.ly/wine_csv_data&#39;) wine.head() wine_input = wine[[&#39;alcohol&#39;, &#39;sugar&#39;, &#39;pH&#39;]].to_numpy() wine_target = wine[&#39;class&#39;].to_numpy() train_input, test_input, train_target, test_target = train_test_split(wine_input, wine_target, test_size = 0.2, random_state = 42) score = cross_validate(RandomForestClassifier(random_state = 42), train_input, train_target, n_jobs = -1, return_train_score = True) print(np.mean(score[&#39;test_score&#39;]), np.mean(score[&#39;train_score&#39;])) rf = RandomForestClassifier(random_state = 42, n_jobs = -1, oob_score = True) rf.fit(train_input, train_target) print(rf.oob_score_) . 0.8905151032797809 0.9973541965122431 0.8934000384837406 . &#50641;&#49828;&#53944;&#46972; &#53944;&#47532; . 부트스트랩 샘플을 사용하지 않는다 --&gt; 알고리즘 훈련 때 전체훈련세트 이용 | 노드를 분할할 때 무작위로 분할한다 | 무작위성 때문에 속도가 빠르다 | . from sklearn.ensemble import ExtraTreesClassifier et = ExtraTreesClassifier(n_jobs = -1, random_state = 42) scores = cross_validate(et, train_input, train_target, return_train_score = True, n_jobs = -1) print(np.mean(score[&#39;train_score&#39;]), np.mean(score[&#39;test_score&#39;])) . 0.9973541965122431 0.8905151032797809 . et.fit(train_input, train_target) print(et.feature_importances_) . [0.20183568 0.52242907 0.27573525] .",
            "url": "https://parkjeongung.github.io/Ung.github.io/2021/12/26/%EC%95%99%EC%83%81%EB%B8%94_%EB%9E%9C%EB%8D%A4_%ED%8F%AC%EB%A0%88%EC%8A%A4%ED%8A%B8_RandomForest_%EC%97%91%EC%8A%A4%ED%8A%B8%EB%9D%BC_%ED%8A%B8%EB%A6%AC_ExtraTreesClassifier.html",
            "relUrl": "/2021/12/26/%EC%95%99%EC%83%81%EB%B8%94_%EB%9E%9C%EB%8D%A4_%ED%8F%AC%EB%A0%88%EC%8A%A4%ED%8A%B8_RandomForest_%EC%97%91%EC%8A%A4%ED%8A%B8%EB%9D%BC_%ED%8A%B8%EB%A6%AC_ExtraTreesClassifier.html",
            "date": " • Dec 26, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "선형 회귀 // LinearRegression",
            "content": ". import numpy as np perch_length = np.array([8.4, 13.7, 15.0, 16.2, 17.4, 18.0, 18.7, 19.0, 19.6, 20.0, 21.0, 21.0, 21.0, 21.3, 22.0, 22.0, 22.0, 22.0, 22.0, 22.5, 22.5, 22.7, 23.0, 23.5, 24.0, 24.0, 24.6, 25.0, 25.6, 26.5, 27.3, 27.5, 27.5, 27.5, 28.0, 28.7, 30.0, 32.8, 34.5, 35.0, 36.5, 36.0, 37.0, 37.0, 39.0, 39.0, 39.0, 40.0, 40.0, 40.0, 40.0, 42.0, 43.0, 43.0, 43.5, 44.0]) # 농어의 타겟 = 무게 perch_weight = np.array([5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0, 115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0, 150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0, 218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0, 556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0, 850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0, 1000.0]) # train test split from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(perch_length, perch_weight, random_state = 42) # 2차원 변형 train_input = train_input.reshape(-1, 1) test_input = test_input.reshape(-1, 1) # 모델링 --&gt; 이웃개수 3개인 최근접회귀 from sklearn.neighbors import KNeighborsRegressor knr = KNeighborsRegressor(n_neighbors = 3) knr.fit(train_input, train_target) . KNeighborsRegressor(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;, metric_params=None, n_jobs=None, n_neighbors=3, p=2, weights=&#39;uniform&#39;) . 50cm &#51064; &#45453;&#50612;&#51032; &#47924;&#44172; &#50696;&#52769; . print(knr.predict([[50]])) # 그러나 실제 50cm인 농어의 무게와 오차가 심하다는 예시 . [1033.33333333] . distancecs, indexes = knr.kneighbors([[50]]) predict = knr.predict([[50]]) import matplotlib.pyplot as plt plt.scatter(train_input, train_target) plt.scatter(50, predict, marker = &#39;^&#39;) plt.scatter(train_input[indexes], train_target[indexes], marker = &#39;D&#39;) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() # 훈련세트의 범위를 벗어났기 때문에 엉뚱한 값을 출력함 --&gt; 100cm 농어의 경우도 똑같은 무게를 출력 print(knr.predict([[100]])) . [1033.33333333] . &#49440;&#54805; &#54924;&#44480; &#47784;&#45944;&#47553; . KNeighborsRegressor로 위 문제를 해결하려면 train 세트를 길이가 긴 농어가 포함되도록 다시 만들어야 한다. | 선형 회귀를 이용하면 train 세트의 범위를 벗어나도 길이가 긴 농어의 무게를 예측할 수 있다. | 선형 회귀에서 알고리즘이 찾아낸 직선의 기울기나 y절편 값을 &quot;모델 파라미터&quot; 라고 한다. | . from sklearn.linear_model import LinearRegression lr = LinearRegression() lr.fit(train_input, train_target) # 50cm 농어의 무게 예측값과 100cm 농어의 무게 예측값 print(lr.predict([[50], [100]])) . [1241.83860323 3192.69585141] . # 이러한 파라미터들은 coef_(기울기) 와 intercept(y절편) 에 저장되어 있다. # 머신러닝에서 &quot;기울기&quot;는 &quot;계수&quot; 또는 &quot;가중치&quot; 라고도 한다. print(lr.coef_, lr.intercept_) # 훈련세트와 농어의 길이 15 ~ 50를 기준으로 알고리즘이 찾아낸 직선 그리기 import matplotlib.pyplot as plt plt.scatter(train_input, train_target) # 15 ~ 50까지 직선 그리기 plt.plot([15, 50], [15 * lr.coef_ + lr.intercept_, 50 * lr.coef_ + lr.intercept_]) # 50cm 농어 데이터 predict = lr.predict([[50]]) plt.scatter(50, predict, marker = &#39;^&#39;) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() . [39.01714496] -709.0186449535477 . print(lr.score(train_input, train_target)) print(lr.score(test_input, test_target)) # 테스트세트의 점수가 현저히 낮은 과대적합 # 훈련세트의 점수 또한 높은 편이 아니기에 전체적으로 과소적합 되었다. # 직선을 확인했을 때 X = 15 인 구간에서 Y 값이 0 이하가 나온다. --&gt; 현실에서는 불가능 . 0.9398463339976039 0.8247503123313558 . &#45796;&#54637; &#54924;&#44480; . import matplotlib.pyplot as plt plt.scatter(train_input, train_target) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() # train세트의 산점도는 일직선 이라기 보다는 곡선의 형태에 가깝다 --&gt; 2차방정식의 그래프를 그려보자 --&gt; y = ax^2 + bx + c . # np.column_stack() 을 이용해서 train세트의 제곱과 train세트 2배열을 나란히 붙인다. --&gt; test세트도 마찬가지 train_poly = np.column_stack((train_input ** 2, train_input)) test_poly = np.column_stack((test_input ** 2, test_input)) print(train_poly.shape, test_poly.shape) # 모든 원소 제곱이 하나의 열을 이루었기 때문에 열이 2개로 늘어났다. . (42, 2) (14, 2) . from sklearn.linear_model import LinearRegression lr = LinearRegression() lr.fit(train_poly, train_target) print(lr.predict([[50 ** 2, 50]])) . [1573.98423528] . print(lr.coef_, lr.intercept_) # trian 세트 산점도를 그리고 그 위에 계수와 절편을 이용해 모델링에 사용된 곡선 그리기 x = np.arange(15, 51) # 15cm ~ 50cm길이의 농어 배열 생성 import matplotlib.pyplot as plt plt.scatter(train_input, train_target) plt.plot(x, lr.coef_[0] * x ** 2 + lr.coef_[1] * x + lr.intercept_) plt.scatter(50, lr.predict([[50 ** 2, 50]]), marker = &#39;^&#39;) plt.show() . [ 1.01433211 -21.55792498] 116.05021078278276 . print(lr.score(train_poly, train_target)) print(lr.score(test_poly, test_target)) # 과소적합이 아직 조금은 남아있음 --&gt; 조금 더 복잡한 모델 생성 . 0.9706807451768623 0.9775935108325122 . &#50672;&#49845; . import numpy as np perch_length = np.array([8.4, 13.7, 15.0, 16.2, 17.4, 18.0, 18.7, 19.0, 19.6, 20.0, 21.0, 21.0, 21.0, 21.3, 22.0, 22.0, 22.0, 22.0, 22.0, 22.5, 22.5, 22.7, 23.0, 23.5, 24.0, 24.0, 24.6, 25.0, 25.6, 26.5, 27.3, 27.5, 27.5, 27.5, 28.0, 28.7, 30.0, 32.8, 34.5, 35.0, 36.5, 36.0, 37.0, 37.0, 39.0, 39.0, 39.0, 40.0, 40.0, 40.0, 40.0, 42.0, 43.0, 43.0, 43.5, 44.0]) # 농어의 타겟 = 무게 perch_weight = np.array([5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0, 115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0, 150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0, 218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0, 556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0, 850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0, 1000.0]) # train test split from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(perch_length, perch_weight, random_state = 42) # 2차원 변형 train_input = train_input.reshape(-1,1) test_input = test_input.reshape(-1,1) # 선형회귀 모델링 from sklearn.linear_model import LinearRegression lr = LinearRegression() lr.fit(train_input, train_target) # 점수 확인 --&gt; 과대적합 이면서 전체적인 과소적합 print(lr.score(train_input, train_target)) print(lr.score(test_input, test_target)) # 산점도와 직선 확인 import matplotlib.pyplot as plt plt.scatter(train_input, train_target) plt.plot([15, 50], [15 * lr.coef_ + lr.intercept_, 50 * lr.coef_ + lr.intercept_]) plt.scatter(50, lr.predict([[50]]), marker = &#39;^&#39;) print(plt.show()) # 2차방정식 선형회귀 모델링 # 제곱형태의 train, test세트 준비 train_poly = np.column_stack((train_input ** 2, train_input)) test_poly = np.column_stack((test_input ** 2, test_input)) # 모델링 lr.fit(train_poly, train_target) # 점수확인 --&gt; 점수 많이 좋아짐 print(lr.score(train_poly, train_target)) print(lr.score(test_poly, test_target)) # 산점도와 곡선확인 plt.scatter(train_input, train_target) x = np.arange(15, 51) plt.plot(x, x ** 2 * lr.coef_[0] + x * lr.coef_[1] + lr.intercept_) plt.scatter(50, lr.predict([[50 ** 2, 50]]), marker = &#39;^&#39;) plt.show() . 0.9398463339976039 0.8247503123313558 . None 0.9706807451768623 0.9775935108325122 .",
            "url": "https://parkjeongung.github.io/Ung.github.io/2021/12/26/%EC%84%A0%ED%98%95_%ED%9A%8C%EA%B7%80_LinearRegression.html",
            "relUrl": "/2021/12/26/%EC%84%A0%ED%98%95_%ED%9A%8C%EA%B7%80_LinearRegression.html",
            "date": " • Dec 26, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "로지스틱회귀 LogisticeRegression // 시그모이드 // 소프트맥스",
            "content": ". import pandas as pd fish = pd.read_csv(&#39;https://bit.ly/fish_csv_data&#39;) fish.head() . Species Weight Length Diagonal Height Width . 0 Bream | 242.0 | 25.4 | 30.0 | 11.5200 | 4.0200 | . 1 Bream | 290.0 | 26.3 | 31.2 | 12.4800 | 4.3056 | . 2 Bream | 340.0 | 26.5 | 31.1 | 12.3778 | 4.6961 | . 3 Bream | 363.0 | 29.0 | 33.5 | 12.7300 | 4.4555 | . 4 Bream | 430.0 | 29.0 | 34.0 | 12.4440 | 5.1340 | . print(pd.unique(fish[&#39;Species&#39;])) . [&#39;Bream&#39; &#39;Roach&#39; &#39;Whitefish&#39; &#39;Parkki&#39; &#39;Perch&#39; &#39;Pike&#39; &#39;Smelt&#39;] . # Species 를 제외한 나머지 열이 특성이 된다. fish_input = fish[[&#39;Weight&#39;, &#39;Length&#39;, &#39;Diagonal&#39;, &#39;Height&#39;, &#39;Width&#39;]].to_numpy() fish_target = fish[&#39;Species&#39;].to_numpy() # 넘파이 배열로 잘 바뀌었는지 확인 print(fish_input[: 5]) . [[242. 25.4 30. 11.52 4.02 ] [290. 26.3 31.2 12.48 4.3056] [340. 26.5 31.1 12.3778 4.6961] [363. 29. 33.5 12.73 4.4555] [430. 29. 34. 12.444 5.134 ]] . from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(fish_input, fish_target, random_state = 42) . from sklearn.preprocessing import StandardScaler ss = StandardScaler() ss.fit(train_input) train_scaled = ss.transform(train_input) test_scaled = ss.transform(test_input) . K - &#52572;&#44540;&#51217; &#51060;&#50883; &#48516;&#47448;&#44592; &#51060;&#50857; . from sklearn.neighbors import KNeighborsClassifier kn = KNeighborsClassifier(n_neighbors = 3) kn.fit(train_scaled, train_target) print(kn.score(train_scaled, train_target)) print(kn.score(test_scaled, test_target)) . 0.8907563025210085 0.85 . print(kn.classes_) # 도미 빙어 분류 --&gt; 이진분류 # 럭키백 --&gt; 다중분류 . [&#39;Bream&#39; &#39;Parkki&#39; &#39;Perch&#39; &#39;Pike&#39; &#39;Roach&#39; &#39;Smelt&#39; &#39;Whitefish&#39;] . print(kn.predict(test_scaled[:5])) . [&#39;Perch&#39; &#39;Smelt&#39; &#39;Pike&#39; &#39;Perch&#39; &#39;Perch&#39;] . # np.round --&gt; 반올림 --&gt; decimals 로 소수점 어디서 반올림 할건지 지정 import numpy as np proba = kn.predict_proba(test_scaled[:5]) print(np.round(proba, decimals = 4)) . [[0. 0. 1. 0. 0. 0. 0. ] [0. 0. 0. 0. 0. 1. 0. ] [0. 0. 0. 1. 0. 0. 0. ] [0. 0. 0.6667 0. 0.3333 0. 0. ] [0. 0. 0.6667 0. 0.3333 0. 0. ]] . distances, indexes = kn.kneighbors(test_scaled[3:4]) # kneighbors 메서드의 입력은 항상 2차원 배열이어야 하므로 슬라이싱으로 해야지 오류가 안남 print(train_target[indexes]) . [[&#39;Roach&#39; &#39;Perch&#39; &#39;Perch&#39;]] . &#50672;&#49845; . import pandas as pd fish = pd.read_csv(&#39;https://bit.ly/fish_csv_data&#39;) fish.head() # 특성 확인 print(pd.unique(fish[&#39;Species&#39;])) # 입력 타깃 나누기 fish_input = fish[[&#39;Weight&#39;, &#39;Length&#39;, &#39;Diagonal&#39;, &#39;Height&#39;, &#39;Width&#39;]].to_numpy() fish_target = fish[&#39;Species&#39;].to_numpy() # train test split from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(fish_input, fish_target, random_state = 42) # standard scaler from sklearn.preprocessing import StandardScaler ss = StandardScaler() ss.fit(train_input) train_scaled = ss.transform(train_input) test_scaled = ss.transform(test_input) # 최근접 이웃 from sklearn.neighbors import KNeighborsClassifier kn = KNeighborsClassifier(n_neighbors = 3) kn.fit(train_scaled, train_target) print(kn.score(train_scaled, train_target)) print(kn.score(test_scaled, test_target)) # 사용가능 타깃값 확인 kn.classes_ print(kn.classes_) # 샘플 5개 예측 print(kn.predict(test_scaled[:5])) # 타깃별 확률값 출력 import numpy as np proba = kn.predict_proba(test_scaled[:5]) print(np.round(proba, decimals = 4)) # 4번째 샘플의 사용 이웃확인 distances, indexes = kn.kneighbors(test_scaled[3: 4]) print(train_target[indexes]) . [&#39;Bream&#39; &#39;Roach&#39; &#39;Whitefish&#39; &#39;Parkki&#39; &#39;Perch&#39; &#39;Pike&#39; &#39;Smelt&#39;] 0.8907563025210085 0.85 [&#39;Bream&#39; &#39;Parkki&#39; &#39;Perch&#39; &#39;Pike&#39; &#39;Roach&#39; &#39;Smelt&#39; &#39;Whitefish&#39;] [&#39;Perch&#39; &#39;Smelt&#39; &#39;Pike&#39; &#39;Perch&#39; &#39;Perch&#39;] [[0. 0. 1. 0. 0. 0. 0. ] [0. 0. 0. 0. 0. 1. 0. ] [0. 0. 0. 1. 0. 0. 0. ] [0. 0. 0.6667 0. 0.3333 0. 0. ] [0. 0. 0.6667 0. 0.3333 0. 0. ]] [[&#39;Roach&#39; &#39;Perch&#39; &#39;Perch&#39;]] . &#47196;&#51648;&#49828;&#54001; &#54924;&#44480; . 이름은 회귀이지만 분류 모델 | 다중회귀를 한 선형 회귀와 동일하게 선형 방정식을 학습 | 시그모이드 0과 1사이의 값만 출력 --&gt; 이진분류에 사용 | 소프트맥스 0과 1사이의 값을 출력 하는데 총합이 1 --&gt; 다중분류에 사용 | . import numpy as np import matplotlib.pyplot as plt z = np.arange(-5, 5, 0.1) phi = 1 / (1 + np.exp(-z)) # 지수 함수의 계산 --&gt; np.exp() plt.plot(z, phi) plt.xlabel(&#39;z&#39;) plt.ylabel(&#39;phi&#39;) plt.show() # 시그모이드 함수 출력이 0.5 보다 크면 양성, 0.5 이하이면 음성 # 도미와 빙어로 이진분류 수행해보기 . # ex --&gt; A, C 만 선택하기 char_arr = np.array([&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;, &#39;E&#39;]) print(char_arr[[True, False, True, False, False]]) # 적용 bream_smelt_indexes = (train_target == &#39;Bream&#39;) | (train_target == &#39;Smelt&#39;) train_bream_smelt = train_scaled[bream_smelt_indexes] target_bream_smelt = train_target[bream_smelt_indexes] . [&#39;A&#39; &#39;C&#39;] . from sklearn.linear_model import LogisticRegression lr = LogisticRegression() lr.fit(train_bream_smelt, target_bream_smelt) . LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=100, multi_class=&#39;auto&#39;, n_jobs=None, penalty=&#39;l2&#39;, random_state=None, solver=&#39;lbfgs&#39;, tol=0.0001, verbose=0, warm_start=False) . lr.predict(train_bream_smelt[:5]) . array([&#39;Bream&#39;, &#39;Smelt&#39;, &#39;Bream&#39;, &#39;Bream&#39;, &#39;Bream&#39;], dtype=object) . print(lr.predict_proba(train_bream_smelt[:5])) # 첫번째 열이 음성클래스 두번째 열이 양성클래스 # 어떤 것이 음성인지 확인 print(lr.classes_) # bream 이 음성 smelt 가 양성 . [[0.99759855 0.00240145] [0.02735183 0.97264817] [0.99486072 0.00513928] [0.98584202 0.01415798] [0.99767269 0.00232731]] [&#39;Bream&#39; &#39;Smelt&#39;] . decisions = lr.decision_function(train_bream_smelt[: 5]) decisions . array([-6.02927744, 3.57123907, -5.26568906, -4.24321775, -6.0607117 ]) . # 시그모이드 함수 --&gt; expit() from scipy.special import expit print(expit(decisions)) # predict_proba 의 두번째 열과 동일하다 --&gt; decision_function 은 양성 클래스에 대한 Z값을 반환한다. # predict_proba 는 음성과 양성 클래스에 대한 확률을 둘다 반환한다. . [0.00240145 0.97264817 0.00513928 0.01415798 0.00232731] . from sklearn.linear_model import LogisticRegression lr = LogisticRegression(C = 20, max_iter = 1000) # C 는 라쏘 릿지의 alpha 값 같은 느낌 --&gt; 규제의 정도를 조절 --&gt; alpha 와는 달리 값이 작을수록 규제의 강도가 큼 lr.fit(train_scaled, train_target) print(lr.score(train_scaled, train_target)) print(lr.score(test_scaled, test_target)) . 0.9327731092436975 0.925 . print(lr.predict(test_scaled[: 5])) . [&#39;Perch&#39; &#39;Smelt&#39; &#39;Pike&#39; &#39;Roach&#39; &#39;Perch&#39;] . import numpy as np proba = lr.predict_proba(test_scaled[: 5]) print(np.round(proba, decimals = 3)) . [[0. 0.014 0.841 0. 0.136 0.007 0.003] [0. 0.003 0.044 0. 0.007 0.946 0. ] [0. 0. 0.034 0.935 0.015 0.016 0. ] [0.011 0.034 0.306 0.007 0.567 0. 0.076] [0. 0. 0.904 0.002 0.089 0.002 0.001]] . print(lr.classes_) . [&#39;Bream&#39; &#39;Parkki&#39; &#39;Perch&#39; &#39;Pike&#39; &#39;Roach&#39; &#39;Smelt&#39; &#39;Whitefish&#39;] . decision = lr.decision_function(test_scaled[: 5]) print(np.round(decision, decimals = 2)) from scipy.special import softmax proba = softmax(decision, axis = 1) print(np.round(proba, decimals = 3)) . [[ -6.5 1.03 5.16 -2.73 3.34 0.33 -0.63] [-10.86 1.93 4.77 -2.4 2.98 7.84 -4.26] [ -4.34 -6.23 3.17 6.49 2.36 2.42 -3.87] [ -0.68 0.45 2.65 -1.19 3.26 -5.75 1.26] [ -6.4 -1.99 5.82 -0.11 3.5 -0.11 -0.71]] [[0. 0.014 0.841 0. 0.136 0.007 0.003] [0. 0.003 0.044 0. 0.007 0.946 0. ] [0. 0. 0.034 0.935 0.015 0.016 0. ] [0.011 0.034 0.306 0.007 0.567 0. 0.076] [0. 0. 0.904 0.002 0.089 0.002 0.001]] . &#50672;&#49845; . import pandas as pd fish = pd.read_csv(&#39;https://bit.ly/fish_csv_data&#39;) fish.head() # 넘파이 전환 fish_input = fish[[&#39;Weight&#39;, &#39;Length&#39;, &#39;Diagonal&#39;, &#39;Height&#39;, &#39;Width&#39;]].to_numpy() fish_target = fish[&#39;Species&#39;].to_numpy() # train test split from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(fish_input, fish_target, random_state = 42) # 스케일링 from sklearn.preprocessing import StandardScaler ss = StandardScaler() ss.fit(train_input) train_scaled = ss.transform(train_input) test_scaled = ss.transform(test_input) # 이진분류 도미 빙어 bream_smelt_indexes = (train_target == &#39;Bream&#39;) | (train_target == &#39;Smelt&#39;) bream_smelt_train = train_scaled[bream_smelt_indexes] bream_smelt_target = train_target[bream_smelt_indexes] # 모델 훈련 from sklearn.linear_model import LogisticRegression lr = LogisticRegression() lr.fit(bream_smelt_train, bream_smelt_target) # 샘플 5개 예측 lr.predict(bream_smelt_train[: 5]) # 샘플 5개 확률 print(lr.predict_proba(bream_smelt_train[: 5])) # Z값 구하기 decision = lr.decision_function(bream_smelt_train[: 5]) print(decision) # 시그모이드 대입 from scipy.special import expit print(expit(decision)) # 다중분류 lr.fit(train_scaled, train_target) # 샘플 5개 예측 print(lr.predict(test_scaled[: 5])) # 샘플 5개 확률 proba = lr.predict_proba(test_scaled[: 5]) import numpy as np print(np.round(proba, decimals=3)) # Z값 구하기 decision = lr.decision_function(test_scaled[: 5]) # 소프트맥스 대입 from scipy.special import softmax softmax = softmax(decision, axis = 1) print(np.round(softmax, decimals=3)) .",
            "url": "https://parkjeongung.github.io/Ung.github.io/2021/12/26/%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1%ED%9A%8C%EA%B7%80_LogisticeRegression_%EC%8B%9C%EA%B7%B8%EB%AA%A8%EC%9D%B4%EB%93%9C_%EC%86%8C%ED%94%84%ED%8A%B8%EB%A7%A5%EC%8A%A4.html",
            "relUrl": "/2021/12/26/%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1%ED%9A%8C%EA%B7%80_LogisticeRegression_%EC%8B%9C%EA%B7%B8%EB%AA%A8%EC%9D%B4%EB%93%9C_%EC%86%8C%ED%94%84%ED%8A%B8%EB%A7%A5%EC%8A%A4.html",
            "date": " • Dec 26, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "데이터 전처리",
            "content": ". fish_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0, 9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0] fish_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0, 6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9] . &#45336;&#54028;&#51060;&#47196; input &#45936;&#51060;&#53552;&#50752; target &#45936;&#51060;&#53552; &#47564;&#46308;&#44592; . import numpy as np fish_data = np.column_stack((fish_length, fish_weight)) # target 데이터 생성 --&gt; np.ones(), np.zeros() # 두개의 배열을 연결한다 --&gt; np.concatenate(()) fish_target = np.concatenate((np.ones(35), np.zeros(14))) . &#49324;&#51060;&#53431;&#47088;&#51004;&#47196; &#54984;&#47144;&#49464;&#53944;&#50752; &#53580;&#49828;&#53944;&#49464;&#53944; &#45208;&#45572;&#44592; . from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(fish_data, fish_target, random_state = 42) # 잘 나누어졌는지 shpae로 데이터크기 출력 print(train_input.shape, test_input.shape) print(train_target.shape, test_target.shape) # 전체 데이터의 도미와 빙어의 비율 = 35 (도미) : 14 (빙어) / 2.5 : 1 # 테스트 세트의 도미와 빙어의 비율 = 10 (도미) : 3 (빙어) / 3.3 : 1 # 샘플링 편향 발생 print(test_target) . (36, 2) (13, 2) (36,) (13,) [1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.] . # stratify 매개변수에 target데이터를 전달하면 클래스 비율에 맞게 데이터를 나눈다. from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(fish_data, fish_target, stratify = fish_target, random_state = 42) # 전체 데이터의 도미와 빙어의 비율 = 35 (도미) : 14 (빙어) / 2.5 : 1 # 테스트 세트의 도미와 빙어의 비율 = 9 (도미) : 4 (빙어) / 2.25 : 1 # 데이터가 작아 비율을 동일하게 맞출 수 없지만 꽤 올바른 샘플링 print(test_target) . [0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1.] . &#49688;&#49345;&#54620; &#46020;&#48120; &#54620; &#47560;&#47532; . [[25, 150]] 이라는 도미 데이터를 예측값으로 넣었더니 빙어로 출력이 된다. | 산점도에 있어서 직관적으로는 [[25, 150]] 데이터가 도미에 가까워 보이지만 x축의 범위와 y축의 범위를 생각하면 [[25, 150]] 데이터가 빙어데이터에 가까운것이 합리적이다. | 두 특성의 스케일이 다르면 알고리즘이 올바른 예측을 하지 못한다. --&gt; 데이터 전처리를 통해 특성값을 일정한 기준으로 맞춰준다. | . from sklearn.neighbors import KNeighborsClassifier kn = KNeighborsClassifier() kn.fit(train_input, train_target) kn.score(test_input, test_target) # [[25, 150]] 이라는 도미 데이터를 예측값으로 넣었더니 빙어[0]로 출력이 된다. print(kn.predict([[25, 150]])) . [0.] . import matplotlib.pyplot as plt plt.scatter(train_input[:, 0], train_input[:, 1]) plt.scatter(25, 150, marker = &#39;^&#39;) # 새로운 데이터를 marker = &#39;^&#39;로 지정하여 삼각형으로 표현함. --&gt; 구분하기 쉬움 plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() . distances, indexes = kn.kneighbors([[25, 150]]) # 주어진 샘플의 이웃 샘플을 따로 구분해서 산점도를 그린다. plt.scatter(train_input[ : , 0], train_input[ : , 1]) plt.scatter(25, 150 , marker = &#39;^&#39;) plt.scatter(train_input[indexes, 0], train_input[indexes, 1], marker = &#39;D&#39;) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() print(train_input[indexes]) print(train_target[indexes]) print(distances) print(indexes) # [[25, 150]]의 샘플은 도미 데이터를 1개밖에 포함하지 않는다. . [[[ 37. 1000. ] [ 41. 975. ] [ 38.5 955. ] [ 39.5 925. ] [ 36. 850. ]]] [[1. 1. 1. 1. 1.]] [[150.24524979 150.25805338 150.35871414 150.43446443 150.71926768]] [[10 35 17 4 7]] . # x축의 길이와 y축의 길이를 같게 해서 산점도를 그려보자. --&gt; xlim() = x축 범위 지정 , ylim() = y축 범위 지정 plt.scatter(train_input[ : , 0], train_input[ : , 1]) plt.scatter(25, 150 , marker = &#39;^&#39;) plt.scatter(train_input[indexes, 0], train_input[indexes, 1], marker = &#39;D&#39;) plt.xlim((0, 1000)) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() # weight 와 length 두 특성의 값이 놓인 범위가 매우 다르다. --&gt; 두 특성의 스케일이 다르다. # 두 특성의 스케일이 다르면 알고리즘이 올바르게 예측할 수 없다. --&gt; 특성값을 일정한 기준으로 맞춰주어야 한다. --&gt; 데이터 전처리를 한다. . &#54364;&#51456;&#51216;&#49688;(standard score) --&gt; &#45936;&#51060;&#53552; &#51204;&#52376;&#47532; . 각 특성값이 평균에서 표준편차의 몇 배만큼 떨어져 있는지를 나타낸다. | 평균을 빼고 표준편차로 나눈다. | 특성마다 값의 스케일이 다르므로 평균과 표준편차는 각 특성별로 계산해야 한다. | 테스트 세트를 스케일할때에도 훈련 세트의 평균과 표준편차를 이용하여 스케일 해야 한다. | . # 평균을 구하는 넘파이 함수 --&gt; np.mean() mean = np.mean(train_input, axis = 0) # 표준편차를 구하는 넘파이 함수 --&gt; np.std() std = np.std(train_input, axis = 0) mean, std . (array([ 27.29722222, 454.09722222]), array([ 9.98244253, 323.29893931])) . train_scaled = (train_input - mean) / std . plt.scatter(train_scaled[ :, 0], train_scaled[ :, 1]) plt.scatter(25, 150, marker = &#39;^&#39;) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() # 새로운 데이터는 전처리가 안돼있어서 따로 멀리 떨어져나옴 --&gt; 새로운 데이터도 전처리 . new = ([25, 150] - mean) / std plt.scatter(train_scaled[ :, 0], train_scaled[ :, 1]) plt.scatter(new[0], new[1], marker = &#39;^&#39;) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() #새로운 데이터도 올바르게 표현됨 . kn.fit(train_scaled, train_target) . KNeighborsClassifier(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;, metric_params=None, n_jobs=None, n_neighbors=5, p=2, weights=&#39;uniform&#39;) . test_scaled = (test_input - mean) / std . kn.score(test_scaled, test_target) . 1.0 . print(kn.predict([new])) # 수상한 도미 한 마리와 가장 근접한 이웃 데이터 확인하기 distances, indexes = kn.kneighbors([new]) plt.scatter(train_scaled[ :, 0], train_scaled[ :, 1]) plt.scatter(new[0], new[1], marker = &#39;^&#39;) plt.scatter(train_scaled[indexes, 0], train_scaled[indexes, 1], marker = &#39;D&#39;) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() . [1.] . &#50672;&#49845; . fish_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0, 9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0] fish_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0, 6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9] # train 세트 test 세트 분류 import numpy as np fish_data = np.column_stack((fish_length, fish_weight)) fish_target = np.concatenate((np.ones(35), np.zeros(14))) from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(fish_data, fish_target, stratify = fish_target, random_state = 42) # 데이터 전처리 mean = np.mean(train_input, axis = 0) std = np.std(train_input, axis = 0) train_scaled = (train_input - mean) / std test_scaled = (test_input - mean) / std # 모델링 from sklearn.neighbors import KNeighborsClassifier kn = KNeighborsClassifier() kn.fit(train_scaled, train_target) kn.score(test_scaled, test_target) # 수상한 도미 한 마리 --&gt; new = [25,150] new = [25, 150] scaled_new = ([25, 150] - mean) / std distances, indexes = kn.kneighbors([scaled_new]) import matplotlib.pyplot as plt plt.scatter(train_scaled[ :, 0], train_scaled[ :, 1]) plt.scatter(scaled_new[0], scaled_new[1], marker = &#39;^&#39;) plt.scatter(train_scaled[indexes, 0], train_scaled[indexes, 1]) plt.show() kn.predict([scaled_new]) . array([1.]) .",
            "url": "https://parkjeongung.github.io/Ung.github.io/2021/12/26/%EB%8D%B0%EC%9D%B4%ED%84%B0_%EC%A0%84%EC%B2%98%EB%A6%AC.html",
            "relUrl": "/2021/12/26/%EB%8D%B0%EC%9D%B4%ED%84%B0_%EC%A0%84%EC%B2%98%EB%A6%AC.html",
            "date": " • Dec 26, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "교차 검증 cross validation //  그리드 서치 GridSearchCV // 랜덤서치 RandomizedSearchCV",
            "content": ". &#44160;&#51613; &#49464;&#53944; . 훈련 세트를 한번 더 나눠서 테스트 세트 이외의 새로운 데이터 세트를 만든다. | 만들어진 검증세트는 모델의 가장 좋은 매개변수를 찾는데 사용된다. | . import pandas as pd wine = pd.read_csv(&#39;https://bit.ly/wine_csv_data&#39;) . wine_input = wine[[&#39;alcohol&#39;, &#39;sugar&#39;, &#39;pH&#39;]].to_numpy() wine_target = wine[&#39;class&#39;].to_numpy() . from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(wine_input, wine_target, test_size = 0.2, random_state = 42) . sub_input, val_input, sub_target, val_target = train_test_split(train_input, train_target, test_size = 0.2, random_state = 42) . print(train_input.shape) print(sub_input.shape, val_input.shape) . (5197, 3) (4157, 3) (1040, 3) . from sklearn.tree import DecisionTreeClassifier dt = DecisionTreeClassifier(random_state = 42) dt.fit(sub_input, sub_target) print(dt.score(sub_input, sub_target)) print(dt.score(val_input, val_target)) # 훈련 세트에 과대적합된 모델 --&gt; 좋은 하이퍼 파라미터를 찾아야함 . 0.9971133028626413 0.864423076923077 . &#44368;&#52264; &#44160;&#51613; // cross validation --&gt; &#44160;&#51613; &#49464;&#53944;&#47484; &#46524;&#50612; &#45236;&#50612; &#54217;&#44032;&#54616;&#45716; &#44284;&#51221;&#51012; &#50668;&#47084; &#48264; &#48152;&#48373;&#54620;&#45796;. . 검증 세트를 만들었기에 훈련 세트의 데이터 수가 줄어듦 | 교차검증을 이용하면 안정적인 검증점수와 더 많은 데이터를 훈련에 사용할 수 있다. | . from sklearn.model_selection import cross_validate scores = cross_validate(dt, train_input, train_target) print(scores) # fit_time = 모델 훈련하는데 걸린 시간 # score_time = 모델 검증하는데 걸린 시간 # test_score = 검증 점수 . {&#39;fit_time&#39;: array([0.00829268, 0.00734234, 0.00762033, 0.00749898, 0.00734591]), &#39;score_time&#39;: array([0.00101829, 0.00070906, 0.00075388, 0.00072241, 0.00079203]), &#39;test_score&#39;: array([0.86923077, 0.84615385, 0.87680462, 0.84889317, 0.83541867])} . import numpy as np print(np.mean(scores[&#39;test_score&#39;])) . 0.855300214703487 . # 분류 모델일 경우 StratifiedKFold() # 분류 모델인 DecisionTreeClassifier 는 StratifiedKFold() 사용 from sklearn.model_selection import StratifiedKFold scores = cross_validate(dt, train_input, train_target, cv = StratifiedKFold()) print(scores) . {&#39;fit_time&#39;: array([0.01132774, 0.00710869, 0.00750995, 0.00733685, 0.00716186]), &#39;score_time&#39;: array([0.00084209, 0.00075936, 0.00071144, 0.00073504, 0.00069094]), &#39;test_score&#39;: array([0.86923077, 0.84615385, 0.87680462, 0.84889317, 0.83541867])} . splitter = StratifiedKFold(n_splits = 10, shuffle = True, random_state = 42) # n_splits --&gt; 몇폴드 교차검증을 할지 결정 --&gt; 기본은 5폴드 scores = cross_validate(dt, train_input, train_target, cv = splitter) print(np.mean(scores[&#39;test_score&#39;])) . 0.8574181117533719 . &#50672;&#49845; . import pandas as pd wine = pd.read_csv(&#39;https://bit.ly/wine_csv_data&#39;) wine.head() wine_input = wine[[&#39;alcohol&#39;, &#39;sugar&#39;, &#39;pH&#39;]].to_numpy() wine_target = wine[&#39;class&#39;].to_numpy() from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(wine_input, wine_target, random_state = 42, test_size = 0.2) sub_input, val_input, sub_target, val_target = train_test_split(train_input, train_target, random_state = 42, test_size = 0.2) from sklearn.tree import DecisionTreeClassifier dt = DecisionTreeClassifier(random_state = 42) dt.fit(sub_input, sub_target) print(dt.score(sub_input, sub_target)) print(dt.score(val_input, val_target)) from sklearn.model_selection import cross_validate score = cross_validate(dt, train_input, train_target) print(score) import numpy as np print(np.mean(score[&#39;test_score&#39;])) from sklearn.model_selection import StratifiedKFold score = cross_validate(dt, train_input, train_target , cv = StratifiedKFold()) splitter = StratifiedKFold(n_splits=10, shuffle = True, random_state = 42) score = cross_validate(dt, train_input, train_target, cv = splitter) print(score) print(np.mean(score[&#39;test_score&#39;])) . 0.9971133028626413 0.864423076923077 {&#39;fit_time&#39;: array([0.00733209, 0.00694227, 0.00748754, 0.00725961, 0.00699782]), &#39;score_time&#39;: array([0.0006938 , 0.00061035, 0.00068307, 0.00061774, 0.00060177]), &#39;test_score&#39;: array([0.86923077, 0.84615385, 0.87680462, 0.84889317, 0.83541867])} 0.855300214703487 {&#39;fit_time&#39;: array([0.00808716, 0.00804591, 0.00808191, 0.00787067, 0.00802708, 0.00803208, 0.00794387, 0.00791359, 0.00801635, 0.00786281]), &#39;score_time&#39;: array([0.00054526, 0.00052047, 0.00048876, 0.00051832, 0.00057459, 0.00050998, 0.00052834, 0.00052619, 0.00053453, 0.00055051]), &#39;test_score&#39;: array([0.83461538, 0.87884615, 0.85384615, 0.85384615, 0.84615385, 0.87307692, 0.85961538, 0.85549133, 0.85163776, 0.86705202])} 0.8574181117533719 . &#44536;&#47532;&#46300; &#49436;&#52824; . 파라미터를 바꿔가면서 교차 검증을 수행한다 | 최적의 하이퍼 파라미터를 찾는다 | . # 0.0001 ~ 0.0005 까지 0.0001 씩 증가하는 5개의 값을 시도 from sklearn.model_selection import GridSearchCV # 파라미터를 딕셔너리 형태로 저장 params = {&#39;min_impurity_decrease&#39; : [0.0001, 0.0002, 0.0003, 0.0004, 0.0005]} . gs = GridSearchCV(DecisionTreeClassifier(random_state=42), params, n_jobs=-1) # n_jobs 는 cpu 코어 사용갯수 지정 # 최적의 하이퍼파라미터를 찾았으면 전체 훈련세트로 모델을 다시 만든다 gs.fit(train_input, train_target) . # 가장 높은 점수의 모델은 best_estimator_ 에 저장됨 dt = gs.best_estimator_ print(dt.score(train_input, train_target)) print(dt.score(test_input, test_target)) . 0.9615162593804117 0.8653846153846154 . print(gs.best_params_) . {&#39;min_impurity_decrease&#39;: 0.0001} . print(gs.cv_results_[&#39;mean_test_score&#39;]) . [0.86819297 0.86453617 0.86492226 0.86780891 0.86761605] . params = {&#39;min_impurity_decrease&#39; : np.arange(0.0001, 0.001, 0.0001), &#39;max_depth&#39; : range(5, 20, 1), &#39;min_samples_split&#39; : range(2, 100, 10) } . gs = GridSearchCV(DecisionTreeClassifier(random_state = 42), params, n_jobs = -1) gs.fit(train_input, train_target) . print(gs.best_params_) . {&#39;max_depth&#39;: 14, &#39;min_impurity_decrease&#39;: 0.0004, &#39;min_samples_split&#39;: 12} . print(gs.cv_results_[&#39;mean_test_score&#39;]) print(np.max(gs.cv_results_[&#39;mean_test_score&#39;])) . [0.85780355 0.85799604 0.85799604 ... 0.86126601 0.86165063 0.86357629] 0.8683865773302731 . &#47004;&#45924; &#49436;&#52824; . 그리드 서치로 매개변수를 일일이 바꿔가며 모델을 훈련시켜 최적의 파라미터를 찾아내지 않을 수 있게 됨 | 그리드 서치는 매개변수의 범위를 내가 지정하는 거기 때문에 범위에 대한 근거가 부족함 | 랜덤 서치로 그 문제를 해결 | . from scipy.stats import uniform, randint # uniform 은 실수를 randint 는 정수를 뽑음 # 0 ~ 9 까지의 정수 중 10개를 랜덤 추출 rgen = randint(0, 10) rgen.rvs(10) . array([3, 4, 8, 5, 2, 7, 3, 8, 6, 4]) . np.unique(rgen.rvs(1000), return_counts = True) . (array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([103, 101, 99, 99, 106, 108, 97, 94, 103, 90])) . ugen = uniform(0, 1) ugen.rvs(10) . array([0.50854909, 0.14208736, 0.22524214, 0.2296259 , 0.54808845, 0.49409384, 0.7933543 , 0.07221253, 0.50430007, 0.18970072]) . params = {&#39;min_impurity_decrease&#39; : uniform(0.0001, 0.001), &#39;max_depth&#39; : randint(20, 50), &#39;min_samples_split&#39; : randint(2, 25), &#39;min_samples_leaf&#39; : randint(1, 25)} . from sklearn.model_selection import RandomizedSearchCV rs = RandomizedSearchCV(DecisionTreeClassifier(random_state = 42), params, n_iter = 100, n_jobs = -1, random_state = 42) # n_iter --&gt; 정의된 매개변수 범위에서 몇 번 샘플링 할 것인지 결정 rs.fit(train_input, train_target) . print(rs.best_params_) . {&#39;max_depth&#39;: 39, &#39;min_impurity_decrease&#39;: 0.00034102546602601173, &#39;min_samples_leaf&#39;: 7, &#39;min_samples_split&#39;: 13} . print(rs.cv_results_[&#39;mean_test_score&#39;]) print(rs.cv_results_[&#39;mean_test_score&#39;].shape) print(np.max(rs.cv_results_[&#39;mean_test_score&#39;])) . [0.86511513 0.86261235 0.86838528 0.86588547 0.86376731 0.86434497 0.86280503 0.86280484 0.86357592 0.86357555 0.86280503 0.8626142 0.86472977 0.86954283 0.86203543 0.86761827 0.86222884 0.86473033 0.86877082 0.86184423 0.86126657 0.86511494 0.8626142 0.86203543 0.86511476 0.86607722 0.86222773 0.86684682 0.86261309 0.86338436 0.8629977 0.86242171 0.86184478 0.86165211 0.86049808 0.86530706 0.86280521 0.86684775 0.86203524 0.86318983 0.86780947 0.86761624 0.86126694 0.86934867 0.86857889 0.86530743 0.86434497 0.86415303 0.86838602 0.86530688 0.86145813 0.86684626 0.8618446 0.86145961 0.86338454 0.86569131 0.86242152 0.86376805 0.86203543 0.86376916 0.86511457 0.86184275 0.86338454 0.86242004 0.86107481 0.86203654 0.86184478 0.86434552 0.86184478 0.86338473 0.86299993 0.8641534 0.86338269 0.85972662 0.86415303 0.86665433 0.86261253 0.86222884 0.86858111 0.86472903 0.86242097 0.86261457 0.86742448 0.86434497 0.86684682 0.86184423 0.86107481 0.86877193 0.86338362 0.86242171 0.8674243 0.86222773 0.86242004 0.86415414 0.86376805 0.86261272 0.86184478 0.86357518 0.86203635 0.86203691] (100,) 0.8695428296438884 . dt = rs.best_estimator_ dt.fit(train_input, train_target) print(dt.score(test_input, test_target)) . 0.86 .",
            "url": "https://parkjeongung.github.io/Ung.github.io/2021/12/26/%EA%B5%90%EC%B0%A8_%EA%B2%80%EC%A6%9D_cross_validation_%EA%B7%B8%EB%A6%AC%EB%93%9C_%EC%84%9C%EC%B9%98_GridSearchCV_%EB%9E%9C%EB%8D%A4%EC%84%9C%EC%B9%98_RandomizedSearchCV.html",
            "relUrl": "/2021/12/26/%EA%B5%90%EC%B0%A8_%EA%B2%80%EC%A6%9D_cross_validation_%EA%B7%B8%EB%A6%AC%EB%93%9C_%EC%84%9C%EC%B9%98_GridSearchCV_%EB%9E%9C%EB%8D%A4%EC%84%9C%EC%B9%98_RandomizedSearchCV.html",
            "date": " • Dec 26, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "결정 트리 // DecisionTreeClassifier",
            "content": ". import pandas as pd wine = pd.read_csv(&#39;https://bit.ly/wine_csv_data&#39;) wine.head() . alcohol sugar pH class . 0 9.4 | 1.9 | 3.51 | 0.0 | . 1 9.8 | 2.6 | 3.20 | 0.0 | . 2 9.8 | 2.3 | 3.26 | 0.0 | . 3 9.8 | 1.9 | 3.16 | 0.0 | . 4 9.4 | 1.9 | 3.51 | 0.0 | . wine.info() # 6497개의 샘플, 4개의 열은 모두 float형, Non-Null Count가 모두 6497 이므로 누락된 값 없음 # 누락된 값이 확인되면 그 데이터 행을 버리거나 평균값으로 채운 후 사용 --&gt; 어떤 방식이 최선인지는 알기 어려우므로 둘다 시도해보기 . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 6497 entries, 0 to 6496 Data columns (total 4 columns): # Column Non-Null Count Dtype -- -- 0 alcohol 6497 non-null float64 1 sugar 6497 non-null float64 2 pH 6497 non-null float64 3 class 6497 non-null float64 dtypes: float64(4) memory usage: 203.2 KB . wine.describe() # 각 열의 스케일이 다름 --&gt; 스케일링 . alcohol sugar pH class . count 6497.000000 | 6497.000000 | 6497.000000 | 6497.000000 | . mean 10.491801 | 5.443235 | 3.218501 | 0.753886 | . std 1.192712 | 4.757804 | 0.160787 | 0.430779 | . min 8.000000 | 0.600000 | 2.720000 | 0.000000 | . 25% 9.500000 | 1.800000 | 3.110000 | 1.000000 | . 50% 10.300000 | 3.000000 | 3.210000 | 1.000000 | . 75% 11.300000 | 8.100000 | 3.320000 | 1.000000 | . max 14.900000 | 65.800000 | 4.010000 | 1.000000 | . wine_input = wine[[&#39;alcohol&#39;, &#39;sugar&#39;, &#39;pH&#39;]].to_numpy() wine_target = wine[&#39;class&#39;].to_numpy() . from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(wine_input, wine_target, test_size = 0.2, random_state = 42) # test_size 를 따로 지정하지 않으면 전체 데이터의 25% 를 테스트 세트로 지정. 샘플 갯수가 충분하므로 &quot;test_size = 0.2&quot; 로 20% 만 테스트 세트로 지정하겠다는 뜻 print(train_input.shape, test_input.shape) . (5197, 3) (1300, 3) . from sklearn.preprocessing import StandardScaler ss = StandardScaler() ss.fit(train_input) train_scaled = ss.transform(train_input) test_scaled = ss.transform(test_input) . from sklearn.linear_model import LogisticRegression lr = LogisticRegression() lr.fit(train_scaled, train_target) print(lr.score(train_scaled, train_target)) print(lr.score(test_scaled, test_target)) . 0.7808350971714451 0.7776923076923077 . from sklearn.tree import DecisionTreeClassifier dt = DecisionTreeClassifier(random_state = 42) dt.fit(train_scaled, train_target) print(dt.score(train_scaled, train_target)) print(dt.score(test_scaled, test_target)) . 0.996921300750433 0.8592307692307692 . import matplotlib.pyplot as plt from sklearn.tree import plot_tree plt.figure(figsize = (16,7)) plot_tree(dt) plt.show() # 가장 위에있는 걸 루트노드 # 가장 밑에있는 걸 리프노드 # 그림만 봐도 현기증 날라함 . # 클래스 별로 색칠하기 --&gt; filled # 특성이름 전달 --&gt; feature_names plt.figure(figsize=(10, 7)) # 사이즈 조절 plot_tree(dt, max_depth = 1, filled = True, feature_names = [&#39;alcohol&#39;, &#39;sugar&#39;, &#39;pH&#39;]) plt.show() # 1. 테스트 조건 # 2. 불순도 # 3. 총 샘플 수 # 4. 클래스별 샘플 수 # 1. 테스트 조건을 만족하면 왼쪽, 아니면 오른쪽 # 2. 마지막 리프 노드에 도착했을 때 음성 클래스 샘플 수가 많으면 음성 클래스로 예측, 양성 클래스 샘플 수가 많으면 양성 클래스로 예측 # 정보이득이 최대가 되도록 노드를 분할함 --&gt; 불순도를 이용한 공식이 따로 있지만 결론은 한 클래스 쪽으로 샘플 수가 치우치게 노드를 분할함 . # 끝까지 자라나는 tree는 훈련 세트에 과대적합 된다. dt = DecisionTreeClassifier(max_depth = 3, random_state = 42) dt.fit(train_scaled, train_target) print(dt.score(train_scaled, train_target)) print(dt.score(test_scaled, test_target)) . 0.8454877814123533 0.8415384615384616 . plt.figure(figsize = (20, 15)) plot_tree(dt, filled = True, feature_names = [&#39;alcohol&#39;, &#39;sugar&#39;, &#39;pH&#39;]) plt.show() . dt = DecisionTreeClassifier(max_depth = 3, random_state = 42) dt.fit(train_input, train_target) print(dt.score(train_input, train_target)) print(dt.score(test_input, test_target)) . 0.8454877814123533 0.8415384615384616 . print(dt.feature_importances_) . [0.12345626 0.86862934 0.0079144 ] . &#50672;&#49845; . import pandas as pd wine = pd.read_csv(&#39;https://bit.ly/wine_csv_data&#39;) wine.head() wine.info() wine.describe() wine_input = wine[[&#39;alcohol&#39;, &#39;sugar&#39;, &#39;pH&#39;]].to_numpy() wine_target = wine[&#39;class&#39;].to_numpy() from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(wine_input, wine_target, random_state = 42, test_size = 0.2) from sklearn.tree import DecisionTreeClassifier dt = DecisionTreeClassifier(random_state = 42) dt.fit(train_input, train_target) print(dt.score(train_input, train_target)) print(dt.score(test_input, test_target)) from sklearn.tree import plot_tree import matplotlib.pyplot as plt plt.figure(figsize = (10, 7)) plot_tree(dt, max_depth = 1, filled = True, feature_names=[&#39;alcohol&#39;,&#39;sugar&#39;,&#39;pH&#39;]) dt = DecisionTreeClassifier(random_state = 42, max_depth=3) dt.fit(train_input, train_target) print(dt.score(train_input, train_target)) print(dt.score(test_input, test_target)) . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 6497 entries, 0 to 6496 Data columns (total 4 columns): # Column Non-Null Count Dtype -- -- 0 alcohol 6497 non-null float64 1 sugar 6497 non-null float64 2 pH 6497 non-null float64 3 class 6497 non-null float64 dtypes: float64(4) memory usage: 203.2 KB 0.996921300750433 0.8584615384615385 0.8454877814123533 0.8415384615384616 .",
            "url": "https://parkjeongung.github.io/Ung.github.io/2021/12/26/%EA%B2%B0%EC%A0%95_%ED%8A%B8%EB%A6%AC_DecisionTreeClassifier.html",
            "relUrl": "/2021/12/26/%EA%B2%B0%EC%A0%95_%ED%8A%B8%EB%A6%AC_DecisionTreeClassifier.html",
            "date": " • Dec 26, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "PolynomialFeatures // 릿지 Ridge // 라쏘 Lasso",
            "content": ". import pandas as pd df = pd.read_csv(&#39;https://bit.ly/perch_csv_data&#39;) # 판다스로 준비된 데이터 넘파이 배열로 바꾸기 --&gt; to_numpy() perch_full = df.to_numpy() import numpy as np perch_weight = np.array([5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0, 115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0, 150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0, 218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0, 556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0, 850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0, 1000.0]) . from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(perch_full, perch_weight, random_state = 42) . PolynomialFeatures . 사이킷런은 특성을 만들거나 전처리하기 위한 다양한 클래스를 제공한다. | . from sklearn.preprocessing import PolynomialFeatures poly = PolynomialFeatures() poly.fit([[2, 3]]) print(poly.transform([[2, 3]])) poly = PolynomialFeatures(include_bias = False) # include_bias = False 로 배열에서 1을 제거한다. 사실 굳이 지정하지 않아도 사이킷런 모델은 1을 무시함. 문과라 평면을 안배워서 뭔말인지 잘모르겠으니까 스킵 poly.fit([[2, 3]]) print(poly.transform([[2, 3]])) # PolynomialFeatures 적용 poly = PolynomialFeatures(include_bias = False) poly.fit(train_input) train_poly = poly.transform(train_input) print(train_poly.shape) print(train_input.shape) . [[1. 2. 3. 4. 6. 9.]] [[2. 3. 4. 6. 9.]] (42, 9) (42, 3) . poly.get_feature_names() . [&#39;x0&#39;, &#39;x1&#39;, &#39;x2&#39;, &#39;x0^2&#39;, &#39;x0 x1&#39;, &#39;x0 x2&#39;, &#39;x1^2&#39;, &#39;x1 x2&#39;, &#39;x2^2&#39;] . test_poly = poly.transform(test_input) # 다중 회귀 모델 훈련 from sklearn.linear_model import LinearRegression lr = LinearRegression() lr.fit(train_poly, train_target) print(lr.score(train_poly, train_target)) print(lr.score(test_poly, test_target)) # 여러개의 특성을 사용하니 과소적합도 해결되고 점수도 좋게 나온다. . 0.9903183436982124 0.9714559911594132 . poly = PolynomialFeatures(degree = 5, include_bias = False) poly.fit(train_input) train_poly = poly.transform(train_input) test_poly = poly.transform(test_input) print(train_poly.shape, test_poly.shape) # 특성이 55개 # 추가된 특성으로 모델 훈련 from sklearn.linear_model import LinearRegression lr = LinearRegression() lr.fit(train_poly, train_target) print(lr.score(train_poly, train_target)) # 거의 완벽한 점수 print(lr.score(test_poly, test_target)) # 음수? --&gt; 특성을 너무 많이 늘리면 train 세트에 과대적합 되므로 테스트 세트의 점수가 굉장히 낮게 나온다. . (42, 55) (14, 55) 0.9999999999991096 -144.40579242335605 . &#44508;&#51228; . 모델이 훈련 세트를 너무 과도하게 학습하지 못하도록 하는 것 --&gt; 훈련세트에 과대적합되지 않도록 하는 것 | 선형 회귀 모델의 경우 특성에 곱해지는 계수의 크기를 작게 만드는 일 | . from sklearn.preprocessing import StandardScaler ss = StandardScaler() ss.fit(train_poly) train_scaled = ss.transform(train_poly) test_scaled = ss.transform(test_poly) . &#47551;&#51648; &#46972;&#50136; // &#49440;&#54805; &#54924;&#44480; &#47784;&#45944;&#50640; &#44508;&#51228;&#47484; &#52628;&#44032;&#54620; &#47784;&#45944; . 릿지 회귀 --&gt; 계수를 제곱한 값을 기준으로 규제를 적용 | 라쏘 회귀 --&gt; 계수의 절댓값을 기준으로 규제를 적용 | from sklearn.linear_model import Ridge ridge = Ridge() ridge.fit(train_scaled, train_target) print(ridge.score(train_scaled, train_target)) print(ridge.score(test_scaled, test_target)) # 많은 특성을 사용했는데도 train세트에 너무 과대적합되지 않음 . 0.9896101671037343 0.9790693977615398 . # alpha 와 같이 사람이 직접 알려줘야 하는 파라미터를 &quot;하이퍼파라미터&quot; 라고 한다. # 적절한 alpha 값 찾는 방법 --&gt; alpha 값에 대한 R^2 그래프 그리기 --&gt; train세트와 test세트의 점수가 가장 가까운 지점이 최적의 alpha값 import matplotlib.pyplot as plt # 리스트 만들기 train_score = [] test_score = [] alpha_list = [0.001, 0.01, 0.1, 1, 10, 100] # 반복문으로 alpha 값 돌리기 for alpha in alpha_list: ridge = Ridge(alpha = alpha) ridge.fit(train_scaled, train_target) train_score.append(ridge.score(train_scaled, train_target)) test_score.append(ridge.score(test_scaled, test_target)) # 그래프 그리기 plt.plot(np.log10(alpha_list), train_score) plt.plot(np.log10(alpha_list), test_score) plt.xlabel(&#39;alpha&#39;) plt.ylabel(&#39;R^2&#39;) plt.show() # train 스코어가 가장 높고 test 세트의 점수가 가장 높은 -1 = 10^-1 = 0.1 --&gt; alpha = 0.1로 하여 최종 모델을 훈련해야 가장 좋은 모델 . from sklearn.linear_model import Ridge ridge = Ridge(alpha = 0.1) ridge.fit(train_scaled, train_target) print(ridge.score(train_scaled, train_target)) print(ridge.score(test_scaled, test_target)) . 0.9903815817570366 0.9827976465386927 . from sklearn.linear_model import Lasso lasso = Lasso() lasso.fit(train_scaled, train_target) print(lasso.score(train_scaled, train_target)) print(lasso.score(test_scaled, test_target)) . 0.9897898972080961 0.9800593698421883 . import matplotlib.pyplot as plt from sklearn.linear_model import Lasso train_score = [] test_score = [] alpha_list = [0.001, 0.01, 0.1, 1, 10, 100] for alpha in alpha_list: lasso = Lasso(alpha = alpha) lasso.fit(train_scaled, train_target) train_score.append(lasso.score(train_scaled, train_target)) test_score.append(lasso.score(test_scaled, test_target)) . /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 23364.075969939808, tolerance: 518.2793833333334 positive) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 20251.975097475122, tolerance: 518.2793833333334 positive) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 806.2370926333242, tolerance: 518.2793833333334 positive) . import matplotlib.pyplot as plt from sklearn.linear_model import Lasso train_score = [] test_score = [] alpha_list = [0.001, 0.01, 0.1, 1, 10, 100] for alpha in alpha_list: lasso = Lasso(alpha = alpha, max_iter = 10000) # 라쏘 모델은 최적의 계수를 찾기 위해 반복적인 계산을 수행하는데 지정한 횟수가 부족할 때 이런 경고가 발생 --&gt; max_iter 를 늘려줌으로써 해결 lasso.fit(train_scaled, train_target) train_score.append(lasso.score(train_scaled, train_target)) test_score.append(lasso.score(test_scaled, test_target)) plt.plot(np.log10(alpha_list), train_score) plt.plot(np.log10(alpha_list), test_score) plt.xlabel(&#39;alpha&#39;) plt.ylabel(&#39;R^2&#39;) plt.show() # 최적의 알파값은 1 = 10 --&gt; alpha = 10 으로 하여 모델 훈련 . /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 18778.697957792876, tolerance: 518.2793833333334 positive) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 12972.821345404844, tolerance: 518.2793833333334 positive) . from sklearn.linear_model import Lasso lasso = Lasso(alpha = 10) lasso.fit(train_scaled, train_target) print(lasso.score(train_scaled, train_target)) print(lasso.score(test_scaled, test_target)) . 0.9888067471131867 0.9824470598706695 . # 라쏘 모델의 계수는 coef_ 에 저장되어 있다. print(lasso.coef_) # np.sum 은 배열을 모두 더한 값을 반환한다 # 넘파이 배열에 비교연산자를 사용하면 각 원소는 True = 1 아니면 False = 0 가 된다. print(np.sum(lasso.coef_ == 0)) # 총 55개의 특성 중 40개의 특성의 계수를 0으로 만듬 --&gt; 15개의 특성만 사용 . [ 0. 0. 0. 12.14852453 55.44856399 42.23100799 0. 0. 13.70596191 0. 43.2185952 5.7033775 47.46254536 7.42309425 11.85823365 0. 0. 13.53038193 21.22111356 0. 0. 0. 0. 0. 0. 18.66993032 0. 0. 0. 15.81041778 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 18.14672268 0. 0. 0. 0. 15.51272953 0. 0. 0. 0. 0. ] 40 . &#50672;&#49845; . import pandas as pd df = pd.read_csv(&#39;https://bit.ly/perch_csv_data&#39;) perch_full = df.to_numpy() import numpy as np perch_weight = np.array([5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0, 115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0, 150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0, 218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0, 556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0, 850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0, 1000.0]) from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(perch_full, perch_weight, random_state = 42) from sklearn.preprocessing import PolynomialFeatures poly = PolynomialFeatures(degree = 5, include_bias = False) poly.fit(train_input) train_poly = poly.transform(train_input) test_poly = poly.transform(test_input) from sklearn.linear_model import LinearRegression lr = LinearRegression() lr.fit(train_poly, train_target) print(lr.score(train_poly, train_target)) print(lr.score(test_poly, test_target)) from sklearn.preprocessing import StandardScaler ss = StandardScaler() ss.fit(train_poly) train_scaled = ss.transform(train_poly) test_scaled = ss.transform(test_poly) from sklearn.linear_model import Ridge ridge = Ridge() ridge.fit(train_scaled, train_target) print(ridge.score(train_scaled, train_target)) print(ridge.score(test_scaled, test_target)) train_score = [] test_score = [] import matplotlib.pyplot as plt alpha_list = [0.001, 0.01, 0.1, 1, 10, 100] for alpha in alpha_list: ridge = Ridge(alpha = alpha) ridge.fit(train_scaled, train_target) train_score.append(ridge.score(train_scaled, train_target)) test_score.append(ridge.score(test_scaled, test_target)) plt.plot(np.log10(alpha_list), train_score) plt.plot(np.log10(alpha_list), test_score) plt.xlabel(&#39;alpha&#39;) plt.ylabel(&#39;R^2&#39;) plt.show() from sklearn.linear_model import Lasso lasso = Lasso() lasso.fit(train_scaled, train_target) print(lasso.score(train_scaled, train_target)) print(lasso.score(test_scaled, test_target)) train_score = [] test_score = [] alpha_list = [0.001, 0.01, 0.1, 1, 10, 100] for alpha in alpha_list: lasso = Lasso(alpha = alpha) lasso.fit(train_scaled, train_target) train_score.append(lasso.score(train_scaled, train_target)) test_score.append(lasso.score(test_scaled, test_target)) plt.plot(np.log10(alpha_list), train_score) plt.plot(np.log10(alpha_list), test_score) plt.xlabel(&#39;alpha&#39;) plt.ylabel(&#39;R^2&#39;) plt.show() . 0.9999999999991096 -144.40579242335605 0.9896101671037343 0.9790693977615398 . 0.9897898972080961 0.9800593698421883 . /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 23364.075969939808, tolerance: 518.2793833333334 positive) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 20251.975097475122, tolerance: 518.2793833333334 positive) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 806.2370926333242, tolerance: 518.2793833333334 positive) .",
            "url": "https://parkjeongung.github.io/Ung.github.io/2021/12/26/PolynomialFeatures_%EB%A6%BF%EC%A7%80_Ridge_%EB%9D%BC%EC%8F%98_Lasso.html",
            "relUrl": "/2021/12/26/PolynomialFeatures_%EB%A6%BF%EC%A7%80_Ridge_%EB%9D%BC%EC%8F%98_Lasso.html",
            "date": " • Dec 26, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "Title",
            "content": ". K - &#52572;&#44540;&#51217;&#51060;&#50883; // from sklearn.neighbors import KNeighborsClassifier . 내가 지정한 데이터와 그 주변에 있는 데이터를 비교해서 결과를 예측한다. | 새로운 데이터를 예측할 때 그 주변 직선거리로 가장 가까운 데이터와 비교한다. | 데이터가 아주 많은 경우 사용하기 어렵다. 직선거리를 계산하는데 많은 시간이 들기 때문이다. | 참고 데이터 기본값은 5이다. n_neighbors로 변경 가능하다 | bream_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0] bream_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0] smelt_length = [9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0] smelt_weight = [6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9] import matplotlib.pyplot as plt plt.scatter(bream_length, bream_weight) plt.scatter(smelt_length, smelt_weight) plt.xlabel(&#39;bream_length&#39;) plt.ylabel(&#39;bream_weight&#39;) plt.show() . bream_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0] bream_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0] smelt_length = [9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0] smelt_weight = [6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9] length = bream_length + smelt_length weight = bream_weight + smelt_weight . fish_data = [[l, w] for l, w in zip(length, weight)] . fish_target = [1] * 35 + [0] * 14 . from sklearn.neighbors import KNeighborsClassifier kn = KNeighborsClassifier() . kn.fit(fish_data, fish_target) . KNeighborsClassifier(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;, metric_params=None, n_jobs=None, n_neighbors=5, p=2, weights=&#39;uniform&#39;) . kn.score(fish_data, fish_target) . kn.predict([[30, 600]]) . kn49 = KNeighborsClassifier(n_neighbors=49) kn49.fit(fish_data, fish_target) kn49.score(fish_data, fish_target) . 0.7142857142857143 . &#50672;&#49845; . bream_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0] bream_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0] smelt_length = [9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0] smelt_weight = [6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9] length = bream_length + smelt_length weight = bream_weight + smelt_weight fish_data = [[l,w] for l,w in zip(length, weight)] fish_target = [1] * 35 + [0] * 14 from sklearn.neighbors import KNeighborsClassifier kn = KNeighborsClassifier() kn.fit(fish_data, fish_target) kn.score(fish_data, fish_target) kn.predict([[30, 600]]) . array([1]) .",
            "url": "https://parkjeongung.github.io/Ung.github.io/2021/12/26/K_%EC%B5%9C%EA%B7%BC%EC%A0%91%EC%9D%B4%EC%9B%83_KNeighborsClassifier.html",
            "relUrl": "/2021/12/26/K_%EC%B5%9C%EA%B7%BC%EC%A0%91%EC%9D%B4%EC%9B%83_KNeighborsClassifier.html",
            "date": " • Dec 26, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "K - 최근접 이웃 회귀 // KNeighborsRegressor",
            "content": ". &#54924;&#44480; . 임의의 어떤 숫자를 예측 | 두 변수 사이의 상관관계를 분석하는 방법 | . import numpy as np # 농어의 특성 = 길이 perch_length = np.array([8.4, 13.7, 15.0, 16.2, 17.4, 18.0, 18.7, 19.0, 19.6, 20.0, 21.0, 21.0, 21.0, 21.3, 22.0, 22.0, 22.0, 22.0, 22.0, 22.5, 22.5, 22.7, 23.0, 23.5, 24.0, 24.0, 24.6, 25.0, 25.6, 26.5, 27.3, 27.5, 27.5, 27.5, 28.0, 28.7, 30.0, 32.8, 34.5, 35.0, 36.5, 36.0, 37.0, 37.0, 39.0, 39.0, 39.0, 40.0, 40.0, 40.0, 40.0, 42.0, 43.0, 43.0, 43.5, 44.0]) # 농어의 타겟 = 무게 perch_weight = np.array([5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0, 115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0, 150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0, 218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0, 556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0, 850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0, 1000.0]) . import matplotlib.pyplot as plt plt.scatter(perch_length, perch_weight) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() # 농어의 길이가 길어질수록 무게가 늘어난다. . from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(perch_length, perch_weight, random_state = 42) . # reshape 메서드는 크기에 -1 을 지정하면 남은 원소 개수로 모두 채우라는 의미 train_input = train_input.reshape(-1, 1) # == train_input.reshape(42, 1) print(train_input.shape) test_input = test_input.reshape(-1, 1) print(test_input.shape) . (42, 1) (14, 1) . from sklearn.neighbors import KNeighborsRegressor knr = KNeighborsRegressor() knr.fit(train_input, train_target) print(knr.score(test_input, test_target)) # 스코어가 1이 안나왔다 --&gt; 회귀에서는 정확한 숫자를 맞힌다는 것이 거의 불가능하다. --&gt; 예측하는 값이 모두 임의의 수치이기 때문 . 0.9928094061010639 . from sklearn.metrics import mean_absolute_error # 테스트 세트의 예측값을 만든다. test_prediction = knr.predict(test_input) # 테스트 세트에 대한 절댓값 오차의 평균을 구한다. mae = mean_absolute_error(test_target, test_prediction) mae # 19그람 정도의 오차가 발생한다. . 19.157142857142862 . &#54984;&#47144; &#49464;&#53944;&#47484; &#49324;&#50857;&#54644; &#47784;&#45944; &#54217;&#44032;&#54616;&#44592; // &#44284;&#49548;&#51201;&#54633;, &#44284;&#45824;&#51201;&#54633; . score(train) &gt; score(test) --&gt; 모델이 훈련세트에 과대적합 되었다. --&gt; 실전에 투입하면 예측이 잘 안됨 --&gt; 모델을 덜 복잡하게 만들어 해결한다. --&gt; 이웃의 개수를 늘린다 | score(train) &lt; score(test) --&gt; 모델이 훈련세트에 과소적합 되었다. --&gt; 모델이 너무 단순하다. --&gt; 모델을 더 복잡하게 만들어 해결한다. --&gt; 이웃의 개수를 줄인다. | . print(knr.score(train_input, train_target)) # 테스트 세트로 점수 확인하기 print(knr.score(test_input, test_target)) # score(train) &lt; score(test) --&gt; 과소적합 --&gt; 데이터가 작을경우 과소적합이 발생할 수 있다. . 0.9698823289099255 0.9928094061010639 . knr.n_neighbors = 3 knr.fit(train_input, train_target) print(knr.score(train_input, train_target)) print(knr.score(test_input, test_target)) # 이웃의 개수를 줄였더니 train 세트와 test 세트의 score가 비슷하게 나온다. . 0.9804899950518966 0.974645996398761 . &#50672;&#49845; . import numpy as np perch_length = np.array([8.4, 13.7, 15.0, 16.2, 17.4, 18.0, 18.7, 19.0, 19.6, 20.0, 21.0, 21.0, 21.0, 21.3, 22.0, 22.0, 22.0, 22.0, 22.0, 22.5, 22.5, 22.7, 23.0, 23.5, 24.0, 24.0, 24.6, 25.0, 25.6, 26.5, 27.3, 27.5, 27.5, 27.5, 28.0, 28.7, 30.0, 32.8, 34.5, 35.0, 36.5, 36.0, 37.0, 37.0, 39.0, 39.0, 39.0, 40.0, 40.0, 40.0, 40.0, 42.0, 43.0, 43.0, 43.5, 44.0]) # 농어의 타겟 = 무게 perch_weight = np.array([5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0, 115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0, 150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0, 218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0, 556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0, 850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0, 1000.0]) # train test split from sklearn.model_selection import train_test_split train_input, test_input, train_target, test_target = train_test_split(perch_length, perch_weight, random_state = 42) # 2차원 배열로 변형 train_input = train_input.reshape(-1, 1) test_input = test_input.reshape(-1, 1) # 모델링 from sklearn.neighbors import KNeighborsRegressor knr = KNeighborsRegressor() knr.fit(train_input, train_target) # 스코어 knr.score(test_input, test_target) # 오차확인 mean absoulte error from sklearn.metrics import mean_absolute_error test_prediction = knr.predict(test_input) mae = mean_absolute_error(test_target, test_prediction) # 과소적합 --&gt; 테스트세트 스코어가 트레인세트 스코어보다 높다 print(knr.score(train_input, train_target)) print(knr.score(test_input, test_target)) # 과소적합 해결 --&gt; 모델을 복잡하게 만든다. --&gt; 이웃수 줄이기 knr.n_neighbors = 3 knr.fit(train_input, train_target) print(knr.score(train_input, train_target)) print(knr.score(test_input, test_target)) . 0.9698823289099255 0.9928094061010639 0.9804899950518966 0.974645996398761 . # n_neighbors = [1, 5, 10] # 농어의 길이를 5에서 45까지 바꿔가며 모델링 결과 확인하기 from sklearn.neighbors import KNeighborsRegressor import matplotlib.pyplot as plt knr = KNeighborsRegressor() x = np.arange(5, 45).reshape(-1, 1) for n in [1, 5, 10]: knr.n_neighbors = n knr.fit(train_input, train_target) predict = knr.predict(x) plt.scatter(train_input, train_target) plt.scatter(x, predict, marker = &#39;^&#39;) plt.title(&quot;n_neighbors = {}&quot;.format(n)) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() .",
            "url": "https://parkjeongung.github.io/Ung.github.io/2021/12/26/K_%EC%B5%9C%EA%B7%BC%EC%A0%91_%EC%9D%B4%EC%9B%83_%ED%9A%8C%EA%B7%80_KNeighborsRegressor.html",
            "relUrl": "/2021/12/26/K_%EC%B5%9C%EA%B7%BC%EC%A0%91_%EC%9D%B4%EC%9B%83_%ED%9A%8C%EA%B7%80_KNeighborsRegressor.html",
            "date": " • Dec 26, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://parkjeongung.github.io/Ung.github.io/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://parkjeongung.github.io/Ung.github.io/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://parkjeongung.github.io/Ung.github.io/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}